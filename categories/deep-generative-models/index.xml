<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep Generative Models - Category - Good Young</title>
        <link>https://goodyoung.github.io/categories/deep-generative-models/</link>
        <description>Deep Generative Models - Category - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 03 Apr 2025 09:32:32 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/categories/deep-generative-models/" rel="self" type="application/rss+xml" /><item>
    <title>[CS236] 2. Background</title>
    <link>https://goodyoung.github.io/posts/cs236/lecture2/</link>
    <pubDate>Thu, 03 Apr 2025 09:32:32 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs236/lecture2/</guid>
    <description><![CDATA[개요 CS236: Deep Generative Models (Stanford)는 스탠포드 대학교에서 진행하는 딥러닝 기반 생성 모델(Deep Generative Models) 에 대한 심화 강의이다. 이번 포스트에서는 CS236 강의의 2강 내용을 정리한다. 2강에서는 말그대로 Generative 분야를 공부하기 전에 필요한 배경 지식을 설명하고 있다. 조건부 독립 (conditional independence) generative model vs discriminative model 생성 모델이란 주어진 데이터(x)의 확률 분포(p(x))를 학습하여 샘플링하여 유사한 데이터를 생성해내는 것이다. ($x ~ p(x)$) 이런 확률 분포 p(x)에 대하여 여러 활용을 할 수 있는데, 아래는 그 활용에 대해서 나타낸다. 밀도 추정: 데이터 유사도 기반 이상 탐지 (원하는 object엔 p(x)가 높다.) Unsupervised representation learning: 공통된 특징 추출 그럼 이제 이러한 데이터의 확률분포 p(x)를 어떻게 표현을 할 것이냐가 먼저이다. Representation p(x) 확률 분포의 표현은 여러가지이다. 베르누이 분포와 범주형(categorical)분포는 위 사진과 같이 표현할 수 있다. 만약 3개의 discrete random variable인 이미지 데이터라면, $p(R = r, G = g, B = b)$의 joint distribution을 구하기 위하여 $256 * 256 * 256 - 1$의 파라미터가 발생한다. 이때 말하는 파라미터란 무엇인가? 파라미터란 확률 분포를 정의하기 위해 필요한 개별 확률값을 의미한다. $X_1, X_2 \in {0, 1}$ 의 변수가 있다고 가정할 때 가능한 조합은 4가지 ((0,0), &hellip; (1,1))이다. 각 조합에 대해 확률 값을 지정해야 하지만, 전체 확률 합은 1이어야 하므로 4개 중 3개만 자유롭게 정하면 나머지 하나는 자동으로 정해진다. 따라서 이 경우 $4 - 1 = 3$ 개의 파라미터를 가진다. 만약 n개의 binary(Bernoulli) random variable이라면 가능한 image의 수는 $2 \times 2 \times &hellip; \times 2 = 2^n$ (n: pixel 수)일 것이다. 그럼 이때 이 분포에서 sampling을 한다면 특정 분포의 joint distribution $p(x_1,&hellip;,x_n)$은 $2^n - 1$의 파라미터가 필요하다. 따라서 random variable의 수에 따라 파라미터의 수가 기하급수적으로 증가한다는 사실을 알 수 있다. 이 모든 값들을 컴퓨터에 저장하기엔 무리가 있다. 그렇기 때문에 수학적 가정이 필요한 순간이다. 그래서 독립성 가정(Independence Assumption)을 하겠다. $X_1, &hellip; , X_n$ 베르누이 분포를 만족하는 확률 변수들이 있다고 가정했을 때 $p(x_1, &hellip; , x_n) = p(x_1)\dot\dot\dot p(x_n)$ 을 만족한다. 이 경우, 가능한 상태(이미지)는 동일하게 $2^n$이고 joint distribution의 파라미터는 $n$이다. Marginal distribution $p(x_1)$의 파라미터가 1이다. 따라서 $1 + 1 + &hellip; + 1$이기 때문에 파라미터는 n이 된다. 독립성 가정은 너무 strong해서 위의 그림 처럼 모든 값이 독립적으로 무작위 값을 선택하여 샘플링 결과가 안좋다. $$p(S_1 \cap S_2 \cap \cdots \cap S_n) = p(S_1) \cdot p(S_2 \mid S_1) \cdot \cdots \cdot p(S_n \mid S_1 \cap \cdots \cap S_{n-1})$$ $$p(S_1 \mid S_2) = \frac{p(S_1 \cap S_2)}{p(S_2)} = \frac{p(S_2 \mid S_1) \cdot p(S_1)}{p(S_2)} $$
따라서 다른 가정이 필요하여 두가지 중요한 rule(공식)이 있다. 위는 순서대로 Chain rule (probability)와 Bayes' rule의 식이다. $$p(x_1, x_2, \ldots, x_n) = p(x_1) \cdot p(x_2 \mid x_1) \cdot p(x_3 \mid x_1, x_2) \cdots p(x_n \mid x_1, \ldots, x_{n-1})$$
Chain Rule을 사용하면 위의 식이 된다. 이때 파라미터는 $1+2+&hellip;+2^\text{n-1} = 2^n - 1$이다. 여전히 exponential하다는 것을 알 수 있다. 이 chain rule을 사용한 식에 conditional independence를 가정을 한다. $$p(x_1, x_2, \ldots, x_n) = p(x_1) \cdot p(x_2 \mid x_1) \cdot p(x_3 \mid x_2) \cdots p(x_n \mid x_{n-1})$$
그렇게 되면 위의 식과 같아지고 파라미터는 $2n - 1$로 해결이 가능해졌다. ($X_{i+1} \perp {X_1, \ldots, X_{i-1}} \mid X_i$ 이렇게도 표기한다.) Bayes Network Chain rule을 통해 모든 확률변수의 joint distribution을 표현할 수 있지만, 이때 conditional independence를 활용하면 필요한 파라미터 수를 대폭 줄일 수 있다.]]></description>
</item>
</channel>
</rss>
