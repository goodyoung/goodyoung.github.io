<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>DL - Category - Good Young</title>
        <link>https://goodyoung.github.io/categories/dl/</link>
        <description>DL - Category - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 18 Jul 2024 15:36:30 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/categories/dl/" rel="self" type="application/rss+xml" /><item>
    <title>[CS231n] 06.Training Neural Networks</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture6/</link>
    <pubDate>Thu, 18 Jul 2024 15:36:30 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture6/</guid>
    <description><![CDATA[개요 CS231n의 6강에 대한 내용을 정리 할 것이다. 저번 강에서는 Convolutional Neural Networks에 대해 배웠고 이번 강에서는 이 Neural Networks를 어떻게 하면 잘 학습시킬 수 있을지에 대해서 배울 것이다. Activation Function Deep Network에 비선형성을 부여하는 Activation Function의 종류들에 대해서 알아볼 것이다. Sigmoid $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
먼저 첫 번째로 Sigmoid이다. 함수의 모양은 위 그림과 같고 (0,1) 사이의 값을 가지게 된다.Sigmoid가 가지게 되는 몇가지 문제들이 있다. 첫 번째 문제는 경사도를 죽일 수 있는 문제이다.
이는 입력으로 들어오는 x가 -10, 10이라면 global gradient(이전 노드의 기울기)는 0이 되고 이것이 역전파를 통해 다운스트림($global grdient * local gradient$) 되면 global gradient가 0이므로 아래 노드들은 0으로 수렴하게 되어 기울기가 소실이 된다. 이러한 기울기 소실로 인하여 W의 값을 더이상 update 하지 못하는 문제를 vanishing gradient라고 한다. 두 번째 문제는 output이 zero-center가 아닌 문제이다. 즉, Signmoid의 output이 항상 양수이기 때문에 0을 중심으로 있지 않는 문제가 발생하게 된다. ouput이 항상 양수 이라면 입력값 또한 양수인 것인데, 그렇게 된다면 back propagation에서 $\frac{dl}{df} * \frac{df}{wi}$ 로 W의 기울기를 chain-rule로 구하게 된다. 근데 우리가 이전 4강에서 배운 chain-rule의 pattern 중 mul gate(곱)일 때 grdient가 서로 전환이 된다고 배웠다. 따라서 $\frac{df}{wi}$ 이것이 바로 $x_i$인 것을 알 수 있게 된다. 이때 $x_i$가 이전 layer의 sigmoid 연산을 거친 output이기 때문에 무조건 양수이다. 입력값은 항상 양수라면 back propagation을 할 때 $\frac{dl}{df}$(global gradient이다.)의 부호를 따라가게 되어서 gradient의 부호는 항상 양수이거나 음수가 된다. 부가적으로 gradient의 부호는 다운 스트림으로 내려온 gradient의 부호와 같아진다고 생각하면 쉽다. 이렇게 부호가 모두 양수 또는 음수가 되어버리면 발생하는 문제는 gradient update를 할 때 나타난다. w1,w2가 있다면 gradient update시 항상 (+)(+) 이거나 (-)(-)가 된다. 따라서 위 그림과 같이 파란색 방향(최적의 gradient update)과는 다르게 비효율적으로 지그재그 방향으로 내려가게 되는 상황이 벌어지게 된다. 이를 해결할 수 있는 방법은 input값에 mean을 모두 빼버리면 zero-mean data가 된다. 그렇게 되면 X가 양수/음수를 모두 가지고 있는 상황을 만들 수 있게 된다. 마지막 세 번째 문제는 Sigmoid에서의 exp의 연산의 계산 비용이 많이 들어간다는 것이다. 따라서 위 세 가지의 문제로 잘 사용하지 않게 된다. tanh 다음은 tanh함수이다. 이는 범위를 (-1,1)까지 하여 Sigmoid의 단점인 non-zero-center문제를 해결하게 되었다. 하지만 여전히 그래프를 봐도 확인 할 수 있지만 경사도를 죽일 수 있는 문제는 해결되지 못하였다. 따라서 Network가 깊어지는 모델이 많아진 요즘 활성화 함수로 잘 사용하지 않게 된다. ReLU $$f(x) = max(0,x)$$
다음은 ReLU함수 이다. 이는 위 두 함수과 다르게 양수에서 경사도를 죽일 수 있는 문제를 해결 하였다. 또한 이 함수는 최대값만 계산하기 때문에 연산이 효율적이고 실제로 앞선 함수들과 비교하여 6배가 빠르다고 한다. 하지만 ReLU함수도 몇가지 문제점이 있다. Non-zero-center문제를 가지고 있고 또한 음수일 경우에는 경사도를 죽일 수 있는 문제가 발생할 수 있다. 따라서 W를 initialization하는 과정에서 잘못되었을 때 dead ReLU가 발생할 수 있다. 따라서 이런 dead ReLU를 피하기 위하여 초기화를 할 때 positive한 bias를 얻게 초기화를 하는 방법도 있지만 효과가 경미하다. 또한 learning rate를 크게 설정하는 대규모 훈련 시 W가 너무 크게 비약해서 학습이 안되는 상황이 발생할 수도 있다. 실제로 ReLU를 사용한 Network에서 10~20% 정도가 dead ReLU에 빠진다고 한다. 문제이긴 하지만 그럼에도 불구하고, train은 잘 된다고 한다. Leaky ReLU $$f(x) = max(0.01x,x)$$
ReLU와 다르게 음수에도 음수 기울기를 주어 경사도를 죽일 수 있는 문제를 해결하고자 한다. ReLU와 같게 연산이 빠르다는 장점도 있다. PReLU $$f(x) = max(ax,x)$$
PReLU는 음수 영역에 backpropagation에서의 파라미터로 기울기 값으로 유연성을 더해주어 조금 더 융통성을 가해 주었고 ELU 다음의 ELU는 평균 출력이 더욱 0에 가까워지게 만든 함수이다.]]></description>
</item>
<item>
    <title>[Paper Review]Fast R-CNN</title>
    <link>https://goodyoung.github.io/posts/paper/fast-rcnn/</link>
    <pubDate>Wed, 17 Jul 2024 18:19:31 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/fast-rcnn/</guid>
    <description><![CDATA[개요 Object Detection분야에서 널리 사용되는 딥러닝 모델인 Fast R-CNN에 대한 논문 리뷰를 해보려고 한다. 이번 Fast R-CNN은 R-CNN의 단점을 고안하고자 나온 모델이다. Introduction 본 논문이 나온 시기의 detection은 classification보다 더 복잡한 모델로 해결을 했는데 multi stage pipeline을 가진 모델들은 slow하고 inelegant하다. 이런 complexity는 object의 localization 때문에 일어난다. 이는 두가지 문제점을 지니고 있는데 수많은 후보 object들이 제안된다 이 후보들이 정확한 localization을 하기 위해 다시 refine해야 된다. 따라서 본 논문에서는 이전 R-CNN과는 달리 single-stage(분류하고 공간 정보를 강화하는) 훈련 기법을 제안한다 VGG16을 사용했으며 R-CNN보다 9배 빠르고, SppNet보다 3배 빠르다. 그리고 논문에서는 이전 모델인 R-CNN의 몇가지 단점을 설명한다. 훈련 시 multi-stage pipeline이다.
object proposals을 cnn을 통하여 특징 추출을 하고 그것들의 feature를 svm이 분류를 해주고 마지막으로 bounding box regressor를 통해 3단계를 거친다. 훈련 시 공간과 시간적으로 낭비가 된다.
svm, bounding-box regressor를 할 때, 오버헤드가 심하다 마지막으로 이미지를 test를 할 때 이미지당 47초가 걸린다.
R-CNN은 계산량 공유 없이 각각의 object마다 계산을 해서 오래걸린다. SppNet은 이러한 점을 극복했다. SppNet은 전체 이미지의 feature map을 계산 한 후 거기서 각각의 object proposal을 분류 한다. proposal을 위해 feature에서 고정된 크기로 추출한다. 그리고 다중 출력 크기로 추출한 다음 그것을 spatial pytamid pooling에서 합친다. 이를 통해 테스트 시간(10~100배)과 훈련 시간(3배)을 크게 줄일 수 있습니다. 근데 이러한 SppNet도 단점이 있다. 여러 단계를 거친 pipeline이라는 것이다. 하지만 R-CNN과 달리, SppNet에서 제안된 미세 조정 알고리즘은 공간 피라미드 풀링 이전의 합성곱 층들을 업데이트할 수 없다. 이러한 제한 사항(고정된 합성곱 층들)은 매우 깊은 네트워크의 정확도를 제한한다. 따라서 이런 단점들을 보안하고자 Fast R-CNN을 고안했다. Fast R-CNN은 몇가지 장점이 있다. 다른 것들 보다 높은 mAP(점수) multi task loss를 사용한 single-stage 훈련 기법 모든 network layer가 update된다. 특징 추출에 disk storage가 필요하지 않다. 사진은 Fast R-CNN의 architecture의 overview이다. Fast R-CNN은 input으로 전체 이미지를 넣고 그와 함께 object proposal (selective search로 구해둠)의 set을 같이 넣는다. 그럼 network는 여러 conv를 거쳐 conv feature map을 생성한다. 그럼 각각의 RoI pooling layer은 추출한다. 고정된 크기의 feature vector가 생성이 된다. (ROI들은 각각 다른 크기를 지녔기 때문) 그 후 Fully Connected (FC)층으로 가며 이것은 또 2가지 분기로 나뉜다. 하나는 (K +1 class의)softmax 확률 추정치를 구한다 다른 하나는 각 K 개의 객체 클래스에 대해 4개의 실수 값을 출력합니다. 4개의 값 집합 각각은 K 개 클래스 중 하나에 대한 세밀한 바운딩 박스 위치를 인코딩합니다. RoI pooling layer RoI pooling layer는 max pooling을 사용하여 region of interest(RoI)를 고정된 크기의 spatial small feature map으로 변환한다. Selective search를 통해 resion proposal을 얻게 된다. 이때 spatial small feature map의 $H, W$의 값은 특정 RoI와는 독립적인 하이퍼 파라미터이다. RoI는 합성곱 특징 맵(conv feature map) 내의 사각형 창을 의미합니다. 각각의 RoI는 **(r,c,h,w)**의 특징을 지니고 있는데 **(r,c)**는 top-left를 의미하고 height와 width는 (h,w)를 의미한다. RoI max pooling은 $h × w$ 크기의 RoI 창을 $h/H × w/W$ 크기의 grid를 만든다. 그 후 grid에 max pooling하여 해당 $H × W$ 크기의 출력 grid 셀에 넣는 방식으로 동작합니다. Pooling은 표준 max pooling에서처럼 각 feature map channel에 독립적으로 적용됩니다. 이는 Sppnet에서 하나의 pyramid level만 사용한 것과 동일하다. 결론적으로 원래 이미지를 CNN에 통과시킨 후 나온 feature map에 이전에 생성한 RoI를 projection시키고, 이 RoI를 FC layer input 크기에 맞게 고정된 크기로 변형할 수가 있다. 이를 통해 RCNN 처럼 2000번의 CNN연산 필요 없이, 단 한번의 연산으로 속도를 대폭 높일 수 있게 된다.]]></description>
</item>
<item>
    <title>[CS231n] 05.Convolutional Neural Networks</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture5/</link>
    <pubDate>Fri, 12 Jul 2024 20:01:34 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture5/</guid>
    <description><![CDATA[개요 CS231n의 5강에 대한 내용을 정리 할 것이다. 저번 강에서는 W를 업데이트 하는 과정 Chain-Rule과 간단한 Neural Networks에 대해서 배웠는데 이번 강에서는 Convolutional Neural Networks에 대해서 배울 것이다. A bit of history 위 그림을 보면 Hubel &amp; Wiesel이 고양이 실험을 했는데 이때 시각 피질 안의 뉴런이 local receptive field를 가지게 된다는 것을 알게 되었다. 또한 이렇게 겹쳐지는 receptive field들이 전체 시야를 이루게 된다는 사실을 알게 되었다. 어떤 뉴런은 low level, 또 어떤 뉴런은 high level의 특징들을 포착하는 것의 조합으로 이루게 된다. 이러한 지식을 기반으로 image처리를 위해 98년에 Lenet, 2012년에 AlexNet이 등장하게 되었다. Convolutional Neural Networks 저번 시간에 node들이 linear하게 모두 연결되어 있는 층을 Fully Connected (FC)층이라고 한다. 또 이 FC층이 행렬의 내적 계산을 통한 아주 효율적인 계산을 할 수 있다고 배웠다. 따라서 위 그림을 보면 하나의 layer는 하나의 연산을 통해 계산을 하여 편의성을 더해주는 사실을 볼 수 있다. 하지만 이미지 처리를 할 땐 위의 FC층이 좋지 않다. 왜냐하면 이미지의 spatial(공간적)정보가 손실이 되기 때문이다. 이러한 문제점을 해결하기 위하여 Convolutional Neural Networks (CNN)이 등장하게 되었다. CNN에 대해 더 자세히 알아보면 위 그림과 같은 이미지가 있고 필터가 존재한다. 이 필터는 우리가 linear classification에서 배웠던 W의 역할을 한다. 이런 필터는 input image위를 아래와 같이 슬라이딩 하면서 요소별 곱을 하고 그것을 또 하나의 합으로 나타낸다.
위 그림에선 3 * 3의 크기의 filter가 5 * 5의 크기의 image 위를 슬라이딩 하고 있다. 이 filter에 중요한 사실이 있다. 바로 filter의 depth 크기는 input volume의 depth랑 항상 같다는 점이다. 왜냐하면 우리가 시각적으로 보기에 image위를 슬라이딩 하는 것 처럼 보이지만 실제로의 연산은 $w^Tx$에서 image에서 filter가 겹쳐지는 부분만큼 가져온 후 1차원으로 늘린 연산이다. 따라서 위 그림과 같이 5*5*3의 크기인 filter연산은 곧 75-1D의 내적 연산(1차원)을 수행하는 것으로 이해하면 될 것 같다. 이렇게 연산을 수행하기 때문에 input volume의 depth랑 filter depth랑 같아야 filter의 내적 연산을 수행할 수 있게 된다. 이렇게 filter가 슬라이딩을 하여 연산을 한 번 모두 하면 위 그림과 같이 하나의 activation map이 나온다. 만약 필터가 6개가 있다면 앞서 말한 내적 연산을 6번 수행하여 ouput의 depth가 6인 activation map이 나오게 된다. 각 필터는 input volume에서 특정 유형의 템플릿이나 개념을 찾는다. 6개의 각각 다른 가중치를 지닌 activation map으로 생각하면 된다. 이런 layer들을 연속적으로 쌓아나가면 그것이 convolution network가 된다. 이런 convolution network에서는 처음에 low-level(edge 등등)의 특징을 추출하고 점점 깊어지면 high-level의 복잡하고 추상적인 개념들이 나타나게 된다. 이 정보들을 FC층에 넣고 각 class 수의 확률 연산을 하게되면 분류가 이루어진다. 왜 FC층이 필요하며, 어떻게 분류가 이루어지는지 의문을 가졌다. high-level features는 넓은 reception field를 가지고 있다. (점점 깊어질 수록 이미지의 resolution이 줄어들기 때문) 이러한 복잡하고 풍부한 정보를 포함하고 있는 feature들을 linear하게 놓고 모든 정보를 연결(Fully connected)을 하여 각각의 class별 weight를 계산을 하게 되면 해당 이미지가 class별 확률(softmax)이 나오게 될 것이다. 따라서 CNN으로 feature를 추출하고 FC 층에서 분류하는 이유는 이러한 과정을 통해 각 클래스별 확률을 효과적으로 계산할 수 있기 때문입니다. 이러한 방법들은 앞서 history에서 말한 인간의 인식 능력에서의 시각피질과 유사한 특징을 보이는 것을 알 수 있다. 픽셀은 항상 일관된 순서를 가지며, 서로 인접한 픽셀끼리 영향을 준다. 만약 모든 근처의 픽셀이 빨간색이라면 해당 픽셀도 빨간색일 가능성이 높다. 이렇게 픽셀은 주변 픽셀 값과 비교하여 정보를 추측할 수 있습니다. 이런 특성을 locality라고 합니다. 따라서 위 그림과 같이 sub sampling과정을 통해 image의 resolution을 줄이고 local feature들에 대한 연산을 통해 global feature(high-level)로 나아가 weight 변수를 줄이고 변화에 무관한 invariance를 얻게 되는 것이다.]]></description>
</item>
<item>
    <title>[Paper Review]Rich feature hierarchies for accurate object detection and semantic segmentation</title>
    <link>https://goodyoung.github.io/posts/paper/r-cnn/</link>
    <pubDate>Wed, 10 Jul 2024 16:01:50 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/r-cnn/</guid>
    <description><![CDATA[개요 Object Detection분야에서 널리 사용되는 딥러닝 모델인 RCNN에 대한 논문 리뷰를 해보려고 한다. RCNN은 이미지 내에서 객체를 정확하게 탐지하고 분류하는 문제를 해결하기 위해 개발되었다. Abstract 최근 몇 년간 Object Detection 분야가 정체 상태에 있었다. 그동안 가장 성능이 좋은 방법론은 low-level image feature와 high-level context를 섞는 것이다. 본 논문에서는 간단하고 확장 가능한 object-algorithm을 제시한다. 이는 VOC2012에서 SOTA 결과 대비 30%나 향상된 mAP를 보여준다. 본 논문에서는 두가지 방법을 사용한다 high-capacity cnn을 하위의 region proposal에 적용시킨다. 이를 통해 RCNN이라고 불린다. data가 부족할 때, 사전 학습 된 모델을 도메인 특화 미세 조정을 하면 성능이 크게 향상된다. 따라서 region proposal을 CNN과 결합했기 때문에 R-CNN(Regions with CNN features)라고 한다. Introduction 지난 10년간 visual recognition task에서의 진전은 SIFT와 HOG의 사용에 의존되어 왔다. 둘 다 컴퓨터 비전에서 널리 사용되는 두 가지 이미지 특징 추출 기법이다. SIFT: 다양한 스케일과 회전에 대해 불변인 키포인트와 불변인 특징을 만드는 기법이다. HOG: 이미지의 지역적인 형태나 외곽선을 표현하는 방법으로, 물체 탐지, 특히 사람 탐지에 사용된다. 2010~2012년가지 소폭적인 개선만 이루어 졌다. AlexNet의 개발로 인하여 CNN이 크게 향상이 되었다. 그 후 Classification이 Object Detection의 결과에 어느정도의 영향을 미치는지에 대해 관건이었다. 본 논문은 HOG와 같은 기법들과 비교하여 CNN이 Object Detection 성능을 향상시켰음을 보여준다. 이러한 결과를 얻기 위하여 두가지 문제에 집중하였다. Deep network에서의 localizing object 작은 양의 detection data로 높은 capacity model 훈련하기 Classification과는 달리 Object Detection에서는 localization이 문제이다.이를 해결하기 위한 방법은 2가지 방법이 있다. Regression problem으로 설정 실용적으로 좋지 않다. (30%의 결과가 나온다) Sliding-window detector를 구축 본 논문의 CNN은 좀 더 깊은(다섯 개의 CNN layer) layer를 구축하였는데 이는 높은 spatial resolution을 유지하기 어렵다. 따라서 이는 객체의 정확한 위치를 찾는데 어려움이 있다는 것이고 이 역시 아직 남아있는 과제임을 나타낸다. 본 논문은 첫 번째 문제를 Recognition using region을 통해 해결하려고 한다. 위 그림 처럼 Region-proposal을 통해 2000개의 카테고리를 만들고 이를 CNN을 사용해 고정적인 길이의 특징 벡터를 추출 한다. 이때 입력되는 이미지의 사이즈도 고정되어야 하기 때문에 아핀 변환 등으로 이미지를 추출한 후 입력으로 사용한다. 그 후 이를 선형 SVM으로 분류한다. 이는 영역의 크기에 상관없이 동일한 크기로 변환이 된다. 본 논문에서는 이를 Region-proposal과 CNN을 같이 사용하므로 R-CNN이라고 한다. 본 논문은 두 번째의 문제를 사전 학습 미세조정에 따른 비지도 사전 훈련을 허용하여 해결했다. ILSVRC인 임의의 큰 데이터로 지도 학습을 한 모델에 PASCAL의 작은 데이터를 domain 특화 미세 조정을 하는 패러다임을 제시한다. 이를 통해 결과가 33%가 올랐다. Object Detection with R-CNN 크게 3가지 모듈을 포함하고 있다. Category-independent region proposal Large Convolutional neural network Class specific linear SVM 본 논문에서는 Region-proposal을 생성하기 위해 Selective Search방법을 이용한다.
Selective Search란 색상, 질감, 영역크기 등을 이용해 non-objective segmentation을 수행한다. 이 작업을 통해 좌측 제일 하단 그림과 같이 많은 small segmented areas들을 얻을 수 있다. Bottom-up 방식으로 small segemented areas들을 합쳐서 더 큰 segemented areas들을 만든다. 두 번째의 작업을 반복하여 최종적으로 2000개의 region proposal을 생성한다. 또한 본 논문에서는 AlexNet의 모델을 사용하여 $227 * 227$의 고정적 크기인 이미지를 받게 한다.
따라서 임의의 다양한 크기를 가진 영역들을 고정된 크기로 바꾸는 작업인 warping의 과정을 거친다. 2000장의 region-proposal이 selective-search에 의해 나오면 ground-truth와 IoU를 비교하여 0.5 보다 큰 경우를 positive로 구분하고 그 외를 negative로 구분한다. 또한 positive랑 negative가 겹치는 객체를 정확히 탐지하기 위하여 IoU overlap threshold를 사용하여 IoU 임계치를 주어 객체 탐지 성능을 높인다.]]></description>
</item>
<item>
    <title>[CS231n] 04.Introduction to Neural Networks</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture4/</link>
    <pubDate>Mon, 08 Jul 2024 21:02:03 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture4/</guid>
    <description><![CDATA[개요 CS231n의 4강에 대한 내용을 정리 할 것이다. 저번 강에서는 Loss Function과 Optimization에 대해서 배웠는데 이번 강에서는 W를 업데이트 하는 과정인 Chain-Rule과 간단한 Neural Networks에 대해서 배울 것이다. Backpropagation 지난 과정에 gradient에는 두가지 종류가 있다고 배웠다. 그 중 빠르고 정확한 analytic gradient에 대해서 활용해볼 것이다. 각 과정의 연산 과정을 Computational graph을 활용하여 표현한다면 analytic gradient를 활용할 수 있게 된다.
이를 통해 함수는 BackPropagation이라는 기술을 사용하고, gradient를 얻기 위하여 Chain-rule를 활용한다.
BackPropagation의 과정은 다음과 같다. 각 입력이 local node로 들어오고 다음 노드로 직접 전달된다. local gradient는 이때의 입력된 노드의 출력의 gradient이다. 각각의 입력마다 그때의 local gradient를 구한다. 즉, z에 대한 x로의 미분, z에 대한 y로의 미분을 구한다. 이를 Forward Pass (Foward Propagation)이라고 한다. Forward Pass의 맨 마지막에는 loss function을 통한 loss가 나온다. Forward Pass가 모든 노드가 진행이 되었으면 Backward Pass (Back Propagation)이 진행된다. 이때 Back Propagation은 수많은 계산을 거쳐 나온 loss에 대한 z의 미분을 나타내고 이는 global gradient (위 그림에선 빨간색 글씨로 gradients라고 표기)라고 칭한다. 이때 그럼 loss에 대한 x, y의 미분값을 구할 수 있게 되는데 이때 활용되는 개념이 Chain-rule이다. Forward Pass로 구한 local gradient의 값과 그 노드의 global gradient를 곱하면 우리고 최종적으로 원하는 gradient가 나오게 된다. $gradient = local \ gradient * global \ gradient$ 이런 Computational graph에서 그룹화를 할 수 있다는 사실도 알 수 있다. 위 그림을 보면 sigmoid gate로 하나의 노드로 묶어서 계산 할 수도 있다. 따라서 얼마나 그룹화를 하여 노드를 표현할 것인지에 대한 고민이 필요할 수 있다. 또한 Back Propagation에는 3가지 패턴이 존재한다고 한다. add gate gradient 전달하는 역할 max gate 한 방향으로 gradient 모두 전달하는 역할 mul gate 서로 gradient 전환하는 역할 종합적으로 위에서 배운 Back Propagation을 아래와 같이 일반화된 식으로 표현한다. $$ \frac{\partial f}{\partial x} = \sum \frac{\partial f}{\partial q_i} \cdot \frac{\partial q_i}{\partial x} $$
이제 위에서 배운 Back Propagation에서 변수가 벡터라고 생각한다면 gradient는 Jacobian matrix로 표현할 수 있게 된다. Jacobian matrix: 각 요소의 미분을 포함하는 행렬 따라서 4096의 input이 들어온다면 이 Jacobian matrix의 크기는 $4096 * 4096$ 일 것이다. 이때 gradient의 각 요소는 함수의 최종 출력에 얼마나 영향을 미치는가를 정량화 한 값으로 표현이 되고 이는 결국 편미분한 값과 이어지게 된다. 따라서 입력의 어떤 차원이 출력의 어떤 차원에 영향을 주는지, 그래서 Jacobian matrix는 입력의 각 요소가 오직 출력의 해당 요소에만 영향을 주기 때문에 대각 행렬이 될 것이다. Neural Networks 위 그림 처럼 2계층 신경망을 얻기 위해 다른 것 위에 비선형 변환을 하면 된다. 이렇게 계속 층층 쌓아가면 Deep Neural Network의 형태가 된다. 위의 W1, W2는 각각 gradient로 학습 시키고, 그 gradient들은 Chain-rule으로 계산하여 구한다. 이런 비선형성의 특징을 표현하기 위해 activation function이라는 함수가 존재한다. 이는 강의 후반부에 더 자세히 다룬다고 한다. 아래는 이 Forward pass과정을 코드로 표현한 것이다. f = lambda x: 1.0/(1.0+ np.exp(-x)) # sigmoid (activation function) x = np.random.randn(3, 1) # random input vector h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer out = np.dot(W3,h2) + b3 # output neuron (1*1) Reference https://chasuyeon.tistory.com/entry/cs231n-4%EA%B0%95-%EC%A0%95%EB%A6%AC-Introduction-to-Neural-Networks ]]></description>
</item>
<item>
    <title>[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)</title>
    <link>https://goodyoung.github.io/posts/paper/deeplab-v2/</link>
    <pubDate>Sat, 06 Jul 2024 17:22:47 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/deeplab-v2/</guid>
    <description><![CDATA[개요 DeepLab V1에 이어서 DeepLab V2에 대한 논문 리뷰를 해보려고 한다. V1과 엄청나게 큰 차이는 없지만 방법론의 변화가 있었다. Abstract 본 논문에서 해당 모델(DeepLab-V2)에서 사용하고 있는 세가지의 주된 방법을 설명하고 있다. 첫 번째로 Atrous convolutional이다. 이는 파라미터의 수 증가 없이 더 많은 context들을 포함하여 FOV(Field of View)를 증가 시킨다. 두 번째로 Atrous spatial pyramid pooling (ASPP)이다. 이는 multi scale로 image context를 다양한 context들을 효과적으로 포착이 가능하다. 따라서 ASPP를 사용하게 되면 더욱 robust하게 객체를 분할 할 수 있다고 설명한다. 세 번째로 Conditional Random Field (CRF)이다. 이는 max-pooling, downsampling의 결합이 배치되어 invariance가 있던 것들을 CRF를 사용하면서 질, 양적으로 localization 성능을 향상시킨다고 나온다. Introduction hand-crafted feature보다 더 좋은 성능을 나타내기 때문에 Deep Convolutional Neural Networks (DCNN)이 classification, object detection에서 많이 사용된다. 이러한 성공을 이끈 것은 image 변형에 대한 추상적인 data representation을 학습하게 하는 built-in invariance(불변성)이다. 하지만 이런 invariance는 classification task에는 좋지만 spatial information이 undesired한 segmentation에는 좋지 않다. 따라서 본 논문에서는 이런 단점을 극복하기 위해 아래 세가지 사항을 고려 했다. reduce feature resolution existence of object at multiple scale reduce localization accuracy 첫 번째 challenge는 max-pooling과 downsampling이 반복되어서 나타낸다. 이는 spatial resolution을 줄이기 때문에 안좋다. 이를 해결하기 위해서 우린 맨 마지막 단의 몇개의 maxpooling layer를 제거하고 높은 sampling 비율(더 높은 해상도)로 계산하기 위하여 대신에 upsample 하는 filter를 추가한다. 이때 filter는 hole algorithm을 사용한 atrous convolution을 추가한다. 실제 atrous convolution과 hole algorithm을 사용하여 계산량을 증가시키지 않고 FOV를 증가시켰다. 두 번째 challenge는 기존의 방법은 동일한 이미지를 rescale 버전을 DCNN에 입력한 후 얻은 feature map을 aggregate한다. 이 방법은 performance는 증가하지만 computing overhead가 발생한다. 따라서 본 논문에서는 spatial pyramid pooling (SPP)을 사용하여 convolution 전에 주어진 feature layer를 resampling 하는 효율적인 구조를 제시한다. 이것은 원본 이미지를 여러(Multiple) 필터로 보는 것과 같아서 효율적인 FOV, useful한 multi scale의 관점에서 다양한 image context를 포착할 수 있다. 우린 병렬로 atrous convolutional layer를 사용한다. 이를 &lsquo;atrous spatial pyramid pooling&rsquo; (ASPP)라고 불린다. 세 번째 challenge는 object-centric classifier는 공간적 변형에 대한 불변성이 필요하다. 이를 해결하기 위해선 마지막 segmentation결과를 계산하기 위한 hyper-column의 특징을 뽑는 skip layer를 사용하여 해결한다. 미세한 edge detail을 포착하기 위하여 fully connected pairwise CRF를 사용한다. 또한, CRF랑 DCNN이랑 연결했을 때 SOTA를 달성했다. 마지막으로 따라서 본 논문에서 DCNN 모델로 VGG-16, ResNet-101을 사용했고 Fully convolutional 한 층을 사용하였다. Methods 앞에서 말한 세가지 방법에 대해 좀 더 자세히 설명하도록 하겠다. Atrous Convolution for Dense Feature Extraction and Field-of-View Enlargement x[i]: input w[k]: filter (length: K) r: rate,stride 1차원에서 atrous convolution의 기존 식은 위 그림과 같다. 하지만 atrous convolution을 사용 하기 위해 위 그림과 같이 r의 값을 조절하여 high resolution input feature map에도 사용할 수 있다. 또한 위 그림 처럼 2-D일때의 feature map의 특징을 보면 더욱 뚜렷한 것을 확인할 수 있다. 기존 방식대로 커널을 사용하게 된다면 1/4 의 이미지의 위치에 있는 데이터만 얻을 수 있게 된다. 하지만 atrous convolution을 사용하게 된다면 모든 이미지에 대한 정보를 얻을 수 있기에 spatial resolution이 증가한다. atorous convolutional layer를 모든 층에 사용하기엔 비용적 오버헤드가 발생하여 하이브리드 전략을 취한다. 따라서 본 논문에서는 마지막 풀링, convolution layer에 atrous convolution layer를 추가하여 (stride = 2) 4배로 늘리고, 이중 선형 보간법을 사용해 8배로 늘려 원래 이미지 해상도에서 특징 맵을 복원한다. 이런 atrous convolution은 어떤 레이어에서든 FOV(field of view)를 임의로 확대할 수 있게 된다.]]></description>
</item>
<item>
    <title>[CS231n] 03.Loss Functions and Optimization</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture3/</link>
    <pubDate>Tue, 02 Jul 2024 15:32:06 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture3/</guid>
    <description><![CDATA[개요 CS231n의 3강에 대한 내용을 정리 할 것이다. 저번 강에서는 선형 분류기에 대해서 공부를 했는데 이때 어떻게 이미지의 가중치인 W의 값들의 badness를 판단하는지(loss function), 또 W의 값들을 업데이트 하는 방법(optimization)에 대해서 집중적으로 다룰 것이다. Loss Function Loss Function이란 W를 가져와서 점수를 보고 얼마나 정량적으로 좋은지 나쁜지 알려주는 함수이다. 또는 실제값과 예측값의 차이를 수치화 해주는 함수이다. Loos Function의 기본 식은 아래와 같다. xi: pixel value yi: predict label - f: predict y Li: loss function - L: loss mean 따라서 위 식을 통해 어떤 W가 가장 좋은 결과를 가져오는지 정량적으로 판단할 수 있게 된다. Multiclass SVM Loss 다음으론 image classification에 적합한 multi-class SVM Loss에 대한 설명이다. loss function의 식은 아래와 같다. sj: 잘못된 label의 score syi: 제대로 된 label의 score - 1: safety margin 따라서 위 식은 제대로 예측을 했고 그 값이 safety margin보다 뛰어나다면 loss는 0을 나타내고, 그것이 아니라면 잘못된 label과 정상 label간의 차이만큼이 loss인 것을 알 수 있다. 이를 그래프로 시각화를 하면 아래 그림과 같은데 이를 hinge loss라고 한다. 이 그래프를 통해 알 수 있는 사실은 잘 분류된 point는 loss가 작거나 없고, 잘못 분류된 point는 loss가 높다. 잘 분류되었을 수록 syi가 클 것이다. 그러므로 sj - syi의 식에서 음수값이 나올 가능성이 크다. 다음은 hinge loss에 관하여 6가지의 질문이 나온다. Q1: 잘 예측 했을 경우 데이터가 미세하게 변한다고 해서 loss의 값이 변하지는 않는다. Q2: hinge loss의 최솟값은 0, 최대값은 무한대일 것이다. (class score가 엄청 낮은 음수 값을 가지고 있다면) Q3: W를 임의의 작은 수로 초기화를 할 때 sj - syi의 값이 0일 경우 classes_num - 1의 값이 나오게 된다. 이는 debugging시 유용하다. 훈련 시 초기의 loss값이 classes_num - 1이 아니라면 과정 중 어떤 것이 잘못되었는지 다시 볼 필요가 있다. Q4: 정답 class도 계산을 한다면 기존 loss에 1이 증가한 형태일 것이다. Q5: Summation이 아니라 Mean이라도 loss의 값은 변함 없다. (rescale만 된다. 큰 영향 없다.) Q6: 아래의 그림과 같이 제곱항으로 변경한다면 결과는 달라진다. 선형적이었던 관계를 비선형적으로 바꿔서 표현이 된다. 따라서 잘못 예측한 것은 제곱만큼 loss가 커지게 되는 상황이 발생한다. 따라서 데이터의 특성에 따라 극심한 차이를 보고싶을 때 사용한다. 따라서 어떤 loss를 사용하냐는 error값을 얼마나 신경쓰고 있고, 어떻게 정량화 할거냐에 따라 다르다. 아래는 hinge-loss의 example code이다.
def L_i_vectorzed(x, y, W): scores = W.dot(x) # Predict margins = np.maximum(0, scores - scores[y] + 1) # Calc Loss margins[y] = 0 # class label에 해당하는 것은 0으로 두는 일종의 trick에 해당한다. loss_i = np.sum(margins) # Summation Loss return loss_i # return Regularization 또다른 질문이 있다. *Loss가 0인 W를 찾았다면 이 값과 동일한 다른 W도 존재하는가?*에 대한 질문이다. 정답은 있다! 이다. 기존 W에 2배를 한다고 해서 Loss는 변하지 않는다. 앞서 loss function이 최적의 W를 찾기 위해 정량적으로 나타내는 것이라고 설명을 했다. 하지만 우리는 수많은 W가 0이 된다는 사실을 방금 알게 되었다. 이는 모순임을 알 수 있다. 왜냐하면 오직 위 식은 data의 loss에만 신경을 쓰고 있기 때문이다. 이때의 data는 xi, 즉 training data이다. 이는 아래 그림과 같이 과대 적합 (Overfitting) 인 문제가 발생한다. 우리는 test data에 대한 performance에 관심이 있지 training data에는 관심이 없다. 따라서 우리는 아래 그림과 같이 되기를 원한다. 따라서 기존 Loss 식에 변화를 주어야 된다. 기존 식에 Regularization을 추가하여 이 식은 분류기가 더 간단한 W를 선택하도록 패널티를 주는 역할을 한다.]]></description>
</item>
<item>
    <title>[Paper Review]Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs(DeepLab V1)</title>
    <link>https://goodyoung.github.io/posts/paper/deeplab-v1/</link>
    <pubDate>Thu, 30 May 2024 21:38:53 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/deeplab-v1/</guid>
    <description><![CDATA[개요 Image Segmentation 문제를 해결하기 위해 제안된 대표적인 딥러닝 모델인 DeepLab V1에 대한 논문 리뷰를 해보려고 한다. Abstract 기존 Deep Convolutional Neural Networks이하(DCNN)의 마지막 층에서 충분한 localized가 이루어지지 않았기 때문에 고수준 작업(이미지 분류)에서 좋은 성능을 발휘한다. 그러나 이런 불변성 특성 때문에 마지막 층의 응답은 특정 객체의 위치를 세밀하게 나타내지 못하여, 각 픽셀의 정확한 분류가 필요한 객체 분할(segementation)에는 단점이 될 수 있다. 불변성이란 특정 객체의 위치나 회전 여부에 상관없이 객체의 가장 두드러지는 특징을 추출할 수 있다는 것이다.
따라서 이러한 단점을 극복하기 위해 CRF랑 DCNN 마지막 층과 결합 하고, GPU에서 초당 8 프레임의 계산이 가능하게 한 hole 알고리즘을 앞으로 설명할 것 이다. Introduction 지난 2년 동안 DCNN은 이미지 분류, 객체 탐지 등 고수준 문제에서의 컴퓨터 비전 시스템의 성능을 향상시켰다. End-to-end 방식으로 학습된 DCNN이 SIFT, HOG과 같은 의존 시스템 보다 현저히 더 나은 결과를 제공한다. 이러한 이유는 불변성 덕분이었지만 지역적 정확도를 원하는 pose estimation, semantic segmentation등의 작업(low-level task)에서는 방해가 된다. signal down-sampling과 spatial insensitivity(invariance)이라는 두가지의 어려움이 있었다. signal down-sampling은 반복되는 downsampling 작업과 max-pool작업 때문에 낮아지는 해상도가 문제였다. 본 논문에서는 해당 문제를 atrous (with holes)알고리즘을 통해 해결을 하려고 한다. invariance은 DCNN의 특징 때문에 공간적 정확성을 본질적으로 제한시키는 문제가 있다. 본 논문에서는 해당 문제를 fully-connected Conditional Random Field(CRF)로 해걸하려고 한다. CRC은 Krahenbühl &amp; Koltun이 제안한 fully connected pairwise를 사용한다. 이는 부스팅 기반의 픽셀 분류기의 성능을 크게 향상시킨다. 아래는 DeepLab system의 세가지 큰 장점들 이다. speed atrous algorithm accuracy PASCAL semantic segmentation challenge에서 SOTA달성 simplicity DCNN &amp; CRFs으로 구성되어 있다. Related Work Segmentation에 대한 여러 work들이 있었지만 DeepLab은 FCN과 비슷하게 픽셀 표현에 직접 작동한다. FCN, 다른 모델들과 DeepLab의 차이점은 CRF와 DCNN-based unary term의 결합이라고 볼 수 있다. 본 논문 이후 더 향상된 방법을 사용한 여러 DeepLab의 버전(V2,V3)들이 나와 있다. CONVOLUTIONAL NEURAL NETWORKS FOR DENSE IMAGE LABELING [그림 1] DeepLabv1의 DCNN 구조 VGG16으로 모델을 구성하였으며 더 효율적인 특징 추출기를 사용하였고 그것을 기반으로 finetuned와 re-purposed를 하였다. EFFICIENT DENSE SLIDING WINDOW FEATURE EXTRACTION WITH THE HOLE ALGORITHM 본 논문에선 효과적인 CNN 특징 추출을 위한 방법에 대해서 설명을 한다. 먼저 그것을 구현하기 앞서 VGG16을 FCN과 같이 FC층을 Fully Convolutional층으로 변환을 하고 최종 이미지가 원본의 해상도를 얻게 변환을 한다. 이때 32 pixel 간격으로 매우 드물게 계산이 되기 때문에 이를 8 pixel로 변경이 필요하였다. 따라서 마지막 2개의 pooling층을 스킵하였다. 32 pixel로 한다면 localization이 떨어지기 때문에 8 pixel로 변경이 필요했다. 또한 VGG16의 네트워크는 5개의 pooling층이 존재하는 네트워크이다. 따라서 마지막 2개를 스킵한다면 원본 해상도의 1/8만큼의 해상도가 나오게 되는 것이다. 원본 이미지의 해상도를 복구 하기 위하여 즉, upsample을 하기 위하여 filter에 zero를 추가하여 sparse한 filter를 얻는 방식의 hole algorithm을 사용하였다. 따라서 마지막 3개의 convolutional은 2배가 되었고 first fully connected layer는 4배가 되었다. 이때 필터를 그대로 유지하면서 각각 입력 스트라이드 2 또는 4 픽셀을 사용하여 적용되는 특징 맵을 드문드문 샘플링함으로써 이 방법을 더 효율적으로 구현할 수 있게 된다. [그림 2] Atrous Convoltion 이때 나타나는 hole algorithm (atrous convolution)이란 필터 중간 중간에 0을 채워 넣어서 학습해야할 파라미터 수는 유지하면서 보다 넓은 영역(Receptive Field)을 참조하게끔 하는 방법이다. Receptive Field(수용 영역): 특징 추출에 사용되는 한 개의 뉴런이 받아들일 수 있는 입력 데이터의 영역
따라서 atrous convolution을 사용하면 Receptive Field가 넓어지며 localization이 완화되는 효과가 있다. 따라서 이런 atrous convolution을 사용하게 된다면 두가지의 이점이 발생한다.]]></description>
</item>
<item>
    <title>[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)</title>
    <link>https://goodyoung.github.io/posts/paper/unet/</link>
    <pubDate>Fri, 17 May 2024 12:59:43 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/unet/</guid>
    <description><![CDATA[개요 Image Segmentation 문제를 해결하기 위해 제안된 대표적인 딥러닝 모델인 UNet에 대한 논문 리뷰를 해보려고 한다. Abstract 딥러닝 네트워크에선 성공적인 훈련을 위해선 수많은 annotated training samples들이 필요한데 이것들을 효율적으로 사용할 수 있는 강력한 data augmentation 방법에 대해서 소개한다. 그 다음으론 Unet의 구조에 대해서 간략한 소개가 나온다. Unet의 구조는 context들을 포착할 수 있는 contracting path와 localization을 정확하게 해주는 expanding path가 서로 대칭적으로 이루어져 있다는 것을 알려준다. 그리고 또한 Unet은 적은 양의 image로 학습을 할 수 있고 ISBI challenge에서 그 전 모델보다 더 좋은 성능을 보여주었다. 이런 Unet에 대해서 좀 더 자세히 Introduction의 파트에서 설명하게 될 것이다. Introduction 본 논문에서는 지난 2년 간 deep convolutional net이 성공을 이룬 이유는 Krizhevsky의 연구(Alex Net) 때문이라고 설명했다. 보통 convolutional network의 사용은 classification task(하나의 이미지를 single class label) 였지만 biomedical image task는 localization(class label이 픽셀 단위로 분류) 이 필요했다. 또한 biomedical tasks에서는 수많은 양의 dataset들을 구하기 어려운 한계도 있었다. [그림 1] sliding-window 따라서 위 그림 처럼 이전에 sliding-window 방식으로 픽셀의 주변 patch가 한 단위가 되어 pixel predict를 하여 localization을 수행하는 방법이 있었지만 두가지의 단점이 있었다. 모든 patch 별로 연산을 하므로 시간이 오래 걸린다. 수많은 중복된 패치들이 많다. patch 크기가 크면 max-pooling layer가 많아져서 localization accuracy가 떨어지고 반대로 patch 크기가 작아지면 littel context를 하여서 localization accuracy와 use of context 간의 trade off가 있다. 따라서 이러한 단점들을 해결하기 위해 Fully convolutional network의 구조를 변형하여 더욱 elegant한 구조로 모델을 만들었다. 적은 데이터셋으로도 work가 가능하고 더욱 더 정밀한 segmentation이 가능해짐 [그림 2] unet main idea Unet의 main idea는 contracting layer(downsampling) 에서 pooling operator가 upsampling operator로 교체되는 연속적인 layer들을 보완하는 것이다. contracting path로 부터 high resolution feature들이 upsampled output과 합쳐진다. 그런 다음 연속적인 합성곱 층들이 더 precise한 output을 조합한다. output의 해상도를 증가시킨다. Upsampling part에서 더 많은 feature channel들을 가지고 있게 된다. 이는 context 정보가 high resolution으로 잘 전달이 되게 한 것이다. 따라서 contracting path와 expansive path가 U-shaped architecture가 된다. 또한 Unet은 fully connected layer를 사용하지 않고 오직 각각의 convolution의 필요한 부분만 사용을 하였다. 이것은 overlap-tile strategy으로 임의의 큰 이미지의 seamless segmentation을 가능하게 한다. 다른 방식은 GPU memory의 한계 때문에 거대한 이미지를 처리할 수 없었다. Sliding Window 방식과 유사하지만 1개의 픽셀만 classification을 하는 것이 아닌 패치 영역을 classification을 한다. 또한 전체 영역을 다 예측하는 것이 아닌 572의 입력을 받아 388 크기의 아웃풋을 생성한다. fully connected layer를 사용하지 않고 오직 convolution만 사용하여 임의의 이미지를 입력받아도 문제가 없다. 큰 이미지를 한정된 GPU에서 학습하게 된다. Border region을 예측하기 위해서 missing context는 input image를 mirroring 하여 extrapolated(나타나게 만들 수 없는 부분을 예측)로 예측이 된다. 위의 오른쪽 그림을 보면 노란색 박스 영역을 예측 하려면 파란색 박스 영역의 이미지(노란 부분보다 더 큰 크기)가 필요한데 이렇게 누락되는 부분은 mirroring의 전략을 가져간다. 실제로 오른쪽 그림의 왼쪽을 자세히 보면 테두리의 이미지가 원본 이미지랑 대칭임을 볼 수 있게 된다. 또한 그 다음 그림은 단계로 넘어가는 과정인데 이전에 예측에 사용했던 patch와 겹치기 때문에 overlap-tile이라고 불린다. 또 만약 572의 입력이 들어올 때 388로 예측을 하기 때문에 외각 부분은 예측할 수 없어서 이 때문에 mirroring 전략을 취하는 것이다. [그림 3] elastic deformation Unet은 little training data에서도 가능했다. 모델 학습 시 data augmentation을 image의 invariance(이미지 불변성)를 학습하는 elastic deformation방법을 적용하였다. deformation: 변형
image를 다양하게 변형하여 augmentation을 하는 방법이다.]]></description>
</item>
<item>
    <title>[CS231n] 02.Image Classification</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture2/</link>
    <pubDate>Tue, 14 May 2024 21:32:06 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture2/</guid>
    <description><![CDATA[개요 CS231n을 공부하면서 정리를 위해 글을 작성해보려고 한다. 1강의 내용은 컴퓨터 비전에 대해 전반적인 역사, 이 course를 통해 얻게 될 내용들에 대해서 소개를 해주었다. 따라서 따로 정리할 것은 없어서 정리하지는 않았다. The Problem 컴퓨터는 사람과 달리 이미지를 숫자로 인식을 한다. 따라서 사람이 의미하는 것과 컴퓨터가 이해하는 것에는 차이(Semantic Gap)가 있기 마련이다. 또한 컴퓨터가 image를 인식할 때 여러 문제(problem)들이 발생하게 된다. Viewpoint variation (카메라의 움직임) Illumination (색상 차이) Deformation (다양한 모습) Occlusion (부분적으로 물체 보이는 현상) Background Clutter (배경 혼란 현상) Intraclass variation (다양한 종류) 따라서 우리는 이런 problem들을 극복할 수 있는 robust하고 확장성이 뛰어난 모델을 만들어야 한다. 이런 모델들을 만들기 위해서 Data-Driven Approach(하나의 접근 방식)를 제안한다. Data-Driven Approach에 대해서 아래에 설명을 작성했다.
image와 label들의 dataset을 모은다. 간단한 분류기를 machine learning에 의해 훈련킨다. (함수에서 train) 새로운 이미지를 가지고 분류기를 평가한다. (함수에서 predict) Nearest Neighbor 이 방법은 학습 데이터와 새로운 이미지들을 비교해서 학습 데이터 중에서 가장 유사한 이미지로 레이블을 예측하는 방법이다. Nearest Neighbor에서 이때 유사한 이미지로 판단하기 위해서 L1(manhattan) distance가 있다. L1 distance는 새로운 이미지의 pixel에 학습 데이터를 뺀 절대값들의 합을 구하여 그 합들이 가장 최소로 나온 이미지를 유사하다고 판단하는 방법이다. [그림 1] Nearest Neighbor 위 그림과 함께 Nearest Neighbor의 과정을 보자면 train 과정에서 모든 훈련 데이터를 저장한다. 그리고 predict 과정에서 모든 훈련 데이터를 비교하여 가장 유사한 훈련 데이터 image를 찾게 된다. 이 과정에서 train: O(1), predict: O(N)(# of iunput)의 시간 복잡도를 가지게 되는데 이것은 잘못되었다고 표현을 한다. 왜냐하면 우리는 보통 분류기가 빠르게 예측하기(predict)를 원하지만 Nearest Neighbor에서는 그것이 반대가 되어있는 것을 알 수 있게 된다. 또한 Nearest Neighbor알고리즘은 가장 가까운 이웃만을 보기 때문에 위와 같은 초록색이 대부분인 영역에서 노란색을 예측하는 문제가 발생 할 수 있다. 따라서 K-Nearest Neighbor알고리즘이 나오게 된다. K-Nearest Neighbor [그림 2] K-Nearest Neighbor 그 다음으로 가까운 Neighbor을 K개의 만큼 찾고, 그것끼리 다수결의 결정으로 예측을 하는 K-Nearest Neighbor방법이 있다. 또 이때 유사도를 결정할 때 L1 distance가 아닌 L2(Euclidean) distance의 계산식이 있다. 이것은 L1과 달리 좌표계가 무엇이든 상관이 없지만 L1의 경우에는 좌표계에 따라 계산값이 변할 수 있다. 각각 어떤것이 좋은지는 데이터의 성격에 따라 다르다 input 데이터가 각각의 항목이 중요한 성격(salary, 성별, 연봉 등)을 가진다면 L1이 적당할 수 있지만 어떤 역할인지 상관이 없으면 L2가 더 좋을 수 있다. 위 그림 2를 보게 되면 K가 커질 수록 좀 더 경계들이 부드러워지는 모습을 볼 수 있게 된다. 그렇다면 K가 무조건 커지면 좋은 것이냐? 그건 또 아니다. 이때 중요한 것이 적절한 K를 결정하는 것이 중요하다. 이런 K같은 요소들을 Hyperparameter이라고 부르고 다음으로 이런 적절한 Hyperparameter을 선택하는 방법에 대해서 설명하겠다. Hyperparameter 이런 Hyperparameter는 직접 시도해보고 가장 좋은 값을 찾는 것이 정답이다. [그림 3] Hyperparameter Setting idea #1: 학습데이터만을 학습하여 하이퍼 파라미터를 선택
idea #2: train한 모델을 바탕으로 Test set에서만 잘 동작하는 하이퍼 파라미터를 선택
idea #3: train set으로 모델을 학습, validation set으로 가장 좋았던 하이퍼 파라미터 선택, validation set에서 고른 하이퍼 파라미터를 바탕으로 test set을 test시작
따라서 이런 적절한 Hyperparameter가 무엇인지 결정할 때 위 3개의 방법 중 idea #3방법이 제일 좋다. 왜냐하면 idea1,2는 unseen data에 대한 정확도가 떨어진다. 결국 idea #2도 새로운 data에 약한 특성을 보인다. 예를 들어, idea #2일 때 K값을 바꿔가면서 이 중 가장 높은 성능을 지닌 K를 정한다고 했을 때 이는 test데이터에 좋은 성능을 보이는 K값인 것이다.]]></description>
</item>
</channel>
</rss>
