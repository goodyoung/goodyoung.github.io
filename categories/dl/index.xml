<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>DL - Category - Good Young</title>
        <link>https://goodyoung.github.io/categories/dl/</link>
        <description>DL - Category - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 02 Sep 2024 14:52:43 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/categories/dl/" rel="self" type="application/rss+xml" /><item>
    <title>[Paper Review]Generative Adversarial Nets(GAN)</title>
    <link>https://goodyoung.github.io/posts/paper/gan/</link>
    <pubDate>Mon, 02 Sep 2024 14:52:43 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/gan/</guid>
    <description><![CDATA[개요 VAE논문 다음으로 Generative분야에서 기초가 되는 논문인 GAN에 관한 리뷰를 할 것이다. Introduction 다양한 data환경에서 확률 분포를 잘 표현할 수 있는 풍부하고 계층적인 모델을 발견하는 것이 딥러닝의 잠재력이다. 딥러닝의 두드러진 성공 사례는 풍부한 감각 input을 class label로 매핑하는 판별 모델이다. 이런 성공은 backpropagation과 dropout의 algorithm, gradient를 가진 조각별 선형적 유닛들(ReLU)에 기반을 두고 있다. 그동안 Deep generative model은 큰 영향을 끼치진 못했다. Likelihood에 대한 추정과 관련된 전략에서 발생하는 계산을 근사화하는 것이 어렵고, 조각별 선형적 유닛들을 활용하여 generative에서 활용하는 것도 어렵기 때문이다. 따라서 본 논문은 이런 어려움들을 피할 수 있는 추정 절차를 제안한다. 제안된 adversarial nets은 generative model은 discriminative model과 맞서게 된다. Discriminative model은 model 분포에서 나온 것인지 data 분포에서 나온 것인지 구분하는 방법을 학습한다. Generative model은 위조자와 비슷하게 생각할 수 있다. 이는 fake 지폐를 만들어 경찰의 탐지 없이 사용하려고 한다. 반면 discriminative model은 경찰에 비유할 수 있다. 이런 fake 지폐를 탐지하려고 한다. 이런 상황 속에서 두개의 model이 자신들의 방법을 개선하려고 한다. 결국 위조지폐가 진짜 지폐와 구별되지 않게 되게 된다. 이 framework는 다양한 모델과 최적화 방법에 대해서 특정한 training algorithm을 도출할 수 있다. Generative 모델이 MLP을 통해 random noise를 통과시켜 sample을 생성하는 특수한 경우를 탐구합니다. 이런 특수한 경우를 적대적 신경망(Adversarial Nets)이라고 부른다. Generative, discriminative model 둘 다 backpropagation을 통하여 업데이트를 하며 generative model을 통하여 sampling을 할 땐 forward만 수행한다. Adversarial nets 이젠 Adversarial nets에 대한 설명을 할 것 이다. Adversarial modeling framework는 both multilayer perceptron 일 때 곧바로 적용할 수 있다. Data x에 $p_g$ 분포를 배우기 위하여 사전에 noise 변수 $p_z(z)$를 선언한다. 그 후 $G(z;\theta_g)$의 data space에 매핑을 하게 된다. 이 때 $G$는 파라미터 $\theta_g$를 지닌 미분 가능한 함수이다. 또한 두번째 multilayer perceptron이고 output이 하나의 scalar가 나오는 $D(x;\theta_d)$가 있게 된다. 이때 $D(x)$는 $p_g$분포와는 다른 $x$로부터 나왔을 확률을 representation한다. 이때 나온 output(single scalar)이 1: real ~ 0: fake일 확률을 나타낸다. 본 논문에서 나오는 훈련은 $D$는 훈련 샘플과 $G$에서 나온 샘플 (fake image) 모두에 대해 올바른 label(real or fake)을 할당하는 확률을 최대화하도록 훈련이 된다. 또 동시에 $G$를 $log(1-D(G(z))$ 를 사용하여 최소화 하는 방식으로 훈련을 하게 된다. 위의 내용을 총 정리한 $V(G,D)$ 의 목적 함수(Object Function)를 나타내면 아래의 식이다. $$\min_G \max_D V(D, G) = E_{x \sim p_{\text{data}}(x)} [\log D(x)] + E_{z \sim p_z(z)} [\log(1 - D(G(z)))]$$
위의 Adversarial net을 나타내는 목적 함수를 좀 더 살펴 보도록 할 것 이다. $\min_G \max_D V(D, G)$ 의 부분을 먼저 살펴보자면 $V(D, G)$ 식에서 $G$ 는 낮추고자 하고 $D$ 는 높이고자 하는 것을 알 수 있다. $D$의 목표는 진짜 데이터에 대해 높은 확률을 부여하고 가짜 데이터에 대해 낮은 값을 부여하도록 하는 역할이기 때문에 최대화를 목표로 한다. $G$의 목표는 생성된 데이터가 $D$에게 진짜 데이터 처럼 보이게 하는 역할이기 때문에 최소화를 목표로 한다. $E_{x \sim p_{\text{data}}(x)} [\log D(x)]$ 은 원본 데이터 $p_\text{data}(x)$에서 한개의 데이터인 $x$를 sampling을 하여 그 $x$를 $D$에 넣은 값에 $log$를 취한 값의 기대값을 나타낸다. 따라서 앞서 13번을 보면 사전에 noise 변수인 $p_z(z)$ 를 선언한다고 나와 있다. 따라서 $E_{z \sim p_z(z)} [\log(1 - D(G(z)))]$ 의 식은 하나의 noise 분포 $p_z(z)$ 에서 한 값을 sampling하여 그 $x$ 를 생성자 $G$ 에 넣고 가짜 이미지를 만든 다음에 $D$에 넣고 $1-D$의 형태로 만든 값에 $log$를 취한 값의 기대값을 나타낸다. 그래서 이런 두 항을 $D$의 관점에서 봤을 때 maximize하기 때문에 원본 데이터(x)에 대해서는 real(1)을 찾을 수 있도록 하고 반면에 가짜 이미지(z)가 들어왔을 때는 그 이미지가 fake(0)인지 분류할 수 있게 가능하게 한다.]]></description>
</item>
<item>
    <title>[Paper Review]Multimodal Machine Learning: A Survey and Taxonomy</title>
    <link>https://goodyoung.github.io/posts/paper/multimodal-survey/</link>
    <pubDate>Thu, 29 Aug 2024 18:37:03 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/multimodal-survey/</guid>
    <description><![CDATA[개요 Multi-Modal Learning에 관련한 서베이 논문을 리뷰해보려고 한다. Introduction 세상을 둘러싼 환경은 여러 modality를 포함하고 있다. 사람들은 이러한 modality를 sensory modality(vision or touch)와 연결을 짓는다.
본 논문은 natural language, visual, vocal signal에 관해서 중점을 두어 설명을 한다.
Multi modal을 AI에 사용하기 위해서는 multimodal message에 대해 여러 정보(multiple modalities)를 연결시킬줄 알아야 한다.
또한 여러 데이터 간 데이터의 이질성으로 인하여 Multimodal machine learning에서는 여러 해결해야할 문제가 있는데 본 논문에서는 5개를 제시한다.
첫 번째는 Representation이다. 이는 multimodal data를 어떻게 잘 요약하고 표현을 할 지에 대한 문제이다.
두 번째는 Translation이다. 이는 하나의 modality에서 다른 하나의 modality로 어떻게 mapping(translate)을 할 지에 대한 문제이다.
이미지에 대한 올바른 방식의 해석이 있어도 단 하나의 완벽한 해석은 존재하지 않는다. 세 번째는 Alignment이다. 이는 여러개의 modality로부터 요소 사이의 관계들을 정렬하여 식별하는 것이다. 서로 다른 modality 간의 유사성을 측정하고, 가능한 장거리 의존성 및 모호성을 처리해야 한다. 네 번째는 Fusion이다. 여러 modality의 추론 결과를 합치는 것이다. 다른 modality로부터 오는 정보들은 다양한 예측 결과를 가져올 수 있다. 다섯 번째는 Co-learning이다. Modality간에 knowledge를 전달하는 것이다. 이것은 한 modality의 data가 부족할 때 유용하다. 위 표는 multi-modal을 적용하는 application에서 위에 설명한 5가지의 challenge의 포함 여부를 나타낸 것이다.
위 표를 확인하며 multi-modal로 활용할 수 있는 task가 무엇인지도 함께 확인하면 될 것 같다.
Representation 이젠 5가지의 challenge에 대해서 설명할 것이다. 먼저 첫 번째로 설명하는 Multimodal Representation에 대한 설명이다.
여러 modality를 representing하는 것은 다양한 어려움이 존재한다.
이질성인 데이터에서 어떻게 섞을 것인지 다른 종류의 noise를 어떻게 처리할 것인지 missing data를 어떻게 처리할 것인지 좋은 representation하는 방법은 model의 성능을 중요하다. (최근 speech recognition, visual object detection 등의 성능 향상 사례가 있다.)
또한 좋은 representation을 위한 몇 가지 속성으로 부드러움(smoothness), 시간적 및 공간적 일관성(temporal and spatial coherence), 희소성(sparsity), 자연스러운 클러스터링(natural clustering) 등이 있다.
Multi modal representation을 위한 여러 속성들이 있다. 그것은 표현 공간에서의 유사성이다. 이는 해당 개념들의 유사성을 반영해야 하며, 일부 modality가 없어도 쉽게 표현을 얻을 수 있어야 하고, 관찰된 modality를 바탕으로 누락된 모달리티를 채울 수 있어야 합니다.
이전까지 단일 modality에 대한 연구는 광범위 하게 연구되어 왔다. 이미지에 관련한 data는 SIFT기법 에서 CNN기법으로 연구되어 왔고 audio domain은 음향적 특징들이 deep neural network에서 rnn으로 연구되어 왔다.
이런 와중 multi-modal에선 단일 modality에 대한 연구들을 단순히 concat하는 방법만 사용하고 있다. 이런 방법론들이 변화되고 있다.
따라서, 본 논문에선 joint와 coordinated라는 두 가지의 representation 방법을 소개한다.
joint는 각각의 modality를 같은 representation space에 결합하는 방식이다. 이는 아래와 같은 수식으로 표현할 수 있다. ($x_1, x_n$등은 각각 modality이다.)
$$ x_m = f(x_1,&hellip;,x_n)$$
coordinated는 각각의 modality를 각각 분리해서 처리하지만 similarity 규정을 사용해 coordinated space에 가져온다. 이는 아래와 같은 수식으로 표현할 수 있다. $$f(x_1) ~ g(x_2)$$
Joint Representation Joint Representation은 독립적인 modality 특징들을 concatenation을 한다고 생각하면 된다.
앞으로는 data의 representation 방법 중 가장 유명한 방법인 Neural network에서 Joint Representation을 하는 방법에 대해서 설명할 것이다.
Neuraul network을 사용해 multi modal representation을 구축하기 위해 각 modality는 여러 개의 개별 신경 계층으로 시작하고, 이후 이 modality들을 joint space에 투영하는 hidden layer가 따른다.
이렇게 joint된 representation 들은 hidden layer를 거치거나 예측에 직접적으로 사용을 한다.
이런 neural network에서 훈련을 할 때, 많은 label data가 필요하게 된다. 따라서 unsupervised data에서 autoencoder를 사용해 이러한 표현을 pre-training하는 것이 일반적이다.
하나의 예시로 denoising하는 여러개 autoencoder를 stack한 후 다른 autoencoder layer를 사용하여 fuse하게 된다.]]></description>
</item>
<item>
    <title>[Paper Review]Auto-Encoding Variational Bayes(VAE)</title>
    <link>https://goodyoung.github.io/posts/paper/vae/</link>
    <pubDate>Mon, 26 Aug 2024 18:21:20 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/vae/</guid>
    <description><![CDATA[개요 Generative분야에서 기초가 되는 논문인 VAE에 관한 리뷰를 할 것이다. Introduction 연속적인 latent variable(잠재변수)나 파라미터가 계산이 힘든 후방 확률 분포를 가지는 확률 모델을 어떻게 효과적으로 추론하고 훈련을 시키는 방법에 대해 설명이 나온다. 디코더만으로 학습 및 훈련을 진행할 수 없어서 인코더를 가져온 것이다. 해당 내용은 Method 부분에서 자세하게 다룰 것이다. 기존의 방법인 Variational Bayesian(VB) 방법은 계산하기 힘든 사후 확률을 적절하게 최적화 한다. 하지만 이 방법은 후방 확률에 대한 기대값의 분석적 해결책을 요구하며, 일반적인 경우에도 계산이 어렵다. 그래서 본 논문에서 기존 변동의 lower bound의 reparameterization가 어떻게 간단하고 다른 비편향적인 estimator의 lower bound를 만드는지 알려준다. Stochastic Gradient Variational Bayes(SGVB) estimator가 posterior inference(후방 추론)을 잘 하도록 한다. 이는 확률적 경사 하강법을 사용하여 곧바로 최적화도 된다. 독립적이고 동일한 분포를 띄는 이 latent variable에서 본 논문은 Auto-Encoding VB 알고리즘을 제안한다. SGVB estimator를 사용하여 추론과 learning을 neural network인 recognition model에 효율적으로 적용한다. 비싼 추론 없이 모델 param을 효과적으로 배우는 방법을 사용한다. 해당 estimator를 사용하면 효과적인 후방 추론을 수행하게 하여 최적화가 된다. 위처럼 neural network에서 recognition으로 사용하게 되면 variational auto-encoder(VAE)가 된다. Method 어떠한 dataset이 있을 때 실제 파라미터 $\theta$와 latent variable $z^\text{(i)}$ 는 우리가 알 수 없다. 따라서 본 논문에선 Intractability하고 large dataset에도 잘 최적화 할 수 있는 알고리즘을 개발을 한다. 그렇게 하기 위하여 세가지 문제를 설명한다. 파라미터 $\theta$ 에 대한 효율적인 근사를 하는 문제 관찰된 값 $x$와 $\theta$를 기반으로 latent variable $z$를 효율적인 사후 추론 하는 문제 $x$의 효율적인 근사 추론하는 문제 이미지 노이즈 제거(denoising), 이미지 복원, 초해상도 등 가능 이런 문제를 해결하기 위하여 recognition model ($q_{\phi}(z \mid x)$)을 도입하였다. 이는 $p_{\theta}(x \mid z)$를 가장 근사화 하는 네트워크 이다. 이 네트워크의 목표는 decoder에서 training data(x)의 likelihood를 최대화 하고 싶은 것이다. 따라서 해당 목표를 수학 식으로 표현하자면 아래와 같다. 이는 x가 나올 수 있는 확률 ($p_{\theta}(x)$)이 가장 커지는 distribution을 찾는 것으로 생각하면 된다. $$p_{\theta}(x) = \int p_{\theta}(z) , p_{\theta}(x \mid z) \ dz $$
이때 위 식은 아래의 식에서 나왔다. $$\frac{P(x, z)}{P_{\theta}(z)} = p(x \mid z)$$
따라서 식을 정리하면 아래와 같다. $$p_{\theta}(x) = \int P(x, z) \ dz $$
위 식을 해석하면 x와 z가 동시에 일어날 확률을 모든 z에 대해서 적분하면 그것이 x의 확률이 되는 것이다. 하지만 문제는 모든 z에 대해서 적분을 하기가 intractability하는 것이다. 이 문제를 해결하기 위하여 확률적 인코더인 $q_{\phi}(z \mid x)$ (encoder)가 나오게 되었다. Evidence LowerBOund (ELBO) 다음은 data likelihood를 어떻게 최적화 하는지에 대해서 설명을 할 것이다. 먼저 우리의 목적인 $p_{\theta}(x^i)$의 식을 풀어보자면 아래와 같다. $$\log p_{\theta}(x^{(i)}) = E_{z \sim q_{\phi}(z \mid x^{(i)})} \left[ \log p_{\theta}(x^{(i)}) \right]$$
적분에서 기대값으로 변경하기 위하여 log를 씌우고 Decoder에서 $z$가 $q_{\phi}(z \mid x^{(i)})$ (encoder)의 분포를 따를 때를 의미한다. 위 식을 조금 더 분해하면 아래와 같다. 베이즈 정리를 사용하여 식을 변형하고 상수를 곱하였다. $$= E_{z} \left[ \log \frac{p_{\theta}(x^{(i)} \mid z) p_{\theta}(z)}{p_{\theta}(z \mid x^{(i)})} \right] (\therefore p(z \mid x^{(i)}) = \frac{p(x^{(i)} \mid z) p(z)}{p(x^{(i)})})$$ $$= E_{z} \left[ \log \frac{p_{\theta}(x^{(i)} \mid z) p_{\theta}(z)}{p_{\theta}(z \mid x^{(i)})} \frac{q_{\phi}(z \mid x^{(i)})}{q_{\phi}(z \mid x^{(i)})} \right] $$
위 식에서 log 변환을 하여 아래와 같은 식을 만들게 된다. $$ = E_{z} \left[ \log p_{\theta}(x^{(i)} \mid z) \right] - E_{z} \left[ \log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z)} \right] + E_{z} \left[ \log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z \mid x^{(i)})} \right] $$
다음의 형태를 KL-divergence의 형태로 변경을 하게 되면 최종적인 아래의 식이 나오게 된다.]]></description>
</item>
<item>
    <title>[CS231n] 10. Recurrent Neural Networks</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture10/</link>
    <pubDate>Wed, 21 Aug 2024 16:55:43 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture10/</guid>
    <description><![CDATA[개요 CS231n의 10강에 대한 내용을 정리 할 것이다. Recurrent Neural Networks 이번 강의에서는 Recurrent Neural Networks에 대해서 설명을 한다. 지금까지의 networks는 one-to-one의 구조를 띈 모델이지만 위 그림과 같이 여러 구조를 띌 수 있게 된다. 따라서 각 구조 별로 다양한 task로 활용할 수 있다. one-to-many image captioning many-to-one sentiment classification many-to-many machine translation videio classification on frame level $$h_t = f_W(h_\text{t-1},x_t)$$
다음은 RNN의 기본 수식이다. 이전 상태($h_\text{t-1}$, hidden state)와 입력 값($x_t$)을 계산하여 new state로 나오게 된다. 더 자세한 수식은 위 그림과 같다. 위의 수식을 기반으로 RNN의 Computational Graph로 표현을 하게 된다면 위 그림과 같다. 보면 각 스텝에서 다 같은 W를 사용하는 것을 확인할 수 있게 된다. 위 그림은 RNN에서 Many-to-Many일 때 Loss 계산 까지 표현되는 그림인데 각각의 time-step마다 local gradient를 계산하게 되고 그것이 합쳐진 L까지 가서 upstream gradient로 내려오게 된다. 다음은 Many-to-One일 때 인데 최종 output인 y가 summarize context를 모든 time-step에 따른 하나의 결과를 담고 있다. 다음은 One-to-Many인데 이것은 fix size 형태로 입력이 되고 variable size인 output을 출력하는 형태이다. 이러한 Many-to-One구조와 One-to-Many를 섞은 구조인 Seq2Seq구조도 있다. Encoder에서 다양한 크기의 input이 들어오고 그것의 전체를 요약하는 $h_t$가 있게 된다. Encoder에서 나온 vector를 Decoder로 넘겨 다양하게 출력을 할 수 있게 한다. 이때 입력, 출력의 크기를 조절하기 위하여 입력 토큰, 출력 토큰을 지정하여 모델이 처음과 끝을 알 수 있게 한다. 이 구조는 번역을 하는 task에서 사용할 수 있게 된다. (영어 -&gt; franch) 이제는 하나의 문자를 입력 받고 다음으로 올 문자를 예측하는 모델을 설명할 것이다. 문자를 모델에 입력하기 위하여 하나의 벡터로 넣기 위하여 embedding 작업을 거치는데 embedding 방법 중 one-hot-encoding의 방법을 사용하였다. 그 후 그것에 가중치 행렬($W_\text{xh}$)을 곱하여 hidden layer로 들어가고 또 그게 다시 output layer로 출력이 된다. 여기서 hello라는 결과를 보여야 하기 때문에 맨 처음에 e라는 출력값을 가진다. 하지만 이 경우 잘못 예측을 하여 o로 예측을 한 경우이다. 다음으로 e가 input으로 들어가고 새로운 hidden state를 만든다. 이러한 과정을 계속 거치면 이 모델은 이전의 문장들의 문맥을 참고하여 다음 문자가 무엇일지 예측을 할 수 있게 되는 것이다. 그렇다면 이 모델의 Test time에서는 어떻게 작동을 하고 있는지 보여준다. 이 모델을 잘 활용을 하기 위해서는 sampling이라는 샘플링 기법을 활용하는 것이다. 위 그림은 모델이 output layer를 거쳐 softmax를 취하여 확률 분포를 사용하여 sampling을 한 과정을 나타낸다. 해당 경우에 e는 뽑힐 확률이 굉장히 작았음에도 불구하고 운이 좋게 e가 샘플링 되었다. 또한 이 샘플링으로 나온 e를 다음 input으로 넣어주고 반복 과정을 거쳐 test를 하게 된다. 가장 높은 스코어를 선택하지 않고 샘플링을 하는 이유는 모델에서 다양성을 얻을 수 있게되기 때문이다. 항상 h를 첫 input으로 놓는다고 할 때, 확률분포로 샘플링을 하게 된다면 그럴듯한 다양한 문장들을 출력할 수 있어 이것이 출력의 다양성으로 이어진다. 다음은 RNN에서 역전파를 사용했을 때를 나타낸다. 각 스텝마다의 loss를 계산해서 최종적인 output을 나타내게 되는데 시퀀스가 너무 길게 되면 문제가 생길수도 있다고 한다. 길면 메모리 사용량도 많고 학습이 너무 느릴 것이다. data point의 모든 요소에 대한 기울기를 계산하는 것은 엄청난 비용을 초래한다. 따라서 이러한 문제를 해결하기 위하여 나온 방법은 truncated backpropagation이다. 입력이 너무 길다고 하더라도 train할 때 한 스텝을 일정 단위로 자르고 단계를 진행하기 때문에 위의 문제를 해결할 수 있다. 이전에 계산한 hidden state는 계속 유지하고 반복해야 한다. backpropagation은 현재 배치만큼만 진행해야 한다. Image Captioning 다음은 Image captioning에 관한 설명이다. Image captioning을 하기 위해선 CNN에 Image를 넣고 CNN의 result vector를 다시 RNN에 넣어서 caption에 사용 될 문자를 하나씩 생성해가는 방식이다.]]></description>
</item>
<item>
    <title>[Paper Review]Training data-efficient image transformers &amp; distillation through attention(DeiT)</title>
    <link>https://goodyoung.github.io/posts/paper/deit/</link>
    <pubDate>Sun, 18 Aug 2024 21:02:30 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/deit/</guid>
    <description><![CDATA[개요 ViT의 단점을 보완하고자 나온 논문인 DeiT에 대한 리뷰를 할 것이다. Abstract Convolution Neural Networks은 image classification에서 대규모 훈련 세트를 같이 사용한 주요 설계 방식이었다. 점점 NLP에서도 attention 기반 모델들의 사용이 많아지고 있었다. 따라서 Vision분야에서도 attention 기반과 CNN을 합치는 하이브리드 아키텍쳐가 있었다. 그 결과 ViT가 나오게 되었는데 이는 3억 개의 사설 라벨 이미지 데이터셋으로 학습을 했다. ViT의 논문에서는 Do not generalize well when trained on insufficient amounts of data으로 설명을 하였고 이를 위해선 방대한 컴퓨팅 자원이 필요했다. 따라서 본 논문에서는 ViT보다 훨씬 적은 데이터로 학습된 Data-efficient image Transformer(DeiT) 모델을 제시한다. 적은 데이터로 학습을 하기 위해서 Knowledge Distillation의 방법을 사용하며 그것을 이루기 위하여 token-based 방식을 취한다. 본 논문에 나온 연구를 요약하자면 다음과 같다. 이번 DeiT는 CNN이 없는 neural network가 외부 데이터 없이 ImageNet의 훈련만으로 SOTA들과 경쟁을 할 수 있게 된다. Distillation token의 기반으로 지식 증류를 하여 기존 증류 방식보다 현저히 더 우수한 성능을 보여준다. 전이 학습을 할 때 경쟁력을 유지하는 성능을 보여 일반화가 잘 되어있다는 것을 알 수 있게 된다. Vision transformer: overview 다음 내용은 본 논문에서 기반을 둔 아키텍쳐인 ViT에 관한 간략한 요약에 관한 내용들이다. 이 내용들은 ViT 논문 리뷰에서 정리를 해두었으니 자세한 내용은 해당 리뷰를 가서 확인 하면 될 것 같다. ViT의 Multi-head Self-Attention, Transformer Block 구성, Class token에 대한 설명이 나와 있다. 또한 더 낮은 해상도로 훈련하고 더 높은 해상도에서 네트워크를 fine-tuning하는 것이 바람직하다고 나온다. 이 방법은 전체 훈련 속도를 높이고 데이터 증강 방식에서 정확도를 향상시킨다. 해상도를 높이고 훈련을 다시 할 때 패치 크기는 동일하게 유지하므로 입력 패치의 개수는 변하지 않지만 위치 임베딩은 조정이 필요하다. 따라서 해상도를 변경할 때 위치 인코딩을 interpolate하는 방법을 사용한다. Distillation through attention 이번 내용은 Knowledge Distillation을 통해 ViT를 학습시키는 방법에 대해서 설명을 한다. 방법에 대해서 설명하기 앞서 Knowledge Distillation의 기본 개념인 Soft distillation과 Hard-label distillation에 대한 설명을 한다. $$L_{\text{global}} = (1 - \lambda) L_{\text{CE}}\left( \psi(Z_s), y \right) +\lambda \tau^2 \text{KL}\left( \psi\left(\frac{Z_s}{\tau}\right), \psi\left(\frac{Z_t}{\tau}\right) \right)$$
Sort distillation은 위 식과 같이 Cross entropy loss를 사용하고 교사 모델 loss($Z_t$)와 학생 모델 loss($Z_s$)를 온도 $\tau$를 사용하여 증류를 한다. $$L_{\text{global}}^{\text{hardDistill}} = \frac{1}{2} L_{\text{CE}}\left( \psi(Z_s), y \right) + \frac{1}{2} L_{\text{CE}}\left( \psi(Z_s), y_t \right)$$
또한 Hard distillation은 실제 레이블로 변형한 형태로 argmax 함수로 표현을 하는데 이는 특정 이미지에 대해 교사와 관련된 하드 라벨은 특정 데이터 augmentation 방법에 따라 변경이 될 수 있다고 한다. 또한 Hard distillation에 Label Smoothing 기법을 사용하여 훈련 데이터의 레이블에 약간의 불확실성을 추가를 하여 Soft distillation으로 변형할 수도 있다고 한다. Label Smoothing을 적용하면 원래 레이블이 $[1,0,0]$이라면 $[ 1 - \epsilon, \frac{\epsilon}{K - 1}, \frac{\epsilon}{K - 1}]$ 으로 Soft distillation이 가능하게 변경될 수 있게 된다. 이를 사용하면 과적합 방지,일반화 성능 향상, 손실 함수 안정화의 장점을 얻게 된다. 본 논문에서는 $\epsilon = 0.1$로 설정하여 훈련을 진행하였다고 한다. Knowledge Distillation 추가적으로 Knowledge Distillation에 대해서 알아보자면 딥러닝에서 지식 증류는 큰 모델(Teacher Network)로부터 증류한 지식을 작은 모델(Student Network)로 transfer하는 일련의 과정이라고 할 수 있다. 이는 방대한 양의 데이터로 학습한 모델을 model deployment의 관점으로 봤을 때 더 가벼운 모델을 만들기 위하여 복잡한 모델의 일반화 능력을 가벼운 모델에게 transfer하는 것을 말한다. Transfer하는 방법은 위 그림의 오른쪽 윗 부분이 있는데 Teacher와 Student의 output을 loss fn으로 계산하고 있다. 또한 오른쪽 아래 부분은 Student의 hard prediction을 Ground truth와 loss fn을 계산하는 구조가 보인다.]]></description>
</item>
<item>
    <title>[CS231n] 09.CNN Architectures</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture9/</link>
    <pubDate>Sun, 11 Aug 2024 22:58:52 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture9/</guid>
    <description><![CDATA[개요 CS231n의 9강에 대한 내용을 정리 할 것이다. 9강에 나오는 CNN Architecture중에 GoogLeNet과 ResNet에 대해서 살펴볼 예정이다. GoogLeNet 이때부터 이제 Network를 깊게 효율적을 만들기 시작했다. Network가 깊어질수록 학습하는데 걸리는 컴퓨팅 시간이 많이 걸린다. 따라서 Network를 깊고 효율적으로 만들기 위해서 GoogLeNet에서는 Inception module을 추가하였다. Inception Module이란 로컬 네트워크의 한 유형이다. 이는 네트워크의 네트워크 (Network In Network(NiN))의 개념에 바탕을 두고 있다.
Inception Module은 위 그림과 같이 병렬로 서로 다른 크기의 filter를 병렬로 돌려 차원을 줄여주는 것이다. 서로 다른 크기의 filter가 있으므로 여러 특징을 추출하는 효과를 가진다. 위 그림은 naive Inception module이다. Filter들의 stride와 padding을 통하여 입력과 출력 차원을 일치시키고 depth를 높였다. 이 module의 문제는 computational complexity이다. Pooling layer가 depth를 유지하기 때문에 every layer에서 전체적인 depth가 깊어진다. 이를 해결하려고 boottleneck layer가 나온다. 33, 55 filter에 들어가기 전 1*1 filter를 사용하여 depth를 줄인다. 따라서 기존과 확연히 다른 연산을 수행하는 것을 확인할 수 있게 된다. 1*1 를 사용하면 정보 손실이 발생할 수 있지만 이러한 예측을 수행하는 경우 이들의 조합을 계산하여 추가적으로 비선형성을 도입하므로 이를 보완할 수 있다. 또한 GoogLeNet에서는 auxiliary classification output이 있다. To inject additional gradient at lower layers Gradient가 잘 전달이 되지 않는 깊은 Network에서 중간 layer도 도움이 된다. 깊은 네트워크 때문에 gradient vanishing 현상을 극복하려고 inject를 한다. 이는 메인 네트워크의 최종 손실과 함께 결합되어 최종 학습 과정에 기여하게 된다. ResNet 다음은 2015년에 ILSVRC에서 우승을 한 ResNet이다. 이는 revolution of Depth인 만큼 많이 깊은 network이다. 엄청 깊게 CNN을 쌓으면 더 나은 결과가 나오나 싶지만 결과는 아니였다. Train시 overfitting이 예상이 되어 오류가 아주 적을 것이라 예상을 했지만 그것의 문제가 아니다. 이는 모델 자체는 학습이 이뤄지고 있어서 vanishing gradient문제가 아닌 degradation problem이다. 이는 optimization problem이다. 적어도 깊은 layer는 shallower의 성능은 기본적으로 있어야 하지만 그 조차도 아니다. 깊은 layer는 shallower model보다 최적화 하기 어렵다. 이걸 해결하려고 residual connection이 나왔다. 이는 아주 간단하게 입력을 출력에 더하는 아이디어 이런 단순한 아이디어 이다. 모델이 학습을 할 때 $H(x)$를 하는 것이 아니라 x를 뺀 나머지(잔차 $F(x)$)를 학습을 하게 된다. 이를 더 자세히 이해하자면 학습이 많이 될수록 x는 점점 출력값 $H(x)$에 근접하게 된다. 이 residual connection에서는 x가 identity mapping이 되기 때문에 $H(x) = F(x) + x$에서 추가 학습량 F(x)는 점점 작아져서 최종적으로 0에 근접하는 최소값으로 수렴되어야 할 것이다. 이렇기 때문에 함수 전체를 학습하는 대신, 더 단순한 잔차만 학습하게 되므로 residual connection은 학습 과정이 더 빠르고 쉽고 깊은 네트워크에서 안정적이다. 이를 수식적인 이해를 한다면 아래와 같다. Residual connection이 있는 신경망에서 아래 같이 표현을 할 수 있다. $$y = F(x) + x$$
기본 신경망에서는 손실 함수 $L$은 체인 룰을 통해 gradient update를 하게 된다. $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$
Residual connection이 있을 때 기울기 전파는 아래와 같다.
$$y = F(x) + x$$
$$\frac{\partial y} {\partial x} = \frac{\partial (F(x) + x)}{\partial x}$$
$\frac{\partial x}{\partial x} = 1$ 이므로 다음과 같이 표현할 수 있다. $$\frac{\partial y}{\partial x} = \frac{\partial F(x)}{\partial x} + 1$$
결국, gradient전파는 다음과 같이 계산이 된다. $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot (\frac{\partial F(x)}{\partial x} + 1)$$
Residual connection이 없는 경우, $(\frac{\partial F(x)}{\partial x})$가 매우 작아지면 기울기 소실 문제가 발생할 수 있다. 하지만, Residual connection이 있는 경우, $(\frac{\partial F(x)}{\partial x} + 1)$의 형태로 계산되므로, $(\frac{\partial F(x)}{\partial x})$가 0에 가까워지더라도, 항상 1이 더해지기 때문에 기울기가 완전히 사라지지 않게 되어 vanishing gradient 현상도 해결이 될 수 있다.]]></description>
</item>
<item>
    <title>[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)</title>
    <link>https://goodyoung.github.io/posts/paper/vit/</link>
    <pubDate>Wed, 31 Jul 2024 19:28:04 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/vit/</guid>
    <description><![CDATA[개요 Vision분야에서 Transformer를 사용한 ViT에 관한 논문에 대한 리뷰를 할 것이다. Introduction Self-Attention기반 archtecture인 Transformer는 NLP에서 중요한 역할을 하고 있다. 이때의 주요 접근 방법은 아주 큰 text data로 학습 시킨 사전 훈련 모델을 fine tuning을 하는 것이다. Transformer의 계산 효율성과 확장성으로 인해 전레없는 크기의 모델을 훈련하는 것이 가능해졌다. Model과 dataset이 커져도 성능의 포화는 보이지 않는다. 하지만 Vision에서는 CNN 구조가 지배적이었다. NLP의 성공 후, attention을 사용하려는 여러 시도들이 있었지만 특수한 attention pattern의 사용 때문에 효과적으로 사용되진 않았다. CNN + self-attention의 구조 CNN을 완전히 대체하는 구조 따라서 이런 상황속에서 최소한의 수정으로 image를 직접적으로 Transformer에 넣는 실험을 했다. Image를 패치로 나누고 이 패치를 linear 한 embedding의 연속으로 나눈 후 이것을 Transformer에 바로 넣는 방식이다. 그렇게 되면 Image patches는 NLP의 관점으론 token(word)가 되는 것이다. 이 ViT를 ImageNet과 같은 mid-size dataset으로 학습을 했다. 이땐 ResNet보다 몇 퍼센트 낮은 정확도를 보였다. 이는 예상된 결과이다. 왜냐하면 기존 CNN은 inductive biases를 가지고 있어 적은 양의 데이터로도 translation equivariance, locality를 잘하여 일반화 성능이 좋다.
inductive biases: 모델이 학습된 데이터 외의 데이터에 대해 얼마나 잘 일반화할 수 있는지에 사용하는 가정
머신러닝의 최종 목표는 generalization, 즉 학습 데이터로 학습시킨 모델이 본 적 없는 데이터에 대해서도 예측(prediction, approximation)을 잘 해내는 것이다. 본 적 없는 상황을 예측하기 위해서는 학습된 가정 이외에 추가적인 가정이 필요한데, 이것이 바로 inductive bias이다. translation equivariance: 입력 데이터가 일정한 변환을 받을 때, 그 변환이 모델의 출력에도 동일하게 반영되는 성질
CNN의 합성곱 층에서는 이미지의 특정 패턴이 위치를 옮겨도 그 패턴을 감지하는 필터의 반응이 동일하게 이동한다. 즉, 입력 이미지의 패턴이 이동하면, 해당 패턴을 인식하는 뉴런의 활성화 위치도 동일하게 이동한다. 어떠한 사물이 들어 있는 이미지를 제공해줄 때 사물의 위치가 바뀌면 CNN과 같은 연산의 activation 위치 또한 바뀌게 된다. Locality: 이미지 내의 픽셀들이 인접한 다른 픽셀들과 더 밀접한 관계를 가지는 특성
그에 반해, Transformer는 inductive biases가 부족하여 데이터가 충분하지 않을 때 일반화가 되지 않는다. 그러나 더 큰 데이터 셋으로 훈련을 했을 경우 모델의 성능이 크게 향상이 된다. 따라서 대규모 훈련이 inductive bias를 능가하는 것을 확인을 하였다. Method 위 사진은 ViT의 전체적인 구성에 대한 그림이다. 본 논문에서는 최대한 가능한 원래의 Transformer구조를 따를려고 했다. 그렇게 되면 확장 가능한 NLP Transformer 아키텍처와 그 효율적인 구현을 거의 그대로 사용할 수 있다는 장점이 있기 때문이다. Vision Transformer (ViT) ViT의 전체적인 구조를 설명하고 있다. 크게 이미지 입력 처리, 패치 임베딩, [class] 토큰 추가, position 임베딩, Transformer Encoder 구조 순으로 설명할 것이다. 먼저 이미지 입력 처리에서 입력 이미지는 $ H * W * C $ (H,W 이미지 높이와 너비, C: 채널 수) 크기를 가지지만 1D 시퀀스의 입력을 받는 Transformer의 구조에 맞게 변형이 필요하다. 따라서 $ H * W * C $ 에서 $ N * (P^2 * C) $ ($N = \frac{H*W}{P^2}$, $P^2$은 각 패치의 크기, C: 채널 수)로 변환을 해야한다. 또한 이 각 패치는 입력에 맞게 linear projection되어 고정된 크기 $D$의 벡터로 매핑이 되어 이 과정을 통해 입력 데이터를 학습 가능하게 변환한다. Linear projcetion: 입력 벡터에 선형 변환을 적용하여 다른 차원의 벡터로 변환하는 과정. $y = W * x + b$ 일 때 $y$는 출력 백터로 고정된 크기의 $D$차원의 벡터이고, W는 가중치 행렬로 학습 가능한 행렬이다. 이미지 패치가 $16 * 16 * 3$일 때 이 패치를 펼쳐서 $16 * 16 * 3 = 768$크기의 벡터로 나타낼 수 있다 이때 Linear projection을 통해 $D = 512$ 크기의 벡터로 변환하려면, $512 * 768$크기의 학습 가능한 가중치 행렬 $W$를 사용해 Linear projection을 사용하여 다른 차원의 벡터로 변환하는 것이다.]]></description>
</item>
<item>
    <title>[Paper Review]Attention Is All You Need</title>
    <link>https://goodyoung.github.io/posts/paper/transformer/</link>
    <pubDate>Wed, 24 Jul 2024 11:32:59 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/transformer/</guid>
    <description><![CDATA[개요 요즘 모든 분야에서 자주 사용되고 있는 아키텍쳐인 Transformer에 대한 논문 리뷰를 해보려고 한다. Abstract 기존의 sequence transduction model(sequence 간 변형이 이루어 지는)은 complex recurrent나 cnn기반으로 구성되어 있다. 그리고 좋은 성능을 내는 것이 attention 기반에 encoder &amp; decoder 기반으로 연결되어 있는 네트워크이다. 본 논문에서는 오로지 attention mechanism만 사용하는 Transformer를 제안한다. 즉, 순환이나 cnn의 연산을 사용하지 않는 네트워크이다. 이는 훈련시간 절감, 행렬곱을 이용하여 병렬적으로 수행하면서 번역 작업에 우수한 품질을 보여준다. English-German 번역 작업에 28.4 BLEU를 달성하였고 다른 task에서도 잘 작동한다. Introduction RNN, long-term memory, GRU이 language modeling, machine translation 문제에서 SOTA를 달성하고 있다. 많은 연구가 encoder-decoder의와 순환 language model의 성과를 달성하려고 하고 있다. 순환 모델의 연산 특성은 training sample 내에서 병렬화를 방해하며, 시퀀스 길이가 길어질수록 메모리 제약으로 인해 예제 간 배칭이 제한되므로 병렬화가 중요해집니다. 기존 RNN의 단점 따라서 기존의 RNN의 단점에 대해서 간략히 살펴 보겠다. 위 그림처럼 sequence에 포함되어 있는 순서 정보를 정렬하고 이것을 차례대로 hidden state의 값을 반복을 통하여 갱신하기 때문에 병렬적으로 수행하기에 어렵다는 사실을 알 수 있게 된다. 또한 input 단어가 많아지면 encoder의 마지막 부분의 출력인 context vector를 만들어야 하기 때문에 병목의 문제도 있다. 다시 논문 내용으로 넘어가면 Factorization trick과 conditional computation의 연구로 인해 이러한 문제점은 상당히 해결되고 있지만 여전히 sequential computation은 제약이다. Attention은 input과 output의 거리에 관여하지 않고 model의 종속성을 허락해준다. 이러한 attention mechanism은 주로 순환 신경망과 함께 사용이 되어 왔다. Attention RNN처럼 하나의 context vector가 소스 문장의 모든 정보를 가지고 있어야하는 단점을 해결하기 위해 나온 기법이 Attention이다. Attention은 매번 출력 단어를 만들어 낼 때마다 소스 문장의 출력 정보 중에서 어떤 정보가 가장 중요한지 가중치를 부여해서 출력 단어를 보다 효과적으로 생성할 수 있게 한다. 간단히 매번 소스 문장에서의 출력 전부를 decoder의 입력으로 받는 방식이다. 결론적으로 본 논문에서 제안하는 Transformer는 순환을 피하고 대신에 attention mechanism에 온전히 의존하여 input과 output 사이의 ``global dependencies를 이끈다. 이러한 구조는 병렬 처리에 특화되었다. Background 기존 연구들은 CNN을 사용하여 sequenrtial computation을 줄이는데 초점을 맞추었지만 이 방법은 다른 position 사이의 종속성을 배우기 쉽지 않다. 하지만 Multi-Head Attention의 방법을 사용하여 상수 시간으로 줄였다. 또한 기존 순환 end-to-end memory network는 attention mechanism을 사용했다. 하지만 Transformer에서는 RNN이나 CNN을 사용하지 않고 input output의 representation을 계산하는 전체의 self-attention을 사용한다. 여기서 나온 Self-Attention이란 input으로 들어오는 각각의 단어가 서로에게 얼만큼의 영향을 미치는지 알려준다. 문맥에 대한 정보를 잘 학습하도록 만드는 것이다. Model Architecture 기존 모델들은 encoder-decoder구조를 따르는데 이전 단계에서 생성한 symbol을 활용해서 decoder가 다음번에 나올 output을 만든다. Transformer 또한 encoder-decoder의 구조를 띄는데 모델을 순환적으로 사용하지 않고 attention mechanism만 활용하여 sequence에 대한 정보를 한 번에 입력으로 준다는 것이 특징이다. 위의 그림은 transformer의 전체적인 구조인데 이를 앞으로 하나씩 살펴볼 예정이다. Encoder and Decoder Stacks Encoder부분은 여러번 stack이 가능한 구조이고 하나의 layer는 크게 두가지의 구조를 지닌다. multi-head self attention feed-forward network 또한 이 두가지의 구조 모두 residual connection을 활용하여 Identity mapping을 거치게 한다. Decoder부분은 여러번 stack이 가능한 구조이고 보통 encoder랑 같은 layer의 개수랑 맞춘다. Decoder third sub-layer에 encoder의 output값을 활용하여 multi-head attention을 수행하도록 한다. residual connection을 활용하여 보다 더 쉽게 global optima를 찾도록 한다. 이전에 등장한 단어들만 참고할 수 있도록 mask를 씌워서 multi-head attention을 사용할 수 있도록 한다. Attention Attention Function은 쿼리와 key-value 쌍을 output으로 mapping을 한다. Query, keys, values, output들은 다 vector이다. Query(Q): 물어보는 주체 (어떤 단어가 가장 중요했는지를 key에서 계산하여 결과를 낸다.]]></description>
</item>
<item>
    <title>[CS231n] 07.Training Neural Networks II</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture7/</link>
    <pubDate>Mon, 22 Jul 2024 10:29:23 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture7/</guid>
    <description><![CDATA[개요 CS231n의 7강에 대한 내용을 정리 할 것이다. 저번 강에서는 Training Neural Networks에 대해 배웠고 이번 강에서는 그것에 추가적인 내용을 배울 것이다. Fancier Optimization Stochastic Gradient Descent(SGD) (SGD 그림)
이전까지 Stochastic Gradient Descent(SGD)에 대해서 배웠다. 이는 매우 간단한 Optimization의 기법인데, SGD에는 여러 문제가 있다. 이 문제들에 대해서 살펴볼 예정이다.
먼저 Taco shell problem이다. 위 그림이고 W1,W2가 있을 때, 손실 함수가 매우 느리게 작동한다. SGD와 같은 목적함수는 방향이 최소값을 향한 방향으로 정렬되어 있지 않고 지그재그의 형식으로 움직이게 된다.
Loss가 수직 방향으로만 sensitive해서 덜 민감한 수평 방향으로 진행 속도가 매우 느려지고 수직 방향으로는 빠르게 진행되어 지그재그의 형태로 학습을 하게 된다. 이것은 고차원에서 더 흔해진다. (graph 그림)
다른 문제는 Local minima과 Saddle point (안장점)이 있다.
x축이 하나의 파라미터를 뜻하고 y축이 loss를 뜻할 때 위 그림에서 SGD는 local minima일 때 멈춘다.
왜냐하면 SGD는 기울기를 계산하고 반대 기울기 방향으로 진행하기 때문에 빨간색 위치에서 gradient가 0이기 때문에 멈추게 된다.
이는 아래 그래프 처럼 Saddle point에서도 똑같이 기울기가 0이기 때문에 멈추게 된다.
Saddle point: 어떤 방향에서는 손실이 증가, 어떤 방향에서는 손실이 감소하는 그 중간 지점
High dimension에서는 saddle point가 local minima보다 더 흔하다. Saddle point의 근처 지점에서도 문제가 된다. Saddle point의 근처에 있을 때 마다 우리는 매우 느린 진행을 하게 될 것이다. 다른 문제는 SGD의 S이다. 확률적으로 모든 dataset의 loss를 계산하기엔 계산 비용이 많이 들어서 우리는 mini Batch의 방법을 사용하여 값을 추정한다.
하지만 이는 W의 값을 쪼개서 사용하므로 noisy data일 수 있다는 것이다.
기본적인 전체 배치를 사용하더라고 이런 모든 문제들은 해결되지 않는다.
SGD + Momentum 앞서 말한 문제들을 해결하기 위한 기본적인 idea가 있다. 그것은 바로 Momentum의 개념이다.
Momentum이란 시간이 지남에 따라 속도를 유지하고 기울기 추정치를 속도에 추가한다는 것이다. 기울기 방향이 아닌 속도 방향으로 gradient update가 일어난다.
마찰 상수인 rho도 존재. 마찰에 의해 속도를 감소시킨다음 gradient에 추가한다.
vx = 0 while True: dx = compute_gradient(x) # gradient 계산 vx = rho * vx + dx # 마찰 상수로 속도 감소 그것을 gradient에 추가 x += learning_rate * vx # 내려가기 (그래프)
이는 위 문제들을 다 해결하게 된다. 앞선 그래프에서 Local minima이든 Saddle point가 속도가 빨라지는 것과 같은 물리적인 해석을 할 수 있게 된다.
이제 속도가 있으면 기울기가 없더라도 해당 점은 여전히 속도를 가진다. 이러면 극복할 수 있게 된다.
따라서 위 그림처럼 지그재그들이 서로 상쇄되고 민감한 방향 (수직 방향)으로 걷는 양을 효과적으로 줄이고 덜 민감한 차원을 가로지르는 하강을 가속할 것이다.
그래서 시간이 지남에 따라 속도가 증가가 되고 노이즈가 기울기 추정에서 평균화가 된다.
그래서 SGD와는 다르게 부드러운 경로를 취하게 된다. (Momentum 그래프) 18. 위 그림은 Momentum의 종류를 나타낸 것이다.
기존 momentum은 현재 지점(빨간색)에서 기울기를 추정한 후 속도 벡터와 섞어서 사용하는 반면 Nesterov momentum은 기존 속도방향으로 나아간 후 그 시점에서 기울기를 추정한다. 그 후 원래 지점으로 돌아가서 이 두개를 섞는 방법이다.
Nesterov momentum은 기존보다 정보를 더 혼합하는 것으로 생각할 수 있게 된다. 이는 Convex optimization에서는 잘 작동하지만, Neural Network와 같은 non-convex의 문제에서는 보장된 방식은 아니다.
(Nesterov momentum 식 사진 )
우리는 항상 동일한 지점에서 loss를 평가하고 싶다. 하지만 Nesterov momentum은 그것이 아니기 때문에 따라서 이 식을 조금 더 변형하면 아래의 식과 같이 항상 동일한 지점에서 손실과 기울기를 평가할 수 있게 된다.
하단의 박스를 보면 현재 시점에서 $v_{t+1}$를 더하고 현재의 $v_{t+1}$와 이전 속도 $v_{t}$의 차이를 더해주면 항상 동일한 지점 $\tilde{x_t}$ 에서 평가하게 된다.]]></description>
</item>
<item>
    <title>[CS231n] 06.Training Neural Networks I</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture6/</link>
    <pubDate>Thu, 18 Jul 2024 15:36:30 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture6/</guid>
    <description><![CDATA[개요 CS231n의 6강에 대한 내용을 정리 할 것이다. 저번 강에서는 Convolutional Neural Networks에 대해 배웠고 이번 강에서는 이 Neural Networks를 어떻게 하면 잘 학습시킬 수 있을지에 대해서 배울 것이다. Activation Function Deep Network에 비선형성을 부여하는 Activation Function의 종류들에 대해서 알아볼 것이다. Sigmoid $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
먼저 첫 번째로 Sigmoid이다. 함수의 모양은 위 그림과 같고 (0,1) 사이의 값을 가지게 된다.Sigmoid가 가지게 되는 몇가지 문제들이 있다. 첫 번째 문제는 경사도를 죽일 수 있는 문제이다.
이는 입력으로 들어오는 x가 -10, 10이라면 global gradient(이전 노드의 기울기)는 0이 되고 이것이 역전파를 통해 다운스트림($global grdient * local gradient$) 되면 global gradient가 0이므로 아래 노드들은 0으로 수렴하게 되어 기울기가 소실이 된다. 이러한 기울기 소실로 인하여 W의 값을 더이상 update 하지 못하는 문제를 vanishing gradient라고 한다. 두 번째 문제는 output이 zero-center가 아닌 문제이다. 즉, Signmoid의 output이 항상 양수이기 때문에 0을 중심으로 있지 않는 문제가 발생하게 된다. ouput이 항상 양수 이라면 입력값 또한 양수인 것인데, 그렇게 된다면 back propagation에서 $\frac{dl}{df} * \frac{df}{wi}$ 로 W의 기울기를 chain-rule로 구하게 된다. 근데 우리가 이전 4강에서 배운 chain-rule의 pattern 중 mul gate(곱)일 때 grdient가 서로 전환이 된다고 배웠다. 따라서 $\frac{df}{wi}$ 이것이 바로 $x_i$인 것을 알 수 있게 된다. 이때 $x_i$가 이전 layer의 sigmoid 연산을 거친 output이기 때문에 무조건 양수이다. 입력값은 항상 양수라면 back propagation을 할 때 $\frac{dl}{df}$(global gradient이다.)의 부호를 따라가게 되어서 gradient의 부호는 항상 양수이거나 음수가 된다. 부가적으로 gradient의 부호는 다운 스트림으로 내려온 gradient의 부호와 같아진다고 생각하면 쉽다. 이렇게 부호가 모두 양수 또는 음수가 되어버리면 발생하는 문제는 gradient update를 할 때 나타난다. w1,w2가 있다면 gradient update시 항상 (+)(+) 이거나 (-)(-)가 된다. 따라서 위 그림과 같이 파란색 방향(최적의 gradient update)과는 다르게 비효율적으로 지그재그 방향으로 내려가게 되는 상황이 벌어지게 된다. 이를 해결할 수 있는 방법은 input값에 mean을 모두 빼버리면 zero-mean data가 된다. 그렇게 되면 X가 양수/음수를 모두 가지고 있는 상황을 만들 수 있게 된다. 마지막 세 번째 문제는 Sigmoid에서의 exp의 연산의 계산 비용이 많이 들어간다는 것이다. 따라서 위 세 가지의 문제로 잘 사용하지 않게 된다. tanh 다음은 tanh함수이다. 이는 범위를 (-1,1)까지 하여 Sigmoid의 단점인 non-zero-center문제를 해결하게 되었다. 하지만 여전히 그래프를 봐도 확인 할 수 있지만 경사도를 죽일 수 있는 문제는 해결되지 못하였다. 따라서 Network가 깊어지는 모델이 많아진 요즘 활성화 함수로 잘 사용하지 않게 된다. ReLU $$f(x) = max(0,x)$$
다음은 ReLU함수 이다. 이는 위 두 함수과 다르게 양수에서 경사도를 죽일 수 있는 문제를 해결 하였다. 또한 이 함수는 최대값만 계산하기 때문에 연산이 효율적이고 실제로 앞선 함수들과 비교하여 6배가 빠르다고 한다. 하지만 ReLU함수도 몇가지 문제점이 있다. Non-zero-center문제를 가지고 있고 또한 음수일 경우에는 경사도를 죽일 수 있는 문제가 발생할 수 있다. 따라서 W를 initialization하는 과정에서 잘못되었을 때 dead ReLU가 발생할 수 있다. 따라서 이런 dead ReLU를 피하기 위하여 초기화를 할 때 positive한 bias를 얻게 초기화를 하는 방법도 있지만 효과가 경미하다. 또한 learning rate를 크게 설정하는 대규모 훈련 시 W가 너무 크게 비약해서 학습이 안되는 상황이 발생할 수도 있다. 실제로 ReLU를 사용한 Network에서 10~20% 정도가 dead ReLU에 빠진다고 한다. 문제이긴 하지만 그럼에도 불구하고, train은 잘 된다고 한다. Leaky ReLU $$f(x) = max(0.01x,x)$$
ReLU와 다르게 음수에도 음수 기울기를 주어 경사도를 죽일 수 있는 문제를 해결하고자 한다. ReLU와 같게 연산이 빠르다는 장점도 있다. PReLU $$f(x) = max(ax,x)$$
PReLU는 음수 영역에 backpropagation에서의 파라미터로 기울기 값으로 유연성을 더해주어 조금 더 융통성을 가해 주었고 ELU 다음의 ELU는 평균 출력이 더욱 0에 가까워지게 만든 함수이다.]]></description>
</item>
</channel>
</rss>
