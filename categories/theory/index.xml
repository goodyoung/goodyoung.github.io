<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Theory - Category - Good Young</title>
        <link>https://goodyoung.github.io/categories/theory/</link>
        <description>Theory - Category - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 30 Jul 2024 22:58:52 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/categories/theory/" rel="self" type="application/rss+xml" /><item>
    <title>[CS231n] 09.CNN Architectures</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture9/</link>
    <pubDate>Tue, 30 Jul 2024 22:58:52 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture9/</guid>
    <description><![CDATA[개요 CS231n의 9강에 대한 내용을 정리 할 것이다.
9강에 나오는 CNN Architecture중에 GoogLeNet과 ResNet에 대해서 살펴볼 예정이다.
GoogLeNet 이때부터 이제 Network를 깊게 효율적을 만들기 시작했다.
Network를 깊고 효율적으로 만들기 위해서 GoogLeNet에서는 Inception module을 추가하였다. Inception Module
로컬 네트워크의 한 유형이다. 네트워크의 네트워크 병렬로 서로 다른 크기의 filter를 병렬로 돌리는 것이다. no FC layers
Stride와 padding을 통하여 입력과 출력 차원을 일치시키고 depth를 높였다. 문제는 computational complexity이다. Pooling layer가 depth를 유지하기 때문에 every layer에서 전체적인 depth가 깊어진다. 이를 해결하려고 &lsquo;boottleneck layer&rsquo;가 나온다. 1*1 filter를 사용하여 depth를 줄인다. 따라서 기존과 확연히 다른 연산을 수행하는 것을 확인할 수 있게 된다. 1*1 를 사용하면 정보 손실이 발생할 수 있지만 이러한 예측을 수행하는 경우 이들의 조합을 계산하여 추가적으로 비선형성을 도입하므로 ?? 또한 auxiliary classification output이 있다. - To inject additional gradient at lower layers - 중간 layer도 도움이 된다. 깊은 네트워크 때문에 gradient vanishing 현상을 극복하려고 inject를 한다. 따라서 이런 추가 신호를 제공한다.
ResNet 2015 우승자 revolution of Depth인 만큼 많이 깊은 network이다
엄청 깊게 cnn을 쌓으면 더 나은 결과가 나오나. 아니다.
- Train시 overfitting이 예상이 되어 오류가 아주 적을 것이라 예상을 했지만 그 조차도 아니다. - overfitting이 아니다. - optimization problem문제이다. - 적어도 깊은 layer는 shallower의 성능은 기본적으로 있어야 하지만 그 조차도 아니다. - 깊은 layer는 shallower model보다 최적화 하기 어렵다. - 이걸 해결하려고 residual connection이 나왔다. - H(x) 바로 하는 것이 아니라 x를 뺀 나머지(잔차)를 학습을 한다 - FC 층이 없다 오직 output linear층만 있다. Resnet50으로 가면 bottleneck layer을 사용하여 11 conv를 사용한다. - 11으로 depth를 처음에 줄이고 나중에 depth를 높인다.]]></description>
</item>
<item>
    <title>[CS231n] 07.Training Neural Networks II</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture7/</link>
    <pubDate>Mon, 22 Jul 2024 10:29:23 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture7/</guid>
    <description><![CDATA[개요 CS231n의 7강에 대한 내용을 정리 할 것이다. 저번 강에서는 Training Neural Networks에 대해 배웠고 이번 강에서는 그것에 추가적인 내용을 배울 것이다. Fancier Optimization Stochastic Gradient Descent(SGD) (SGD 그림)
이전까지 Stochastic Gradient Descent(SGD)에 대해서 배웠다. 이는 매우 간단한 Optimization의 기법인데, SGD에는 여러 문제가 있다. 이 문제들에 대해서 살펴볼 예정이다.
먼저 Taco shell problem이다. 위 그림이고 W1,W2가 있을 때, 손실 함수가 매우 느리게 작동한다. SGD와 같은 목적함수는 방향이 최소값을 향한 방향으로 정렬되어 있지 않고 지그재그의 형식으로 움직이게 된다.
Loss가 수직 방향으로만 sensitive해서 덜 민감한 수평 방향으로 진행 속도가 매우 느려지고 수직 방향으로는 빠르게 진행되어 지그재그의 형태로 학습을 하게 된다. 이것은 고차원에서 더 흔해진다. (graph 그림)
다른 문제는 Local minima과 Saddle point (안장점)이 있다.
x축이 하나의 파라미터를 뜻하고 y축이 loss를 뜻할 때 위 그림에서 SGD는 local minima일 때 멈춘다.
왜냐하면 SGD는 기울기를 계산하고 반대 기울기 방향으로 진행하기 때문에 빨간색 위치에서 gradient가 0이기 때문에 멈추게 된다.
이는 아래 그래프 처럼 Saddle point에서도 똑같이 기울기가 0이기 때문에 멈추게 된다.
Saddle point: 어떤 방향에서는 손실이 증가, 어떤 방향에서는 손실이 감소하는 그 중간 지점
High dimension에서는 saddle point가 local minima보다 더 흔하다. Saddle point의 근처 지점에서도 문제가 된다. Saddle point의 근처에 있을 때 마다 우리는 매우 느린 진행을 하게 될 것이다. 다른 문제는 SGD의 S이다. 확률적으로 모든 dataset의 loss를 계산하기엔 계산 비용이 많이 들어서 우리는 mini Batch의 방법을 사용하여 값을 추정한다.
하지만 이는 W의 값을 쪼개서 사용하므로 noisy data일 수 있다는 것이다.
기본적인 전체 배치를 사용하더라고 이런 모든 문제들은 해결되지 않는다.
SGD + Momentum 앞서 말한 문제들을 해결하기 위한 기본적인 idea가 있다. 그것은 바로 Momentum의 개념이다.
Momentum이란 시간이 지남에 따라 속도를 유지하고 기울기 추정치를 속도에 추가한다는 것이다. 기울기 방향이 아닌 속도 방향으로 gradient update가 일어난다.
마찰 상수인 rho도 존재. 마찰에 의해 속도를 감소시킨다음 gradient에 추가한다.
vx = 0 while True: dx = compute_gradient(x) # gradient 계산 vx = rho * vx + dx # 마찰 상수로 속도 감소 그것을 gradient에 추가 x += learning_rate * vx # 내려가기 (그래프)
이는 위 문제들을 다 해결하게 된다. 앞선 그래프에서 Local minima이든 Saddle point가 속도가 빨라지는 것과 같은 물리적인 해석을 할 수 있게 된다.
이제 속도가 있으면 기울기가 없더라도 해당 점은 여전히 속도를 가진다. 이러면 극복할 수 있게 된다.
따라서 위 그림처럼 지그재그들이 서로 상쇄되고 민감한 방향 (수직 방향)으로 걷는 양을 효과적으로 줄이고 덜 민감한 차원을 가로지르는 하강을 가속할 것이다.
그래서 시간이 지남에 따라 속도가 증가가 되고 노이즈가 기울기 추정에서 평균화가 된다.
그래서 SGD와는 다르게 부드러운 경로를 취하게 된다. (Momentum 그래프) 18. 위 그림은 Momentum의 종류를 나타낸 것이다.
기존 momentum은 현재 지점(빨간색)에서 기울기를 추정한 후 속도 벡터와 섞어서 사용하는 반면 Nesterov momentum은 기존 속도방향으로 나아간 후 그 시점에서 기울기를 추정한다. 그 후 원래 지점으로 돌아가서 이 두개를 섞는 방법이다.
Nesterov momentum은 기존보다 정보를 더 혼합하는 것으로 생각할 수 있게 된다. 이는 Convex optimization에서는 잘 작동하지만, Neural Network와 같은 non-convex의 문제에서는 보장된 방식은 아니다.
(Nesterov momentum 식 사진 )
우리는 항상 동일한 지점에서 loss를 평가하고 싶다. 하지만 Nesterov momentum은 그것이 아니기 때문에 따라서 이 식을 조금 더 변형하면 아래의 식과 같이 항상 동일한 지점에서 손실과 기울기를 평가할 수 있게 된다.
하단의 박스를 보면 현재 시점에서 $v_{t+1}$를 더하고 현재의 $v_{t+1}$와 이전 속도 $v_{t}$의 차이를 더해주면 항상 동일한 지점 $\tilde{x_t}$ 에서 평가하게 된다.]]></description>
</item>
<item>
    <title>[CS231n] 06.Training Neural Networks I</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture6/</link>
    <pubDate>Thu, 18 Jul 2024 15:36:30 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture6/</guid>
    <description><![CDATA[개요 CS231n의 6강에 대한 내용을 정리 할 것이다. 저번 강에서는 Convolutional Neural Networks에 대해 배웠고 이번 강에서는 이 Neural Networks를 어떻게 하면 잘 학습시킬 수 있을지에 대해서 배울 것이다. Activation Function Deep Network에 비선형성을 부여하는 Activation Function의 종류들에 대해서 알아볼 것이다. Sigmoid $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
먼저 첫 번째로 Sigmoid이다. 함수의 모양은 위 그림과 같고 (0,1) 사이의 값을 가지게 된다.Sigmoid가 가지게 되는 몇가지 문제들이 있다. 첫 번째 문제는 경사도를 죽일 수 있는 문제이다.
이는 입력으로 들어오는 x가 -10, 10이라면 global gradient(이전 노드의 기울기)는 0이 되고 이것이 역전파를 통해 다운스트림($global grdient * local gradient$) 되면 global gradient가 0이므로 아래 노드들은 0으로 수렴하게 되어 기울기가 소실이 된다. 이러한 기울기 소실로 인하여 W의 값을 더이상 update 하지 못하는 문제를 vanishing gradient라고 한다. 두 번째 문제는 output이 zero-center가 아닌 문제이다. 즉, Signmoid의 output이 항상 양수이기 때문에 0을 중심으로 있지 않는 문제가 발생하게 된다. ouput이 항상 양수 이라면 입력값 또한 양수인 것인데, 그렇게 된다면 back propagation에서 $\frac{dl}{df} * \frac{df}{wi}$ 로 W의 기울기를 chain-rule로 구하게 된다. 근데 우리가 이전 4강에서 배운 chain-rule의 pattern 중 mul gate(곱)일 때 grdient가 서로 전환이 된다고 배웠다. 따라서 $\frac{df}{wi}$ 이것이 바로 $x_i$인 것을 알 수 있게 된다. 이때 $x_i$가 이전 layer의 sigmoid 연산을 거친 output이기 때문에 무조건 양수이다. 입력값은 항상 양수라면 back propagation을 할 때 $\frac{dl}{df}$(global gradient이다.)의 부호를 따라가게 되어서 gradient의 부호는 항상 양수이거나 음수가 된다. 부가적으로 gradient의 부호는 다운 스트림으로 내려온 gradient의 부호와 같아진다고 생각하면 쉽다. 이렇게 부호가 모두 양수 또는 음수가 되어버리면 발생하는 문제는 gradient update를 할 때 나타난다. w1,w2가 있다면 gradient update시 항상 (+)(+) 이거나 (-)(-)가 된다. 따라서 위 그림과 같이 파란색 방향(최적의 gradient update)과는 다르게 비효율적으로 지그재그 방향으로 내려가게 되는 상황이 벌어지게 된다. 이를 해결할 수 있는 방법은 input값에 mean을 모두 빼버리면 zero-mean data가 된다. 그렇게 되면 X가 양수/음수를 모두 가지고 있는 상황을 만들 수 있게 된다. 마지막 세 번째 문제는 Sigmoid에서의 exp의 연산의 계산 비용이 많이 들어간다는 것이다. 따라서 위 세 가지의 문제로 잘 사용하지 않게 된다. tanh 다음은 tanh함수이다. 이는 범위를 (-1,1)까지 하여 Sigmoid의 단점인 non-zero-center문제를 해결하게 되었다. 하지만 여전히 그래프를 봐도 확인 할 수 있지만 경사도를 죽일 수 있는 문제는 해결되지 못하였다. 따라서 Network가 깊어지는 모델이 많아진 요즘 활성화 함수로 잘 사용하지 않게 된다. ReLU $$f(x) = max(0,x)$$
다음은 ReLU함수 이다. 이는 위 두 함수과 다르게 양수에서 경사도를 죽일 수 있는 문제를 해결 하였다. 또한 이 함수는 최대값만 계산하기 때문에 연산이 효율적이고 실제로 앞선 함수들과 비교하여 6배가 빠르다고 한다. 하지만 ReLU함수도 몇가지 문제점이 있다. Non-zero-center문제를 가지고 있고 또한 음수일 경우에는 경사도를 죽일 수 있는 문제가 발생할 수 있다. 따라서 W를 initialization하는 과정에서 잘못되었을 때 dead ReLU가 발생할 수 있다. 따라서 이런 dead ReLU를 피하기 위하여 초기화를 할 때 positive한 bias를 얻게 초기화를 하는 방법도 있지만 효과가 경미하다. 또한 learning rate를 크게 설정하는 대규모 훈련 시 W가 너무 크게 비약해서 학습이 안되는 상황이 발생할 수도 있다. 실제로 ReLU를 사용한 Network에서 10~20% 정도가 dead ReLU에 빠진다고 한다. 문제이긴 하지만 그럼에도 불구하고, train은 잘 된다고 한다. Leaky ReLU $$f(x) = max(0.01x,x)$$
ReLU와 다르게 음수에도 음수 기울기를 주어 경사도를 죽일 수 있는 문제를 해결하고자 한다. ReLU와 같게 연산이 빠르다는 장점도 있다. PReLU $$f(x) = max(ax,x)$$
PReLU는 음수 영역에 backpropagation에서의 파라미터로 기울기 값으로 유연성을 더해주어 조금 더 융통성을 가해 주었고 ELU 다음의 ELU는 평균 출력이 더욱 0에 가까워지게 만든 함수이다.]]></description>
</item>
<item>
    <title>[CS231n] 05.Convolutional Neural Networks</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture5/</link>
    <pubDate>Fri, 12 Jul 2024 20:01:34 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture5/</guid>
    <description><![CDATA[개요 CS231n의 5강에 대한 내용을 정리 할 것이다. 저번 강에서는 W를 업데이트 하는 과정 Chain-Rule과 간단한 Neural Networks에 대해서 배웠는데 이번 강에서는 Convolutional Neural Networks에 대해서 배울 것이다. A bit of history 위 그림을 보면 Hubel &amp; Wiesel이 고양이 실험을 했는데 이때 시각 피질 안의 뉴런이 local receptive field를 가지게 된다는 것을 알게 되었다. 또한 이렇게 겹쳐지는 receptive field들이 전체 시야를 이루게 된다는 사실을 알게 되었다. 어떤 뉴런은 low level, 또 어떤 뉴런은 high level의 특징들을 포착하는 것의 조합으로 이루게 된다. 이러한 지식을 기반으로 image처리를 위해 98년에 Lenet, 2012년에 AlexNet이 등장하게 되었다. Convolutional Neural Networks 저번 시간에 node들이 linear하게 모두 연결되어 있는 층을 Fully Connected (FC)층이라고 한다. 또 이 FC층이 행렬의 내적 계산을 통한 아주 효율적인 계산을 할 수 있다고 배웠다. 따라서 위 그림을 보면 하나의 layer는 하나의 연산을 통해 계산을 하여 편의성을 더해주는 사실을 볼 수 있다. 하지만 이미지 처리를 할 땐 위의 FC층이 좋지 않다. 왜냐하면 이미지의 spatial(공간적)정보가 손실이 되기 때문이다. 이러한 문제점을 해결하기 위하여 Convolutional Neural Networks (CNN)이 등장하게 되었다. CNN에 대해 더 자세히 알아보면 위 그림과 같은 이미지가 있고 필터가 존재한다. 이 필터는 우리가 linear classification에서 배웠던 W의 역할을 한다. 이런 필터는 input image위를 아래와 같이 슬라이딩 하면서 요소별 곱을 하고 그것을 또 하나의 합으로 나타낸다.
위 그림에선 3 * 3의 크기의 filter가 5 * 5의 크기의 image 위를 슬라이딩 하고 있다. 이 filter에 중요한 사실이 있다. 바로 filter의 depth 크기는 input volume의 depth랑 항상 같다는 점이다. 왜냐하면 우리가 시각적으로 보기에 image위를 슬라이딩 하는 것 처럼 보이지만 실제로의 연산은 $w^Tx$에서 image에서 filter가 겹쳐지는 부분만큼 가져온 후 1차원으로 늘린 연산이다. 따라서 위 그림과 같이 5*5*3의 크기인 filter연산은 곧 75-1D의 내적 연산(1차원)을 수행하는 것으로 이해하면 될 것 같다. 이렇게 연산을 수행하기 때문에 input volume의 depth랑 filter depth랑 같아야 filter의 내적 연산을 수행할 수 있게 된다. 이렇게 filter가 슬라이딩을 하여 연산을 한 번 모두 하면 위 그림과 같이 하나의 activation map이 나온다. 만약 필터가 6개가 있다면 앞서 말한 내적 연산을 6번 수행하여 ouput의 depth가 6인 activation map이 나오게 된다. 각 필터는 input volume에서 특정 유형의 템플릿이나 개념을 찾는다. 6개의 각각 다른 가중치를 지닌 activation map으로 생각하면 된다. 이런 layer들을 연속적으로 쌓아나가면 그것이 convolution network가 된다. 이런 convolution network에서는 처음에 low-level(edge 등등)의 특징을 추출하고 점점 깊어지면 high-level의 복잡하고 추상적인 개념들이 나타나게 된다. 이 정보들을 FC층에 넣고 각 class 수의 확률 연산을 하게되면 분류가 이루어진다. 왜 FC층이 필요하며, 어떻게 분류가 이루어지는지 의문을 가졌다. high-level features는 넓은 reception field를 가지고 있다. (점점 깊어질 수록 이미지의 resolution이 줄어들기 때문) 이러한 복잡하고 풍부한 정보를 포함하고 있는 feature들을 linear하게 놓고 모든 정보를 연결(Fully connected)을 하여 각각의 class별 weight를 계산을 하게 되면 해당 이미지가 class별 확률(softmax)이 나오게 될 것이다. 따라서 CNN으로 feature를 추출하고 FC 층에서 분류하는 이유는 이러한 과정을 통해 각 클래스별 확률을 효과적으로 계산할 수 있기 때문입니다. 이러한 방법들은 앞서 history에서 말한 인간의 인식 능력에서의 시각피질과 유사한 특징을 보이는 것을 알 수 있다. 픽셀은 항상 일관된 순서를 가지며, 서로 인접한 픽셀끼리 영향을 준다. 만약 모든 근처의 픽셀이 빨간색이라면 해당 픽셀도 빨간색일 가능성이 높다. 이렇게 픽셀은 주변 픽셀 값과 비교하여 정보를 추측할 수 있습니다. 이런 특성을 locality라고 합니다. 따라서 위 그림과 같이 sub sampling과정을 통해 image의 resolution을 줄이고 local feature들에 대한 연산을 통해 global feature(high-level)로 나아가 weight 변수를 줄이고 변화에 무관한 invariance를 얻게 되는 것이다.]]></description>
</item>
<item>
    <title>[CS231n] 04.Introduction to Neural Networks</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture4/</link>
    <pubDate>Mon, 08 Jul 2024 21:02:03 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture4/</guid>
    <description><![CDATA[개요 CS231n의 4강에 대한 내용을 정리 할 것이다. 저번 강에서는 Loss Function과 Optimization에 대해서 배웠는데 이번 강에서는 W를 업데이트 하는 과정인 Chain-Rule과 간단한 Neural Networks에 대해서 배울 것이다. Backpropagation 지난 과정에 gradient에는 두가지 종류가 있다고 배웠다. 그 중 빠르고 정확한 analytic gradient에 대해서 활용해볼 것이다. 각 과정의 연산 과정을 Computational graph을 활용하여 표현한다면 analytic gradient를 활용할 수 있게 된다.
이를 통해 함수는 BackPropagation이라는 기술을 사용하고, gradient를 얻기 위하여 Chain-rule를 활용한다.
BackPropagation의 과정은 다음과 같다. 각 입력이 local node로 들어오고 다음 노드로 직접 전달된다. local gradient는 이때의 입력된 노드의 출력의 gradient이다. 각각의 입력마다 그때의 local gradient를 구한다. 즉, z에 대한 x로의 미분, z에 대한 y로의 미분을 구한다. 이를 Forward Pass (Foward Propagation)이라고 한다. Forward Pass의 맨 마지막에는 loss function을 통한 loss가 나온다. Forward Pass가 모든 노드가 진행이 되었으면 Backward Pass (Back Propagation)이 진행된다. 이때 Back Propagation은 수많은 계산을 거쳐 나온 loss에 대한 z의 미분을 나타내고 이는 global gradient (위 그림에선 빨간색 글씨로 gradients라고 표기)라고 칭한다. 이때 그럼 loss에 대한 x, y의 미분값을 구할 수 있게 되는데 이때 활용되는 개념이 Chain-rule이다. Forward Pass로 구한 local gradient의 값과 그 노드의 global gradient를 곱하면 우리고 최종적으로 원하는 gradient가 나오게 된다. $gradient = local \ gradient * global \ gradient$ 이런 Computational graph에서 그룹화를 할 수 있다는 사실도 알 수 있다. 위 그림을 보면 sigmoid gate로 하나의 노드로 묶어서 계산 할 수도 있다. 따라서 얼마나 그룹화를 하여 노드를 표현할 것인지에 대한 고민이 필요할 수 있다. 또한 Back Propagation에는 3가지 패턴이 존재한다고 한다. add gate gradient 전달하는 역할 max gate 한 방향으로 gradient 모두 전달하는 역할 mul gate 서로 gradient 전환하는 역할 종합적으로 위에서 배운 Back Propagation을 아래와 같이 일반화된 식으로 표현한다. $$ \frac{\partial f}{\partial x} = \sum \frac{\partial f}{\partial q_i} \cdot \frac{\partial q_i}{\partial x} $$
이제 위에서 배운 Back Propagation에서 변수가 벡터라고 생각한다면 gradient는 Jacobian matrix로 표현할 수 있게 된다. Jacobian matrix: 각 요소의 미분을 포함하는 행렬 따라서 4096의 input이 들어온다면 이 Jacobian matrix의 크기는 $4096 * 4096$ 일 것이다. 이때 gradient의 각 요소는 함수의 최종 출력에 얼마나 영향을 미치는가를 정량화 한 값으로 표현이 되고 이는 결국 편미분한 값과 이어지게 된다. 따라서 입력의 어떤 차원이 출력의 어떤 차원에 영향을 주는지, 그래서 Jacobian matrix는 입력의 각 요소가 오직 출력의 해당 요소에만 영향을 주기 때문에 대각 행렬이 될 것이다. Neural Networks 위 그림 처럼 2계층 신경망을 얻기 위해 다른 것 위에 비선형 변환을 하면 된다. 이렇게 계속 층층 쌓아가면 Deep Neural Network의 형태가 된다. 위의 W1, W2는 각각 gradient로 학습 시키고, 그 gradient들은 Chain-rule으로 계산하여 구한다. 이런 비선형성의 특징을 표현하기 위해 activation function이라는 함수가 존재한다. 이는 강의 후반부에 더 자세히 다룬다고 한다. 아래는 이 Forward pass과정을 코드로 표현한 것이다. f = lambda x: 1.0/(1.0+ np.exp(-x)) # sigmoid (activation function) x = np.random.randn(3, 1) # random input vector h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer out = np.dot(W3,h2) + b3 # output neuron (1*1) Reference https://chasuyeon.tistory.com/entry/cs231n-4%EA%B0%95-%EC%A0%95%EB%A6%AC-Introduction-to-Neural-Networks ]]></description>
</item>
<item>
    <title>[CS231n] 03.Loss Functions and Optimization</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture3/</link>
    <pubDate>Tue, 02 Jul 2024 15:32:06 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture3/</guid>
    <description><![CDATA[개요 CS231n의 3강에 대한 내용을 정리 할 것이다. 저번 강에서는 선형 분류기에 대해서 공부를 했는데 이때 어떻게 이미지의 가중치인 W의 값들의 badness를 판단하는지(loss function), 또 W의 값들을 업데이트 하는 방법(optimization)에 대해서 집중적으로 다룰 것이다. Loss Function Loss Function이란 W를 가져와서 점수를 보고 얼마나 정량적으로 좋은지 나쁜지 알려주는 함수이다. 또는 실제값과 예측값의 차이를 수치화 해주는 함수이다. Loos Function의 기본 식은 아래와 같다. xi: pixel value yi: predict label - f: predict y Li: loss function - L: loss mean 따라서 위 식을 통해 어떤 W가 가장 좋은 결과를 가져오는지 정량적으로 판단할 수 있게 된다. Multiclass SVM Loss 다음으론 image classification에 적합한 multi-class SVM Loss에 대한 설명이다. loss function의 식은 아래와 같다. sj: 잘못된 label의 score syi: 제대로 된 label의 score - 1: safety margin 따라서 위 식은 제대로 예측을 했고 그 값이 safety margin보다 뛰어나다면 loss는 0을 나타내고, 그것이 아니라면 잘못된 label과 정상 label간의 차이만큼이 loss인 것을 알 수 있다. 이를 그래프로 시각화를 하면 아래 그림과 같은데 이를 hinge loss라고 한다. 이 그래프를 통해 알 수 있는 사실은 잘 분류된 point는 loss가 작거나 없고, 잘못 분류된 point는 loss가 높다. 잘 분류되었을 수록 syi가 클 것이다. 그러므로 sj - syi의 식에서 음수값이 나올 가능성이 크다. 다음은 hinge loss에 관하여 6가지의 질문이 나온다. Q1: 잘 예측 했을 경우 데이터가 미세하게 변한다고 해서 loss의 값이 변하지는 않는다. Q2: hinge loss의 최솟값은 0, 최대값은 무한대일 것이다. (class score가 엄청 낮은 음수 값을 가지고 있다면) Q3: W를 임의의 작은 수로 초기화를 할 때 sj - syi의 값이 0일 경우 classes_num - 1의 값이 나오게 된다. 이는 debugging시 유용하다. 훈련 시 초기의 loss값이 classes_num - 1이 아니라면 과정 중 어떤 것이 잘못되었는지 다시 볼 필요가 있다. Q4: 정답 class도 계산을 한다면 기존 loss에 1이 증가한 형태일 것이다. Q5: Summation이 아니라 Mean이라도 loss의 값은 변함 없다. (rescale만 된다. 큰 영향 없다.) Q6: 아래의 그림과 같이 제곱항으로 변경한다면 결과는 달라진다. 선형적이었던 관계를 비선형적으로 바꿔서 표현이 된다. 따라서 잘못 예측한 것은 제곱만큼 loss가 커지게 되는 상황이 발생한다. 따라서 데이터의 특성에 따라 극심한 차이를 보고싶을 때 사용한다. 따라서 어떤 loss를 사용하냐는 error값을 얼마나 신경쓰고 있고, 어떻게 정량화 할거냐에 따라 다르다. 아래는 hinge-loss의 example code이다.
def L_i_vectorzed(x, y, W): scores = W.dot(x) # Predict margins = np.maximum(0, scores - scores[y] + 1) # Calc Loss margins[y] = 0 # class label에 해당하는 것은 0으로 두는 일종의 trick에 해당한다. loss_i = np.sum(margins) # Summation Loss return loss_i # return Regularization 또다른 질문이 있다. *Loss가 0인 W를 찾았다면 이 값과 동일한 다른 W도 존재하는가?*에 대한 질문이다. 정답은 있다! 이다. 기존 W에 2배를 한다고 해서 Loss는 변하지 않는다. 앞서 loss function이 최적의 W를 찾기 위해 정량적으로 나타내는 것이라고 설명을 했다. 하지만 우리는 수많은 W가 0이 된다는 사실을 방금 알게 되었다. 이는 모순임을 알 수 있다. 왜냐하면 오직 위 식은 data의 loss에만 신경을 쓰고 있기 때문이다. 이때의 data는 xi, 즉 training data이다. 이는 아래 그림과 같이 과대 적합 (Overfitting) 인 문제가 발생한다. 우리는 test data에 대한 performance에 관심이 있지 training data에는 관심이 없다. 따라서 우리는 아래 그림과 같이 되기를 원한다. 따라서 기존 Loss 식에 변화를 주어야 된다. 기존 식에 Regularization을 추가하여 이 식은 분류기가 더 간단한 W를 선택하도록 패널티를 주는 역할을 한다.]]></description>
</item>
<item>
    <title>[CS231n] 02.Image Classification</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture2/</link>
    <pubDate>Tue, 14 May 2024 21:32:06 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture2/</guid>
    <description><![CDATA[개요 CS231n을 공부하면서 정리를 위해 글을 작성해보려고 한다. 1강의 내용은 컴퓨터 비전에 대해 전반적인 역사, 이 course를 통해 얻게 될 내용들에 대해서 소개를 해주었다. 따라서 따로 정리할 것은 없어서 정리하지는 않았다. The Problem 컴퓨터는 사람과 달리 이미지를 숫자로 인식을 한다. 따라서 사람이 의미하는 것과 컴퓨터가 이해하는 것에는 차이(Semantic Gap)가 있기 마련이다. 또한 컴퓨터가 image를 인식할 때 여러 문제(problem)들이 발생하게 된다. Viewpoint variation (카메라의 움직임) Illumination (색상 차이) Deformation (다양한 모습) Occlusion (부분적으로 물체 보이는 현상) Background Clutter (배경 혼란 현상) Intraclass variation (다양한 종류) 따라서 우리는 이런 problem들을 극복할 수 있는 robust하고 확장성이 뛰어난 모델을 만들어야 한다. 이런 모델들을 만들기 위해서 Data-Driven Approach(하나의 접근 방식)를 제안한다. Data-Driven Approach에 대해서 아래에 설명을 작성했다.
image와 label들의 dataset을 모은다. 간단한 분류기를 machine learning에 의해 훈련킨다. (함수에서 train) 새로운 이미지를 가지고 분류기를 평가한다. (함수에서 predict) Nearest Neighbor 이 방법은 학습 데이터와 새로운 이미지들을 비교해서 학습 데이터 중에서 가장 유사한 이미지로 레이블을 예측하는 방법이다. Nearest Neighbor에서 이때 유사한 이미지로 판단하기 위해서 L1(manhattan) distance가 있다. L1 distance는 새로운 이미지의 pixel에 학습 데이터를 뺀 절대값들의 합을 구하여 그 합들이 가장 최소로 나온 이미지를 유사하다고 판단하는 방법이다. [그림 1] Nearest Neighbor 위 그림과 함께 Nearest Neighbor의 과정을 보자면 train 과정에서 모든 훈련 데이터를 저장한다. 그리고 predict 과정에서 모든 훈련 데이터를 비교하여 가장 유사한 훈련 데이터 image를 찾게 된다. 이 과정에서 train: O(1), predict: O(N)(# of iunput)의 시간 복잡도를 가지게 되는데 이것은 잘못되었다고 표현을 한다. 왜냐하면 우리는 보통 분류기가 빠르게 예측하기(predict)를 원하지만 Nearest Neighbor에서는 그것이 반대가 되어있는 것을 알 수 있게 된다. 또한 Nearest Neighbor알고리즘은 가장 가까운 이웃만을 보기 때문에 위와 같은 초록색이 대부분인 영역에서 노란색을 예측하는 문제가 발생 할 수 있다. 따라서 K-Nearest Neighbor알고리즘이 나오게 된다. K-Nearest Neighbor [그림 2] K-Nearest Neighbor 그 다음으로 가까운 Neighbor을 K개의 만큼 찾고, 그것끼리 다수결의 결정으로 예측을 하는 K-Nearest Neighbor방법이 있다. 또 이때 유사도를 결정할 때 L1 distance가 아닌 L2(Euclidean) distance의 계산식이 있다. 이것은 L1과 달리 좌표계가 무엇이든 상관이 없지만 L1의 경우에는 좌표계에 따라 계산값이 변할 수 있다. 각각 어떤것이 좋은지는 데이터의 성격에 따라 다르다 input 데이터가 각각의 항목이 중요한 성격(salary, 성별, 연봉 등)을 가진다면 L1이 적당할 수 있지만 어떤 역할인지 상관이 없으면 L2가 더 좋을 수 있다. 위 그림 2를 보게 되면 K가 커질 수록 좀 더 경계들이 부드러워지는 모습을 볼 수 있게 된다. 그렇다면 K가 무조건 커지면 좋은 것이냐? 그건 또 아니다. 이때 중요한 것이 적절한 K를 결정하는 것이 중요하다. 이런 K같은 요소들을 Hyperparameter이라고 부르고 다음으로 이런 적절한 Hyperparameter을 선택하는 방법에 대해서 설명하겠다. Hyperparameter 이런 Hyperparameter는 직접 시도해보고 가장 좋은 값을 찾는 것이 정답이다. [그림 3] Hyperparameter Setting idea #1: 학습데이터만을 학습하여 하이퍼 파라미터를 선택
idea #2: train한 모델을 바탕으로 Test set에서만 잘 동작하는 하이퍼 파라미터를 선택
idea #3: train set으로 모델을 학습, validation set으로 가장 좋았던 하이퍼 파라미터 선택, validation set에서 고른 하이퍼 파라미터를 바탕으로 test set을 test시작
따라서 이런 적절한 Hyperparameter가 무엇인지 결정할 때 위 3개의 방법 중 idea #3방법이 제일 좋다. 왜냐하면 idea1,2는 unseen data에 대한 정확도가 떨어진다. 결국 idea #2도 새로운 data에 약한 특성을 보인다. 예를 들어, idea #2일 때 K값을 바꿔가면서 이 중 가장 높은 성능을 지닌 K를 정한다고 했을 때 이는 test데이터에 좋은 성능을 보이는 K값인 것이다.]]></description>
</item>
<item>
    <title>[DL]Image Segmentation 3</title>
    <link>https://goodyoung.github.io/posts/segmentation/segmentation-3/</link>
    <pubDate>Thu, 04 Apr 2024 19:31:48 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/segmentation/segmentation-3/</guid>
    <description><![CDATA[개요 이전 글에 이어서 Image segmentation의 접근 방법 (Edge 기반 방법, 영역 기반 방법)에 대해서 알아보고자 한다. 먼저 Edge 기반 방법의 Edge Detection에 대해 알아보겠다. Edge Detection Edge Detection은 영상(Image)의 Edge(경계선, 윤곽선)을 검출하는 것이다. 이때 Edge는 깊이, 색깔, 조명이 달라서등 다양한 원인에 의해서 생길 수 있다. Edge는 이러한 원인들에 의해서 밝기가 급격하게 변한다라고 말할 수 있다. 따라서 영상상의 Edge의 밝기를 분석했을 때 Edge에 해당하는 부분은 픽셀의 밝기가 급격하게 변한다라는 특징을 가지고 있다. [그림 1] intensity function 위 [그림 1]을 보면 중간 그림 처럼 아래로 떨어지는 부분이나, 위로 오르는 부분이 Edge이다. 이 부분을 미분을 해보면 우측 그림 처럼 극값이 Edge라고 할 수 있다. 따라서 Edge 검출 방법은 미분을 이용하여 근처 픽셀과의 변화율을 찾아 검출하는 것이다. 변화율을 찾을 때 1차 미분, 2차 미분을 이용하는 방법이 있다. Edge Detection에선 1차 미분의 중앙 차분 방법을 사용하는데 이 방법을 영상 속 픽셀에 적용시키기 위하여 미분 마스크를 사용하게 된다. 미분 마스크는 가로(x), 세로(y) 방향이 있고 x,y 방향 둘 다 계산(1차 미분)을 하여 두 개의 결과를 합쳐서 일정 threshold를 기준으로 Edge를 검출 할 수 있게 된다. [그림 2] Edge Detection flow [그림 3] Mask Filter 미분 마스크의 여러 형태(마스크의 값)이 있는데 이 중 일반적으로 간단한 sobel을 많이 사용한다. 라이브러리 cv2의 Sobel 메서드를 사용하면 미분 마스크를 사용해볼 수 있다. import cv2 img = cv2.imread(&#34;some_img.png&#34;, cv2.IMREAD_GRAYSCALE) dx = cv2.Sobel(img,-1,1,0, delta = 128) # delta는 시각화를 위하여 dy = cv2.Sobel(img,-1,0,1, delta = 128) # delta는 시각화를 위하여 3*3 필터로 각각 x방향, y방향 편미분을 하는 방식 Region Growing 영역 기반 방식에서 가장 많이 사용되는 방식이 region-growing 알고리즘이다. 이 방식은 **기준 픽셀(SEED)**을 정하고 기준 픽셀과 비슷한 속성을 갖는 픽셀로 영역을 확장하여 더 이상 속성을 갖는 것들이 없으면 확장을 마치는 방식이다. **기준 픽셀(SEED)**를 정하는 방식은 Thresholding &amp; Morphological등의 방법이 사용될 수 있다. Thresholding을 사용하여 그림을 추출 한 후 구체적인 포인트를 뽑기 위해 Morphological Erosion을 하는 방식이 있다. 시드포인트를 정한 후 확장하는 방법도 다양하다. 원래의 시드 픽셀과 비교 기존 시드 픽셀과 비교하여 일정 범위 이내가 되면 영역을 확장하는 방법. (잡음에 민감, seed에 민감) 확장된 위치의 픽셀과 비교 원래 시드 픽셀의 위치가 아니라 영역이 커지는 만큼 시드 픽셀도 방향에 맞게 같이 커지는 방식. [그림 4] Region Growing Reference https://faceyourfear.tistory.com/78 https://gaussian37.github.io/vision-concept-edge_detection/ https://jstar0525.tistory.com/53#0.-edge https://blog.naver.com/laonple/220875555860 https://blog.naver.com/laonple/220890216653 https://medium.com/dawn-cau/region-based-segmentation-c1b2e06a3e2f ]]></description>
</item>
<item>
    <title>[DL]Image Segmentation 2</title>
    <link>https://goodyoung.github.io/posts/segmentation/segmentation-2/</link>
    <pubDate>Tue, 02 Apr 2024 13:53:37 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/segmentation/segmentation-2/</guid>
    <description><![CDATA[개요 이전 segmentation 관련 글에 대해 조사를 해보니 segmentation을 위한 다양한(전통적인) 이미지 방법론(전처리)이 많은 사실을 알게 되었다. 따라서 이에 대해 정리를 해보려고 한다. Image segmentation은 접근 방법에 따라 3가지로 나뉜다. 픽셀 기반 방법 thresholding에 기반한 방식으로 threshold에 따라 픽셀 별로 이진화를 할 때 많이 사용한다. 전역, 지역으로 적용하는 영역에 따른 구분도 가능하고, 적응적, 고정 방식으로 경계값을 설정하는 방식으로도 구분 가능하다. Thresholding 방식 등이 있다. Edge 기반 방법 Edge를 추출하는 필터 등을 사용하여 영상으로부터 경계를 추출하는 방법이다. Edge Detection 방법 등이 있다. 영역 기반 방법 위 두 방식은 데이터 noise에 민감하다. 영역 기반 방법은 동질성에 기반하여 다른 데이터 보다 의미있는 영역으로 나누는데 적합하지만 동질성을 규정하는 rule을 어떻게 정할 것인가는 문제이다. region growing, region merging 방법 등이 있다. 따라서 Image segmentation의 기본적인 방법론 픽셀 기반 방법에 대해 설명하고자 한다. Thresholding 영상 처리에서 Thresholding은 영상(Image)을 분할하는 가장 간단한 방법이다. Thresholding은 말 그대로 특정 임계값을 정하고 그보다 작으면 0, 크면 1로 이진화를 하는 간단한 방법이다. T(Threshold)가 한개만 적용한 경우
T(Threshold)가 두개 적용한 경우
그럼 T의 값은 어떻게 결정할까? 다음으로 T의 값을 설정하는 3가지 방법에 대해서 알아볼 것이다. Global Thresholding 첫번째 방법으로는 Global Thresholding 방법이 있다. 이 방법은 영상 픽셀값의 누적 분포를 알 수 있는 히스토그램을 사용한다.
히스토그램의 peak(정점)의 분포를 확인하여 적절한 임계값을 찾아내는 방식이다. 아래는 임계값을 찾는 예시이다. [그림 1] Gray Scale Image [그림 1]을 보면 Gray Image가 있는 것을 확인 할 수 있다. 우리는 이 Image의 픽셀을 히스토그램으로 시각화 하여 peak를 찾을 것이다. [그림 2] Gray Scale Image Histogram [그림 2]를 보게 되면 [그림 1]의 픽셀을 빈도수 별로 나누어 히스토그램을 그린 것 이다. Peak가 크게 2개 있는 것을 확인이 가능하고 우리는 그 사이 값으로 임계값을 설정하면 된다. [그림 3] Final Image [그림 3]은 임계값을 225으로 잡고 Thresholding을 적용시킨 Image이다. 영상속 모양을 가진 물체들을 잘 탐지한 것을 볼 수 있게 된다.
관련 코드는 GITHUB에 있다. Otsu Thresholding 두번째 방법으로는 Otsu Thresholding 방법이 있다. 이 방법은 peak의 경계가 명확하지 않고, peak 근처에 데이터가 몰려있지 않은 경우, 즉 히스토그램만 분석해서 잘 안나오는 영상에 대해서 최적의 T를 결정하는 방법이다. Otsu Thresholding방법은 오츠의 이진화 알고리즘을 이용하여 자동으로 T의 값을 찾는 방법으로 알려져있다. 영상에서 임의의 T값을 기준으로 하여 T보다 작은 픽셀 (T보다 어두운 픽셀)과 T와 같거나 큰 픽셀을 각각 다른 2개의 그룹으로 나누어 각각의 평균과 표준편차를 구한다. Otsu Thresholding은 2개 그룹 사이의 그룹 내 분산을 최소화 하거나 혹은 그룹 간 분산을 최대로 하는 방향으로 T를 정하면, 가장 적절한 임계값을 자동으로 얻을 수 있게 된다. 왜냐하면 그룹 내로는 비슷한 분포를(분산 최소) 보이고, 그룹 간으로 다른 성격을 띄는(분산이 최대) 분포를 보이면 그 T값이 우리가 위에서(Thresholding) 정한 histogram에서 peak을 보고 T를 정한 것이랑 같기 때문이다. 위 그림을 Otsu Thresholding을 사용하여 처리하는 과정을 나타낸 예시이다.
[그림 4] Otsu Thresholding Histogram 16. 원본 그림의 픽셀 분포를 나타낸 것이며, Otsu Thresholding을 통해 가장 적절한 T의 값을 찾은 것이다. [그림 4]에 있는 그래프를 봤을 때 Thresholding방식으로 바로 적절한 T의 값을 찾기란 번거로웠을 것이다. [그림 5] Otsu Thresholding Output [그림 5]는 T값을 기준으로 이진 변환을 한 결과이다. 관련 코드는 GITHUB에 있다.
Thresholding이나 Otsu Thresholding방식은 전역 임계값을 사용한 것이다. 전역 임계값은 단순성과 효율성에서 이점이 있지만 조명이나 특정 noise, pixel 분포가 복잡한 영상에 상당한 변화가 있는 영상에는 적합하지 않을 수도 있다.]]></description>
</item>
<item>
    <title>[DL]Image Segmentation 1</title>
    <link>https://goodyoung.github.io/posts/segmentation/segmentation-1/</link>
    <pubDate>Wed, 20 Mar 2024 22:03:00 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/segmentation/segmentation-1/</guid>
    <description><![CDATA[개요 컴퓨터 비전(Computer Vision)의 분야에 있어서 대표적인 기술은 객체 분류(Object Classification), 객체 탐지 및 위치 식별(Object Detection &amp; Localization), 객체 분할 (Object Segmentation), 이미지 캡셔닝(Image captioning)등이 있다. 이런 다양한 기술 중 하나인 이미지 분할(Image Segmentation)에 대해서 설명하고자 한다. 이미지 분할(Image Segmentation) [그림 1] Image Segmentation의 예 이미지 분할(Image Segmentation)은 신경망을 훈련시켜 이미지를 픽셀 단위로 분할하는 것이다. 위의 그림과 같이 이미지를 분할하게 되면 각각의 픽셀을 라벨(범주)별로 분류하는 작업을 거치게 된다. 이미지 분할(Image Segmentation)은 이미지 탐지(Image Detection)과는 다른 문제이다. 위의 그림 처럼 사람(Person)이라는 라벨을 가진 이미지에서도 여러 사람들을 분류 하는 것이 이미지 탐지(Image Detection)이다. 이미지 분할(Image Segmentation)은 여러 사람들을 사람이라는 하나의 객체(라벨) 안에 각각 다른 하나의 객체로 구분한다. 다음으론 이런 이미지 분할(Image Segmentation)도 Semantic Segmentation, Instance Segmentation, Panoptic Segmentation등 다양한 종류가 존재하는데 이에 대해 설명하고자 한다. 의미적 분할(Semantic Segmentation) [그림 2] Sementic Segmentation의 예 의미적 분할은 Semantic: '의미론'의 단어 뜻 처럼 이미지 내에 있는 객체들을 의미있는 단위로 분할하는 것을 말한다. 이미지를 봤을 때 큰 의미로 분할을 하게 된다. (위의 예시로 보자면 큰 객체들 위주: 사람, 자전거, 배경) 하지만 의미적 분할은 큰 의미로만 분할을 하지 하나의 객체 안에서 개별 객체로 구분하지는 않는다.(이미지에서 다른 사람이더라도 상관없이 같은 라벨로 표현) 따라서 이를 보완하기 위해 나타난 것이 인스턴스 분할(Instance Segmentation) 이다. 인스턴스 분할(Instance Segmentation) [그림 3] Instance Segmentation의 예 인스턴스 분할은 객체별로 각각 다른 라벨을 가지게 한다. 또한 각 객체의 경계 또한 식별이 가능하게 된다. 인스턴스 분할은 의미적 분할처럼 각 픽셀별로 어떤 카테고리에 속하는지 계산하는 것이 아닌 각 픽셀별로 object가 있는지 없는지 여부만 계산한다.
2-stage detector(Mask R-CNN)에서 객체들을 bounding box를 통해 localization을 시킨다. 그 후 localize된 RoI마다 class의 개수 만큼 binary mask(instance인지 아닌지) 마스크를 씌워준다. RoI별로 class 개수만큼 output 채널이 존재하고 동일 class더라도 서로다른 객체(instance)로 판별. (-&gt; 객체 별 각각 다른 객체) 즉, RoI가 focus하는 객체(instance)부분만 value를 갖도록 한다. 용어 설명 2-stage detector: localization과 classification이 순차적으로 이루어지는 방식 Localization: 객체 주변의 bounding box를 식별하여 객체의 위치를 찾는 방식 RoI: 관심 영역, 이미지 상에서 관심 있는 영역
따라서 인스턴스 분할은 background와 같이 구분하기 애매한 것들은 제외시키고 object를 대상으로 한다. 파놉틱 분할(Panoptic Segmentation) [그림 4] Panoptic Segmentation의 예 파놉틱 분할은 위 두가지 방법인 의미적, 인스턴스 분할을 합친 개념이다. 즉, 모든 pixel에 대해 라벨을 분류하고 객체 별 각각 다른 객체로 판별하는 과정을 수행하는 것이다. Image Segmentation 활용 분야 의학 사진(Medical Image)에서 많이 활용되고 있다. 예를 들어 환자의 체내 이미지에서 다양한 질병을 감지하는데 사용되고 있다. [그림 5] Image Segmentation의 활용 예-1 자율 주행 자동차처럼 자기 스스로 이미지를 분석하여 움직이는 것들은 이미지 분할의 도움을 받을 수 있게 된다. [그림 5] Image Segmentation의 활용 예-2 Reference https://velog.io/@dongho5041/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%84%B8%EA%B7%B8%EB%A9%98%ED%85%8C%EC%9D%B4%EC%85%98Image-Segmentation https://velog.io/@lighthouse97/%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%84%B8%EA%B7%B8%EB%A9%98%ED%85%8C%EC%9D%B4%EC%85%98Image-Segmentation https://velog.io/@claude_ssim/%EC%8B%9C%EA%B0%81%EC%A7%80%EB%8A%A5-Instance-segmentation https://ganghee-lee.tistory.com/44 https://ganghee-lee.tistory.com/34 ]]></description>
</item>
</channel>
</rss>
