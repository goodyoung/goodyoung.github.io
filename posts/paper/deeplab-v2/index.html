<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2) - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)" />
<meta property="og:description" content="Abstract Atrous convolution
효과적으로 FOV(Field of View)를 확장시킨다. (파라미터의 수 증가 없이 더 많은 context들을 포함하여 reception field를 증가시킨다.) Atrous spatial pyramid pooling (ASPP)
multi scale로 image context를 효과적으로 포착 가능 따라서 robust하게 객체를 분할 할 수 있다. CRF
max-pooling, downsampling의 결합이 배치되어 invariance가 있던 것들을 CRF를 사용하면서 질, 양적으로 localization 성능을 향상시켰다. Introduction DCNN이 classification, object detection에서 많이 사용된다. hand-crafted feature보다 더 좋은 성능을 나타내기 때문에 이 성공을 이끈 것은 image 변형에 대한 추상적인 data representation을 학습하게 하는 built-in invariance이다. 이 invariance는 classification task에는 좋지만 spatial information이 undesired한 segmentation에는 방해가 된다.
그래서 우리는 Segmentation을 위한 DCNN에 아래 3가지 사항을 고려했다.
reduce feature resolution existence of object at multiple scale reduce localization accuracy 첫번째 challenge는 max-pooling과 downsampling이 반복되어서 나타낸다. 이는 spatial resolution을 줄이기 때문에 안좋다. 이를 해결하기 위해서 우린 맨 마지막 단의 몇개의 maxpooling layer를 제거하고 높은 sampling 비율( 더 높은 해상도)로 계산하기 위하여 대신에 upsample 하는 filter를 추가한다. 이때 filter는 hole algorithm을 사용한 atrous convolution을 추가한다. 이것을 이용하여 실제 atrous convolution과 hole algorithm을 사용하여 계산량을 증가시키지 않고 FOV를 증가시켰다.
두번째 challenge는 기존의 방법은 동일한 이미지를 rescale버전을 DCNN에 입력한 후 얻은 feature map을 aggregate한다. 이 방법은 performance는 증가하지만 computing overhead가 발생한다. 따라서 본 논문에서는 spatial pyramid pooling을 사용하여 convolution 전에 주어진 feature layer를 resampling 하는 효율적인 구조를 제시한다. 이것은 원본 이미지를 여러 필터로 보는 것과 같아서 효율적인 FOV, useful한 multi scale의 관점에서 다양한 image context를 포착할 수 있다. 우린 병렬로 atrous convolutional layer를 사용한다. 이를 &lsquo;atrous spatial pyramid pooling&rsquo; (ASPP)라고 불린다.
세번째 challenge는 object-centric classifier는 공간적 변형에 대한 불변성이 필요하다. 이를 해결하기 위해선 마지막 segmentation결과를 계산하기 위한 hyper-column의 특징을 뽑는 skip layer를 사용하여 해결한다. CRF를 이용하여 미세한 디테일을 감지하는 능력을 키운다. 미세한 edge detail을 포착하기 위하여 fully connected pairwise CRF를 사용한다. 또한, CRF랑 DCNN이랑 연결했을 때 SOTA를 달성함.
따라서 본 논문에서 dcnn 모델로 VGG-16, ResNet-101을 사용했고 Fully convolutional 한 층을 사용한다. 그리고 feature resolution을 증가시키기 위해 atrous convolutional layer을 사용하고 8 pixel로 계산한다. bi-linear interpolation을 사용하여 upsample을 하고 다시 CRF를 사용하여 segmentation result를 도출하게 된다.
이건 결론적으로 빠르고, 정확하고, 간단하다. 기존보다 더 좋은 버전으로 나온다. 결론적으로 multi scale input processin (ASPP)를 사용하고 DCNN을 resnet 사용으로 변경하여 SOTA Methods Atrous Convolution for Dense Feature Extraction and Field-of-View Enlargement atrous convolution의 기존 식은 이것이다. 하지만 atrous convolution을 사용 하기 위해 아래 그림과 같이 r의 값을 조절하여 high resolution input feature map에도 사용할 수 있다.
2-D일때의 feature map의 특징을 보면 더욱 뚜렷한 것을 확인할 수 있다.
기존 방식대로 커널을 사용하게 된다면 1/4 의 이미지의 위치에 있는 데이터만 얻을 수 있게 된다. 하지만 atrous convolution을 사용하게 된다면 모든 이미지에 대한 정보를 얻을 수 있기에 공간 resolution이 증가한다.
atorous convolutional layer를 모든 층에 사용하기엔 비용적 오버헤드가 발생하여 하이브리드 전략을 취한다. 따라서 본 논문에서는 마지막 풀링, convolution layer에 atrous convolution layer를 추가하여 (stride = 2) 4배로 늘리고, 이중 선형 보간법을 사용해 8배로 늘려 원래 이미지 해상도에서 특징 맵을 복원한다.
이런 atrous convolution은 어떤 레이어에서든 field of view를 임의로 확대할 수 있게 된다. 따라서 본 논문에서 VGG-16의 fc6 레이어에서 r = 12로 설정하면 성능이 크게 향상이 되었음이 나온다.
첫 번째 방법은 필터를 업샘플링하여 구멍(0)을 삽입하거나, 동등하게 입력 특징 맵을 희소하게 샘플링하는 것입니" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/paper/deeplab-v2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-04T21:22:47+09:00" />
<meta property="article:modified_time" content="2024-07-04T21:22:47+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)"/>
<meta name="twitter:description" content="Abstract Atrous convolution
효과적으로 FOV(Field of View)를 확장시킨다. (파라미터의 수 증가 없이 더 많은 context들을 포함하여 reception field를 증가시킨다.) Atrous spatial pyramid pooling (ASPP)
multi scale로 image context를 효과적으로 포착 가능 따라서 robust하게 객체를 분할 할 수 있다. CRF
max-pooling, downsampling의 결합이 배치되어 invariance가 있던 것들을 CRF를 사용하면서 질, 양적으로 localization 성능을 향상시켰다. Introduction DCNN이 classification, object detection에서 많이 사용된다. hand-crafted feature보다 더 좋은 성능을 나타내기 때문에 이 성공을 이끈 것은 image 변형에 대한 추상적인 data representation을 학습하게 하는 built-in invariance이다. 이 invariance는 classification task에는 좋지만 spatial information이 undesired한 segmentation에는 방해가 된다.
그래서 우리는 Segmentation을 위한 DCNN에 아래 3가지 사항을 고려했다.
reduce feature resolution existence of object at multiple scale reduce localization accuracy 첫번째 challenge는 max-pooling과 downsampling이 반복되어서 나타낸다. 이는 spatial resolution을 줄이기 때문에 안좋다. 이를 해결하기 위해서 우린 맨 마지막 단의 몇개의 maxpooling layer를 제거하고 높은 sampling 비율( 더 높은 해상도)로 계산하기 위하여 대신에 upsample 하는 filter를 추가한다. 이때 filter는 hole algorithm을 사용한 atrous convolution을 추가한다. 이것을 이용하여 실제 atrous convolution과 hole algorithm을 사용하여 계산량을 증가시키지 않고 FOV를 증가시켰다.
두번째 challenge는 기존의 방법은 동일한 이미지를 rescale버전을 DCNN에 입력한 후 얻은 feature map을 aggregate한다. 이 방법은 performance는 증가하지만 computing overhead가 발생한다. 따라서 본 논문에서는 spatial pyramid pooling을 사용하여 convolution 전에 주어진 feature layer를 resampling 하는 효율적인 구조를 제시한다. 이것은 원본 이미지를 여러 필터로 보는 것과 같아서 효율적인 FOV, useful한 multi scale의 관점에서 다양한 image context를 포착할 수 있다. 우린 병렬로 atrous convolutional layer를 사용한다. 이를 &lsquo;atrous spatial pyramid pooling&rsquo; (ASPP)라고 불린다.
세번째 challenge는 object-centric classifier는 공간적 변형에 대한 불변성이 필요하다. 이를 해결하기 위해선 마지막 segmentation결과를 계산하기 위한 hyper-column의 특징을 뽑는 skip layer를 사용하여 해결한다. CRF를 이용하여 미세한 디테일을 감지하는 능력을 키운다. 미세한 edge detail을 포착하기 위하여 fully connected pairwise CRF를 사용한다. 또한, CRF랑 DCNN이랑 연결했을 때 SOTA를 달성함.
따라서 본 논문에서 dcnn 모델로 VGG-16, ResNet-101을 사용했고 Fully convolutional 한 층을 사용한다. 그리고 feature resolution을 증가시키기 위해 atrous convolutional layer을 사용하고 8 pixel로 계산한다. bi-linear interpolation을 사용하여 upsample을 하고 다시 CRF를 사용하여 segmentation result를 도출하게 된다.
이건 결론적으로 빠르고, 정확하고, 간단하다. 기존보다 더 좋은 버전으로 나온다. 결론적으로 multi scale input processin (ASPP)를 사용하고 DCNN을 resnet 사용으로 변경하여 SOTA Methods Atrous Convolution for Dense Feature Extraction and Field-of-View Enlargement atrous convolution의 기존 식은 이것이다. 하지만 atrous convolution을 사용 하기 위해 아래 그림과 같이 r의 값을 조절하여 high resolution input feature map에도 사용할 수 있다.
2-D일때의 feature map의 특징을 보면 더욱 뚜렷한 것을 확인할 수 있다.
기존 방식대로 커널을 사용하게 된다면 1/4 의 이미지의 위치에 있는 데이터만 얻을 수 있게 된다. 하지만 atrous convolution을 사용하게 된다면 모든 이미지에 대한 정보를 얻을 수 있기에 공간 resolution이 증가한다.
atorous convolutional layer를 모든 층에 사용하기엔 비용적 오버헤드가 발생하여 하이브리드 전략을 취한다. 따라서 본 논문에서는 마지막 풀링, convolution layer에 atrous convolution layer를 추가하여 (stride = 2) 4배로 늘리고, 이중 선형 보간법을 사용해 8배로 늘려 원래 이미지 해상도에서 특징 맵을 복원한다.
이런 atrous convolution은 어떤 레이어에서든 field of view를 임의로 확대할 수 있게 된다. 따라서 본 논문에서 VGG-16의 fc6 레이어에서 r = 12로 설정하면 성능이 크게 향상이 되었음이 나온다.
첫 번째 방법은 필터를 업샘플링하여 구멍(0)을 삽입하거나, 동등하게 입력 특징 맵을 희소하게 샘플링하는 것입니"/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/paper/deeplab-v2/" /><link rel="prev" href="https://goodyoung.github.io/posts/cs231n/lecture3/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/paper\/deeplab-v2\/"
        },"genre": "posts","keywords": "UNet, Implement, 논문 리뷰, computer vision, segmentation","wordcount":  505 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/paper\/deeplab-v2\/","datePublished": "2024-07-04T21:22:47+09:00","dateModified": "2024-07-04T21:22:47+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-07-04">2024-07-04</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;505 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;3 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#methods">Methods</a>
      <ul>
        <li><a href="#atrous-convolution-for-dense-feature-extraction-and-field-of-view-enlargement">Atrous Convolution for Dense Feature Extraction and Field-of-View Enlargement</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="abstract">Abstract</h2>
<ul>
<li>
<p>Atrous convolution</p>
<ul>
<li>효과적으로 FOV(Field of View)를 확장시킨다. (파라미터의 수 증가 없이 더 많은 context들을 포함하여 reception field를 증가시킨다.)</li>
</ul>
</li>
<li>
<p>Atrous spatial pyramid pooling (ASPP)</p>
<ul>
<li>multi scale로 image context를 효과적으로 포착 가능</li>
<li>따라서 robust하게 객체를 분할 할 수 있다.</li>
</ul>
</li>
<li>
<p>CRF</p>
<ul>
<li>max-pooling, downsampling의 결합이 배치되어 invariance가 있던 것들을 CRF를 사용하면서 질, 양적으로 localization 성능을 향상시켰다.</li>
</ul>
</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>DCNN이 classification, object detection에서 많이 사용된다.
<ul>
<li>hand-crafted feature보다 더 좋은 성능을 나타내기 때문에
이 성공을 이끈 것은 image 변형에 대한 추상적인 data representation을 학습하게 하는 built-in invariance이다.</li>
</ul>
</li>
</ul>
<p>이 invariance는 classification task에는 좋지만 spatial information이 undesired한 segmentation에는 방해가 된다.</p>
<p>그래서 우리는 Segmentation을 위한 DCNN에 아래 3가지 사항을 고려했다.</p>
<ul>
<li>reduce feature resolution</li>
<li>existence of object at multiple scale</li>
<li>reduce localization accuracy</li>
</ul>
<p>첫번째 challenge는 max-pooling과 downsampling이 반복되어서 나타낸다.
이는 spatial resolution을 줄이기 때문에 안좋다.
이를 해결하기 위해서 우린 맨 마지막 단의 몇개의 maxpooling layer를 제거하고 높은 sampling 비율( 더 높은 해상도)로 계산하기 위하여 대신에 upsample 하는 filter를 추가한다.
이때 filter는 hole algorithm을 사용한 atrous convolution을 추가한다.
이것을 이용하여 실제 atrous convolution과 hole algorithm을 사용하여 계산량을 증가시키지 않고 FOV를 증가시켰다.</p>
<p>두번째 challenge는 기존의 방법은 동일한 이미지를 rescale버전을 DCNN에 입력한 후 얻은 feature map을 aggregate한다.
이 방법은 performance는 증가하지만 computing overhead가 발생한다.
따라서 본 논문에서는 spatial pyramid pooling을 사용하여 convolution 전에 주어진 feature layer를 resampling 하는 효율적인 구조를 제시한다.
이것은 원본 이미지를 여러 필터로 보는 것과 같아서 효율적인 FOV, useful한 multi scale의 관점에서 다양한 image context를 포착할 수 있다.
우린 병렬로 atrous convolutional layer를 사용한다. 이를 &lsquo;atrous spatial pyramid pooling&rsquo; (ASPP)라고 불린다.</p>
<p>세번째 challenge는 object-centric classifier는 공간적 변형에 대한 불변성이 필요하다.
이를 해결하기 위해선 마지막 segmentation결과를 계산하기 위한 hyper-column의 특징을 뽑는 skip layer를 사용하여 해결한다.
CRF를 이용하여 미세한 디테일을 감지하는 능력을 키운다.
미세한 edge detail을 포착하기 위하여 fully connected pairwise CRF를 사용한다. 또한, CRF랑 DCNN이랑 연결했을 때 SOTA를 달성함.</p>
<p>따라서 본 논문에서 dcnn 모델로 VGG-16, ResNet-101을 사용했고 Fully convolutional 한 층을 사용한다.
그리고 feature resolution을 증가시키기 위해 atrous convolutional layer을 사용하고 8 pixel로 계산한다.
bi-linear interpolation을 사용하여 upsample을 하고 다시 CRF를 사용하여 segmentation result를 도출하게 된다.</p>
<ul>
<li>이건 결론적으로 빠르고, 정확하고, 간단하다.
기존보다 더 좋은 버전으로 나온다.
결론적으로
multi scale input processin (ASPP)를 사용하고 DCNN을 resnet 사용으로 변경하여 SOTA</li>
</ul>
<h2 id="methods">Methods</h2>
<h3 id="atrous-convolution-for-dense-feature-extraction-and-field-of-view-enlargement">Atrous Convolution for Dense Feature Extraction and Field-of-View Enlargement</h3>
<p>atrous convolution의 기존 식은 이것이다.
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="image.png"
        data-srcset="image.png, image.png 1.5x, image.png 2x"
        data-sizes="auto"
        alt="image.png"
        title="alt text" /></p>
<p>하지만 atrous convolution을 사용 하기 위해 아래 그림과 같이 r의 값을 조절하여 high resolution input feature map에도 사용할 수 있다.</p>
<p>2-D일때의 feature map의 특징을 보면 더욱 뚜렷한 것을 확인할 수 있다.</p>
<p>기존 방식대로 커널을 사용하게 된다면 1/4 의 이미지의 위치에 있는 데이터만 얻을 수 있게 된다. 하지만 atrous convolution을 사용하게 된다면 모든 이미지에 대한 정보를 얻을 수 있기에 공간 resolution이 증가한다.</p>
<p>atorous convolutional layer를 모든 층에 사용하기엔 비용적 오버헤드가 발생하여 하이브리드 전략을 취한다.
따라서 본 논문에서는 마지막 풀링, convolution layer에 atrous convolution layer를 추가하여 (stride = 2) 4배로 늘리고, 이중 선형 보간법을 사용해 8배로 늘려 원래 이미지 해상도에서 특징 맵을 복원한다.</p>
<p>이런 atrous convolution은 어떤 레이어에서든 field of view를 임의로 확대할 수 있게 된다.
따라서 본 논문에서 VGG-16의 fc6 레이어에서 r = 12로 설정하면 성능이 크게 향상이 되었음이 나온다.</p>
<p>첫 번째 방법은 필터를 업샘플링하여 구멍(0)을 삽입하거나, 동등하게 입력 특징 맵을 희소하게 샘플링하는 것입니</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2024-07-04</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/paper/deeplab-v2/" data-title="[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)" data-hashtags="UNet,Implement,논문 리뷰,computer vision,segmentation"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/paper/deeplab-v2/" data-hashtag="UNet"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/paper/deeplab-v2/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/paper/deeplab-v2/" data-title="[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/paper/deeplab-v2/" data-title="[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/paper/deeplab-v2/" data-title="[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/paper/deeplab-v2/" data-title="[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/unet/">UNet</a>,&nbsp;<a href="/tags/implement/">Implement</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/segmentation/">segmentation</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/cs231n/lecture3/" class="prev" rel="prev" title="[CS231n] 03.Loss Functions and Optimization"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[CS231n] 03.Loss Functions and Optimization</a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.121.2">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2022 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://goodyoung.github.io" target="_blank">GoodYoung</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
