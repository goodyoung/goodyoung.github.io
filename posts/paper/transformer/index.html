<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[Paper Review]Attention Is All You Need - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[Paper Review]Attention Is All You Need" />
<meta property="og:description" content="개요 요즘 모든 분야에서 자주 사용되고 있는 아키텍쳐인 Transformer에 대한 논문 리뷰를 해보려고 한다. Abstract 기존의 sequence transduction model(sequence 간 변형이 이루어 지는)은 complex recurrent나 cnn기반으로 구성되어 있다. 그리고 좋은 성능을 내는 것이 attention 기반에 encoder &amp; decoder 기반으로 연결되어 있는 네트워크이다. 본 논문에서는 오로지 attention mechanism만 사용하는 Transformer를 제안한다. 즉, 순환이나 cnn의 연산을 사용하지 않는 네트워크이다. 이는 훈련시간 절감, 행렬곱을 이용하여 병렬적으로 수행하면서 번역 작업에 우수한 품질을 보여준다. English-German 번역 작업에 28.4 BLEU를 달성하였고 다른 task에서도 잘 작동한다. Introduction RNN, long-term memory, GRU이 language modeling, machine translation 문제에서 SOTA를 달성하고 있다. 많은 연구가 encoder-decoder의와 순환 language model의 성과를 달성하려고 하고 있다. 순환 모델의 연산 특성은 training sample 내에서 병렬화를 방해하며, 시퀀스 길이가 길어질수록 메모리 제약으로 인해 예제 간 배칭이 제한되므로 병렬화가 중요해집니다. 기존 RNN의 단점 따라서 기존의 RNN의 단점에 대해서 간략히 살펴 보겠다. 위 그림처럼 sequence에 포함되어 있는 순서 정보를 정렬하고 이것을 차례대로 hidden state의 값을 반복을 통하여 갱신하기 때문에 병렬적으로 수행하기에 어렵다는 사실을 알 수 있게 된다. 또한 input 단어가 많아지면 encoder의 마지막 부분의 출력인 context vector를 만들어야 하기 때문에 병목의 문제도 있다. 다시 논문 내용으로 넘어가면 Factorization trick과 conditional computation의 연구로 인해 이러한 문제점은 상당히 해결되고 있지만 여전히 sequential computation은 제약이다. Attention은 input과 output의 거리에 관여하지 않고 model의 종속성을 허락해준다. 이러한 attention mechanism은 주로 순환 신경망과 함께 사용이 되어 왔다. Attention RNN처럼 하나의 context vector가 소스 문장의 모든 정보를 가지고 있어야하는 단점을 해결하기 위해 나온 기법이 Attention이다. Attention은 매번 출력 단어를 만들어 낼 때마다 소스 문장의 출력 정보 중에서 어떤 정보가 가장 중요한지 가중치를 부여해서 출력 단어를 보다 효과적으로 생성할 수 있게 한다. 간단히 매번 소스 문장에서의 출력 전부를 decoder의 입력으로 받는 방식이다. 결론적으로 본 논문에서 제안하는 Transformer는 순환을 피하고 대신에 attention mechanism에 온전히 의존하여 input과 output 사이의 ``global dependencies를 이끈다. 이러한 구조는 병렬 처리에 특화되었다. Background 기존 연구들은 CNN을 사용하여 sequenrtial computation을 줄이는데 초점을 맞추었지만 이 방법은 다른 position 사이의 종속성을 배우기 쉽지 않다. 하지만 Multi-Head Attention의 방법을 사용하여 상수 시간으로 줄였다. 또한 기존 순환 end-to-end memory network는 attention mechanism을 사용했다. 하지만 Transformer에서는 RNN이나 CNN을 사용하지 않고 input output의 representation을 계산하는 전체의 self-attention을 사용한다. 여기서 나온 Self-Attention이란 input으로 들어오는 각각의 단어가 서로에게 얼만큼의 영향을 미치는지 알려준다. 문맥에 대한 정보를 잘 학습하도록 만드는 것이다. Model Architecture 기존 모델들은 encoder-decoder구조를 따르는데 이전 단계에서 생성한 symbol을 활용해서 decoder가 다음번에 나올 output을 만든다. Transformer 또한 encoder-decoder의 구조를 띄는데 모델을 순환적으로 사용하지 않고 attention mechanism만 활용하여 sequence에 대한 정보를 한 번에 입력으로 준다는 것이 특징이다. 위의 그림은 transformer의 전체적인 구조인데 이를 앞으로 하나씩 살펴볼 예정이다. Encoder and Decoder Stacks Encoder부분은 여러번 stack이 가능한 구조이고 하나의 layer는 크게 두가지의 구조를 지닌다. multi-head self attention feed-forward network 또한 이 두가지의 구조 모두 residual connection을 활용하여 Identity mapping을 거치게 한다. Decoder부분은 여러번 stack이 가능한 구조이고 보통 encoder랑 같은 layer의 개수랑 맞춘다. Decoder third sub-layer에 encoder의 output값을 활용하여 multi-head attention을 수행하도록 한다. residual connection을 활용하여 보다 더 쉽게 global optima를 찾도록 한다. 이전에 등장한 단어들만 참고할 수 있도록 mask를 씌워서 multi-head attention을 사용할 수 있도록 한다. Attention Attention Function은 쿼리와 key-value 쌍을 output으로 mapping을 한다. Query, keys, values, output들은 다 vector이다. Query(Q): 물어보는 주체 (어떤 단어가 가장 중요했는지를 key에서 계산하여 결과를 낸다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/paper/transformer/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-24T11:32:59+09:00" />
<meta property="article:modified_time" content="2024-07-24T11:32:59+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[Paper Review]Attention Is All You Need"/>
<meta name="twitter:description" content="개요 요즘 모든 분야에서 자주 사용되고 있는 아키텍쳐인 Transformer에 대한 논문 리뷰를 해보려고 한다. Abstract 기존의 sequence transduction model(sequence 간 변형이 이루어 지는)은 complex recurrent나 cnn기반으로 구성되어 있다. 그리고 좋은 성능을 내는 것이 attention 기반에 encoder &amp; decoder 기반으로 연결되어 있는 네트워크이다. 본 논문에서는 오로지 attention mechanism만 사용하는 Transformer를 제안한다. 즉, 순환이나 cnn의 연산을 사용하지 않는 네트워크이다. 이는 훈련시간 절감, 행렬곱을 이용하여 병렬적으로 수행하면서 번역 작업에 우수한 품질을 보여준다. English-German 번역 작업에 28.4 BLEU를 달성하였고 다른 task에서도 잘 작동한다. Introduction RNN, long-term memory, GRU이 language modeling, machine translation 문제에서 SOTA를 달성하고 있다. 많은 연구가 encoder-decoder의와 순환 language model의 성과를 달성하려고 하고 있다. 순환 모델의 연산 특성은 training sample 내에서 병렬화를 방해하며, 시퀀스 길이가 길어질수록 메모리 제약으로 인해 예제 간 배칭이 제한되므로 병렬화가 중요해집니다. 기존 RNN의 단점 따라서 기존의 RNN의 단점에 대해서 간략히 살펴 보겠다. 위 그림처럼 sequence에 포함되어 있는 순서 정보를 정렬하고 이것을 차례대로 hidden state의 값을 반복을 통하여 갱신하기 때문에 병렬적으로 수행하기에 어렵다는 사실을 알 수 있게 된다. 또한 input 단어가 많아지면 encoder의 마지막 부분의 출력인 context vector를 만들어야 하기 때문에 병목의 문제도 있다. 다시 논문 내용으로 넘어가면 Factorization trick과 conditional computation의 연구로 인해 이러한 문제점은 상당히 해결되고 있지만 여전히 sequential computation은 제약이다. Attention은 input과 output의 거리에 관여하지 않고 model의 종속성을 허락해준다. 이러한 attention mechanism은 주로 순환 신경망과 함께 사용이 되어 왔다. Attention RNN처럼 하나의 context vector가 소스 문장의 모든 정보를 가지고 있어야하는 단점을 해결하기 위해 나온 기법이 Attention이다. Attention은 매번 출력 단어를 만들어 낼 때마다 소스 문장의 출력 정보 중에서 어떤 정보가 가장 중요한지 가중치를 부여해서 출력 단어를 보다 효과적으로 생성할 수 있게 한다. 간단히 매번 소스 문장에서의 출력 전부를 decoder의 입력으로 받는 방식이다. 결론적으로 본 논문에서 제안하는 Transformer는 순환을 피하고 대신에 attention mechanism에 온전히 의존하여 input과 output 사이의 ``global dependencies를 이끈다. 이러한 구조는 병렬 처리에 특화되었다. Background 기존 연구들은 CNN을 사용하여 sequenrtial computation을 줄이는데 초점을 맞추었지만 이 방법은 다른 position 사이의 종속성을 배우기 쉽지 않다. 하지만 Multi-Head Attention의 방법을 사용하여 상수 시간으로 줄였다. 또한 기존 순환 end-to-end memory network는 attention mechanism을 사용했다. 하지만 Transformer에서는 RNN이나 CNN을 사용하지 않고 input output의 representation을 계산하는 전체의 self-attention을 사용한다. 여기서 나온 Self-Attention이란 input으로 들어오는 각각의 단어가 서로에게 얼만큼의 영향을 미치는지 알려준다. 문맥에 대한 정보를 잘 학습하도록 만드는 것이다. Model Architecture 기존 모델들은 encoder-decoder구조를 따르는데 이전 단계에서 생성한 symbol을 활용해서 decoder가 다음번에 나올 output을 만든다. Transformer 또한 encoder-decoder의 구조를 띄는데 모델을 순환적으로 사용하지 않고 attention mechanism만 활용하여 sequence에 대한 정보를 한 번에 입력으로 준다는 것이 특징이다. 위의 그림은 transformer의 전체적인 구조인데 이를 앞으로 하나씩 살펴볼 예정이다. Encoder and Decoder Stacks Encoder부분은 여러번 stack이 가능한 구조이고 하나의 layer는 크게 두가지의 구조를 지닌다. multi-head self attention feed-forward network 또한 이 두가지의 구조 모두 residual connection을 활용하여 Identity mapping을 거치게 한다. Decoder부분은 여러번 stack이 가능한 구조이고 보통 encoder랑 같은 layer의 개수랑 맞춘다. Decoder third sub-layer에 encoder의 output값을 활용하여 multi-head attention을 수행하도록 한다. residual connection을 활용하여 보다 더 쉽게 global optima를 찾도록 한다. 이전에 등장한 단어들만 참고할 수 있도록 mask를 씌워서 multi-head attention을 사용할 수 있도록 한다. Attention Attention Function은 쿼리와 key-value 쌍을 output으로 mapping을 한다. Query, keys, values, output들은 다 vector이다. Query(Q): 물어보는 주체 (어떤 단어가 가장 중요했는지를 key에서 계산하여 결과를 낸다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/paper/transformer/" /><link rel="prev" href="https://goodyoung.github.io/posts/cs231n/lecture7/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[Paper Review]Attention Is All You Need",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/paper\/transformer\/"
        },"genre": "posts","keywords": "Self-Attention, 논문 리뷰, computer vision, Transformer","wordcount":  913 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/paper\/transformer\/","datePublished": "2024-07-24T11:32:59+09:00","dateModified": "2024-07-24T11:32:59+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[Paper Review]Attention Is All You Need</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-07-24">2024-07-24</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;913 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;5 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#기존-rnn의-단점">기존 RNN의 단점</a></li>
        <li><a href="#attention">Attention</a></li>
      </ul>
    </li>
    <li><a href="#background">Background</a></li>
    <li><a href="#model-architecture">Model Architecture</a></li>
    <li><a href="#encoder-and-decoder-stacks">Encoder and Decoder Stacks</a></li>
    <li><a href="#attention-1">Attention</a></li>
    <li><a href="#scaled-dot-product-attention">Scaled Dot-Product Attention</a></li>
    <li><a href="#multi-head-attention">Multi-Head Attention</a></li>
    <li><a href="#applications-of-attention-in-our-model">Applications of Attention in our Model</a></li>
    <li><a href="#position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</a></li>
    <li><a href="#positional-encoding">Positional Encoding</a></li>
    <li><a href="#why-self-attention">Why Self-Attention</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- image format
<div style="text-align:center;">
<img src="/images/cs231n/lecture4/back-1.png" height="100%" width="80%"> </div>
 -->
<h2 id="개요">개요</h2>
<ol start="0">
<li>요즘 <strong>모든 분야에서 자주 사용</strong>되고 있는 아키텍쳐인 <code>Transformer</code>에 대한 논문 리뷰를 해보려고 한다.</li>
</ol>
<hr>
<h2 id="abstract">Abstract</h2>
<ol>
<li>기존의 <code>sequence transduction model</code>(sequence 간 변형이 이루어 지는)은 <code>complex recurrent</code>나 <code>cnn</code>기반으로 구성되어 있다.</li>
</ol>
<br>
<ol start="2">
<li>그리고 좋은 성능을 내는 것이 <code>attention</code> 기반에 <code>encoder &amp; decoder</code> 기반으로 연결되어 있는 네트워크이다.</li>
</ol>
<br>
<ol start="3">
<li>본 논문에서는 <strong>오로지 attention mechanism만</strong> 사용하는 <code>Transformer</code>를 제안한다.
<ul>
<li>즉, <code>순환</code>이나 <code>cnn의 연산</code>을 <strong>사용하지 않는 네트워크이다.</strong></li>
</ul>
</li>
</ol>
<br>
<ol start="4">
<li>이는 <code>훈련시간 절감</code>, <strong>행렬곱을 이용하여 병렬적으로 수행</strong>하면서 번역 작업에 우수한 품질을 보여준다.</li>
</ol>
<br>
<ol start="5">
<li>English-German 번역 작업에 28.4 BLEU를 달성하였고  다른 task에서도 잘 작동한다.</li>
</ol>
<hr>
<h2 id="introduction">Introduction</h2>
<ol start="6">
<li><code>RNN</code>, <code>long-term memory</code>, <code>GRU</code>이 language modeling, machine translation 문제에서 SOTA를 달성하고 있다.</li>
</ol>
<br>
<ol start="7">
<li>많은 연구가 encoder-decoder의와 순환 language model의 성과를 달성하려고 하고 있다.</li>
</ol>
<br>
<ol start="8">
<li>순환 모델의 연산 특성은 <code>training sample</code> 내에서 병렬화를 <strong>방해하며</strong>, 시퀀스 길이가 길어질수록 <strong>메모리 제약으로</strong> 인해 예제 간 배칭이 제한되므로 <strong>병렬화가 중요해집니다.</strong></li>
</ol>
<h3 id="기존-rnn의-단점">기존 RNN의 단점</h3>
<div style="text-align:center;">
<img src="/images/paper/transformer/rnn.png" height="100%" width="80%"> </div>
<ol start="9">
<li>따라서 기존의 <code>RNN의 단점</code>에 대해서 간략히 살펴 보겠다.</li>
</ol>
<br>
<ol start="10">
<li>위 그림처럼 <code>sequence</code>에 포함되어 있는 순서 정보를 <strong>정렬하고</strong> 이것을 <strong>차례대로 hidden state의 값을 반복을 통하여 갱신하기 때문에</strong> 병렬적으로 수행하기에 어렵다는 사실을 알 수 있게 된다.</li>
</ol>
<br>
<ol start="11">
<li>또한 input 단어가 많아지면 <strong>encoder의 마지막 부분의 출력인</strong> <code>context vector</code>를 만들어야 하기 때문에 <code>병목</code>의 문제도 있다.</li>
</ol>
<br>
<ol start="12">
<li>다시 논문 내용으로 넘어가면 <code>Factorization trick</code>과 conditional computation의 연구로 인해 이러한 <strong>문제점은 상당히 해결되고</strong> 있지만 여전히 <code>sequential computation</code>은 제약이다.</li>
</ol>
<br>
<ol start="13">
<li><code>Attention</code>은 <code>input</code>과 <code>output</code>의 <strong>거리에 관여하지 않고</strong> <code>model의 종속성</code>을 허락해준다. 이러한 <code>attention mechanism</code>은 주로 <code>순환 신경망</code>과 함께 사용이 되어 왔다.</li>
</ol>
<h3 id="attention">Attention</h3>
<div style="text-align:center;">
<img src="/images/paper/transformer/attention.png" height="100%" width="80%"> </div>
<ol start="14">
<li><code>RNN</code>처럼 하나의 <code>context vector</code>가 소스 문장의 모든 정보를 가지고 있어야하는 단점을 해결하기 위해 나온 기법이 <code>Attention</code>이다.</li>
</ol>
<br>
<ol start="15">
<li><code>Attention</code>은 매번 출력 단어를 만들어 낼 때마다 소스 문장의 출력 정보 중에서 어떤 정보가 가장 중요한지 <code>가중치를</code> 부여해서<code> 출력 단어를 보다 효과적으로 생성</code>할 수 있게 한다.</li>
</ol>
<ul>
<li>간단히 매번 소스 문장에서의 출력 전부를 <code>decoder의 입력</code>으로 받는 방식이다.</li>
</ul>
<br>
<ol start="16">
<li>결론적으로 본 논문에서 제안하는 <code>Transformer</code>는 <code>순환을</code> 피하고 대신에 <code>attention mechanism</code>에 <strong>온전히 의존</strong>하여 <code>input과 output 사이의 ``global dependencies</code>를 이끈다. 이러한 구조는 <strong>병렬 처리에 특화되었다.</strong></li>
</ol>
<hr>
<h2 id="background">Background</h2>
<ol start="17">
<li>기존 연구들은 CNN을 사용하여 <strong>sequenrtial computation을 줄이는데 초점을 맞추었지만</strong> 이 방법은 <strong>다른 position 사이의 종속성을 배우기 쉽지 않다.</strong></li>
</ol>
<br>
<ol start="18">
<li>하지만 <code>Multi-Head Attention</code>의 방법을 사용하여 <code>상수 시간</code>으로 줄였다.</li>
</ol>
<br>
<ol start="19">
<li>또한 기존 순환 <code>end-to-end memory network</code>는 attention mechanism을 사용했다. 하지만 Transformer에서는 RNN이나 CNN을 사용하지 않고 <code>input output의</code> representation을 계산하는 <code>전체의 self-attention</code>을 사용한다.</li>
</ol>
<br>
<ol start="20">
<li>여기서 나온 <code>Self-Attention</code>이란 input으로 들어오는 각각의 단어가 <strong>서로에게 얼만큼의 영향을 미치는지 알려준다.</strong>
<ul>
<li><code>문맥</code>에 대한 정보를 잘 학습하도록 만드는 것이다.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="model-architecture">Model Architecture</h2>
<ol start="21">
<li>기존 모델들은 <code>encoder-decoder</code>구조를 따르는데 <strong>이전 단계에서 생성한 symbol을 활용해서</strong> decoder가 다음번에 나올 output을 만든다.</li>
</ol>
<br>
<ol start="22">
<li><code>Transformer</code> 또한 <code>encoder-decoder</code>의 구조를 띄는데 모델을 <code>순환적으로 사용하지 않고</code> <code>attention mechanism</code>만 활용하여 sequence에 대한 <strong>정보를 한 번에 입력으로 준다는 것이 특징이다.</strong></li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/paper/transformer/over.png" height="100%" width="80%"> </div>
<ol start="23">
<li>위의 그림은 <code>transformer</code>의 전체적인 구조인데 이를 앞으로 하나씩 살펴볼 예정이다.</li>
</ol>
<hr>
<h2 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h2>
<ol start="24">
<li><code>Encoder</code>부분은 <code>여러번 stack이 가능한 구조</code>이고 하나의 layer는 크게 <strong>두가지의 구조를</strong> 지닌다.</li>
</ol>
<ul>
<li>multi-head self attention</li>
<li>feed-forward network</li>
</ul>
<br>
<ol start="25">
<li>또한 이 두가지의 구조 모두 <code>residual connection</code>을 활용하여 <code>Identity mapping</code>을 거치게 한다.</li>
</ol>
<br>
<ol start="26">
<li><code>Decoder</code>부분은 <code>여러번 stack이 가능한 구조</code>이고 보통 encoder랑 같은 layer의 개수랑 맞춘다.</li>
</ol>
<br>
<ol start="27">
<li><code>Decoder third sub-layer</code>에 <code>encoder의</code> <code>output값을</code> 활용하여 <code>multi-head attention</code>을 수행하도록 한다.
<ul>
<li><code>residual connection</code>을 활용하여 보다 더 쉽게 <code>global optima</code>를 찾도록 한다.</li>
</ul>
</li>
</ol>
<br>
<ol start="28">
<li><strong>이전에 등장한 단어들만 참고할 수 있도록</strong> mask를 씌워서 <code>multi-head attention</code>을 사용할 수 있도록 한다.</li>
</ol>
<hr>
<h2 id="attention-1">Attention</h2>
<ol start="29">
<li><code>Attention Function</code>은 <code>쿼리</code>와 <code>key-value</code> 쌍을 output으로 mapping을 한다.
<ul>
<li><code>Query, keys, values, output들</code>은 다 <code>vector</code>이다.</li>
</ul>
<blockquote>
<p>Query(Q): 물어보는 주체 (어떤 단어가 가장 중요했는지를 key에서 계산하여 결과를 낸다.)</p>
<p>Key(K): 물어보는 대상</p>
</blockquote>
</li>
</ol>
<br>
<ol start="30">
<li>이는 모두 <code>weighted sum</code>으로 계산이 된다. 그래서 <code>Scaled Dot-Product Attention</code>에 대해서 알아볼 것이다.</li>
</ol>
<h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h2>
<div style="text-align:center;">
<img src="/images/paper/transformer/single.png" height="100%" width="40%"> </div>
<p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p>
<ol start="31">
<li><code>Attention</code>을 계산하기 위한 방법이다. <code>Input</code>은 <code>query와</code> <code>key의</code> dimension $d_k$, value의 dimension $d_v$로 구성이 되어있다.</li>
</ol>
<br>
<ol start="32">
<li>또한 <code>query</code>와 <code>key</code>에 대해 <code>dot연산을</code> 통해 구하고 $\sqrt{d_k}$로 나눈다.
<ul>
<li>이때 나누는 이유는 <strong>softmax로 들어가는 값</strong>을 <code>scaling</code>을 한다.</li>
</ul>
</li>
</ol>
<br>
<ol start="33">
<li>필요할 때 <code>mask vector</code>를 사용하고 최종적으로 각각의 key에 대해서 얼마나 중요한지에 대한 값을 <code>확률 형태(softmax)</code>로 표현한 뒤에 각각 실제 value랑 곱하여 <code>attention value</code>를 구하게 된다.</li>
</ol>
<hr>
<h2 id="multi-head-attention">Multi-Head Attention</h2>
<div style="text-align:center;">
<img src="/images/paper/transformer/multi.png" height="100%" width="40%"> </div>
<p>$$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O $$
$$ \ \text{where } \text{head}_i = \text{Attention}(QW_i^Q,KW_i^K, VW_i^V)  $$</p>
<ol start="34">
<li>$d_{model}$로 구성된 <code>single attention function</code>을 수행하는 대신에 다른 <code>linearly project</code>를 여러번 다르게 학습된 것이 <strong>더욱 효과적이라는 것을 알 수 있게 되었다.</strong></li>
</ol>
<br>
<ol start="35">
<li>그리고 이 다른 것 <code>attention function</code>들을 <strong>병렬적으로 수행하고 마지막에 합쳐진다.</strong>
<ul>
<li>이는 <strong>서로 다른 위치에서 다양한 표현 하위 공간의 정보를 동시에 주목(attend)할 수 있게 해줍니다.</strong></li>
</ul>
</li>
</ol>
<br>
<ol start="36">
<li>각 <code>head</code>마다 차원을 줄이기 때문에, <strong>전체 계산 비용은 전체 차원에 대한 single-head attention과 비슷한 결과를 보인다.</strong></li>
</ol>
<hr>
<h2 id="applications-of-attention-in-our-model">Applications of Attention in our Model</h2>
<ol start="37">
<li><code>Attention</code>은 본 논문에서 <code>3가지의</code> 다른 형태로 나타나게 된다.</li>
</ol>
<br>
<ol start="38">
<li><code>첫 번째 방법</code>은 <code>Encoder-decoder attention</code>방법이다. 이는 <code>각각의 출력 단어</code>가 <code>인코더의 출력 정보</code>를 받아와서 <code>attention</code>을 수행할 수 있게 만든다. 각각의 출력되고 있는 단어가 <strong>src문장에서의 어떤 단어와 연관성이 있는지를 구해주는 것이다.</strong>
<ul>
<li><code>Encoder파트</code>에 있는 attention의 정보가 <code>Key</code>, <code>Value</code>가 되고 decoder에 있는 것이 <code>Query</code>가 된다.</li>
</ul>
</li>
</ol>
<br>
<ol start="39">
<li><code>두 번째 방법은</code> <code>self attention</code>은 <code>query</code>, <code>key</code>, <code>value</code>가 다 이전 encoder에서 받아온 결과이다.
<ul>
<li><strong>즉, 각각의 단어가 서로에게 얼만큼의 영향을 미치는지 알려준다.</strong></li>
<li><code>문맥</code>에 대한 정보를 잘 학습하도록 만드는 것이다.</li>
<li><code>Self attention</code>의 관점에서 <code>'I am a teacher'</code>가 있을 때 <code>I가</code> 얼마나 연관성이 있는지 확인하고 싶은 경우 <code>I가</code> <code>Query가</code> 되고 <code>'I am a teacher'</code>의 <code>각각 단어가 Key</code>가 된다.</li>
</ul>
</li>
</ol>
<br>
<ol start="40">
<li><code>세 번째 방법은</code> <code>Masked decoder self-attention</code>방법이다. 이는 <code>mask dot 연산</code>을 하여 <strong>출력 단어가 앞쪽에 등장했던 단어만 참고할 수 있도록 만든다.</strong></li>
</ol>
<hr>
<h2 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h2>
<ol start="41">
<li>네트워크에 비선형성과 깊이를 추가하기 위하여 <code>feed forward network</code>를 추가한다.
<blockquote>
<p>Activation Function: ReLU</p>
</blockquote>
</li>
</ol>
<hr>
<h2 id="positional-encoding">Positional Encoding</h2>
<div style="text-align:center;">
<img src="/images/paper/transformer/position.png" height="100%" width="80%"> </div>
<ol start="42">
<li><code>Transformer</code>에서는 <code>RNN</code>을 사용하지 않고 <code>attension mechanism</code>만 사용하기 때문에 input data에 대한 <strong>위치 정보를 포함하고 있는 임베딩</strong>을 사용해야 한다.</li>
</ol>
<br>
<ol start="43">
<li>따라서 <code>위 그림</code>처럼 <strong>위치 정보를 가진 행렬을 만들고</strong> 그것을 <code>요소별 덧셈</code>을 하여 계산한다.</li>
</ol>
<hr>
<h2 id="why-self-attention">Why Self-Attention</h2>
<ol start="44">
<li>본 논문은 <code>Self-Attention</code>을 사용했을 때 가지는 세가지 장점에 대해서 설명하고 있다.</li>
</ol>
<br>
<ol start="45">
<li><code>첫 번째 장점</code>은 전체적인 <code>computational complexity</code>가 다른 순환 모델에 비해서 <strong>작다.</strong></li>
</ol>
<br>
<ol start="46">
<li><code>두 번째 장점</code>은 병렬적인 처리가 가능하다.</li>
</ol>
<br>
<ol start="47">
<li><code>세 번째 장점</code>은 <code>long range dependency</code>에 대해서도 처리가 가능하다.</li>
</ol>
<br>
<ol start="48">
<li>또한 <code>interpretable</code>한 모델을 만들 수 있다. 단순히 <code>self-attention</code>과 <code>softmax</code>를 거친 그 결과값을 보고 <strong>모델이 어느 곳을 더 중점을 두는지 확인할 수 있기 때문</strong>이다.
한다.</li>
</ol>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://wandukong.tistory.com/19" target="_blank" rel="noopener noreffer ">https://wandukong.tistory.com/19</a></li>
<li><a href="https://davidlds.tistory.com/5" target="_blank" rel="noopener noreffer ">https://davidlds.tistory.com/5</a></li>
<li><a href="https://www.youtube.com/watch?v=AA621UofTUA" target="_blank" rel="noopener noreffer ">https://www.youtube.com/watch?v=AA621UofTUA</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2024-07-24</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/paper/transformer/" data-title="[Paper Review]Attention Is All You Need" data-hashtags="Self-Attention,논문 리뷰,computer vision,Transformer"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/paper/transformer/" data-hashtag="Self-Attention"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/paper/transformer/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/paper/transformer/" data-title="[Paper Review]Attention Is All You Need"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/paper/transformer/" data-title="[Paper Review]Attention Is All You Need"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/paper/transformer/" data-title="[Paper Review]Attention Is All You Need"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/paper/transformer/" data-title="[Paper Review]Attention Is All You Need" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/self-attention/">Self-Attention</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/transformer/">Transformer</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/cs231n/lecture7/" class="prev" rel="prev" title="[CS231n] 07.Training Neural Networks II"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[CS231n] 07.Training Neural Networks II</a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
