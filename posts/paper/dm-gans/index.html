<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G) - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)" />
<meta property="og:description" content="개요 GAN 대비 Diffusion Model(DM)의 이미지 생성 성능을 비교하며, DM이 GAN을 능가하는 이유를 분석한 Diffusion Models Beat GANs 논문을 리뷰할 것이다. Introduction 요즘의 생성 모델은 많이 발전해오고 있다. 하지만 그 중에도 발전 가능성이 아직 많다. 생성 모델이 더욱 발전 한다면, 사용할 수 있는 분야가 셀 수 없이 많다. GAN은 여러 평가 지표 (FID, Inception Score, Precision, &hellip; 등)에 의하여 image generation에서 SOTA를 달성하고 있다. 하지만 이 평가 지표는 다양성을 완전히 포착하지 않고, likelihood-based model보다 더 다양성을 포착하지 않는다. 또한, GAN은 최적의 하이퍼 파라미터와 정규화를 하지 않으면 모델이 붕괴하기 때문에 훈련이 어렵다. 이러한 GAN의 단점 때문에 다른 domain에 적용하기에도 어렵고, 확장시키기도 어려워졌다. 그 결과 likelihood-based model이 GAN의 sampling image quality와 비슷하게 발전해왔다. likelihood-based model도 결국 단점이 있었는데, sampling 시 GAN보다 매우 느리고 sample quality 또한 기대에 미치진 못한다. Likelihood-based model의 한 종류인 Diffusion model이 등장하여 확장성도 있고, 높은 품질을 만들어내는 성능을 보였주었다. CIFAR-10에서 SOTA를 달성했지만 다른 어려운 dataset(LSUN, ImageNet)에 대해서는 GAN에 밀려있었다. 논문 저자들이 Improved Denoising Diffusion Probabilistic Models에서 diffusion model의 신뢰성을 증가시키는 연구를 했지만 FID가 GAN에 비하여 경쟁력 있지는 않았었다. 본 논문에선 GAN과 Diffusion model이 차이가 나는 두 가지 요인에서 비롯된다고 가정한다. 최근 GAN 연구에서 사용된 모델 아키텍처는 광범위하게 탐색되고 최적화되었다. GAN은 다양성(diversity)과 정확성(fidelity) 사이에서 trade-off의 균형을 조절한다. 따라서 GAN은 높은 품질의 샘플을 생성하는 대신 전체 데이터 분포를 완전히 포괄하지는 못한다. GAN은 Generator, Discriminator로 나뉘어져 있으므로 둘 간 균형을 조정한다. 본 논문에선 이 두 가지의 요인을 Diffusion model에도 적용하고자 한다. 먼저 모델의 아키텍쳐를 개선하고, 이후 다양성과 정확성 간의 trade-off의 균형을 조절 할 수 있는 기법을 개발한다. 이 결과 본 논문에서 제안한 diffusion model이 새로운 SOTA를 달성하며, GAN을 이기게 되었다. Background 이번 단락에서는 Diffusion model 중 DDPM에 대한 간단한 배경을 설명하고 있다. Diffusion model에 대한 자세한 설명은 해당 링크에서 보면 될 것 같다. DDPM에서의 목표는 조금 더 덜 노이즈가 포함된 $x_\text{t-1}$ 를 $x_t$에서 생성하는 과정을 학습하게 된다. DDPM에서 학습하기 위한 loss로 실제 변분 하한(Variational Lower Bound) $L_\text{vlb}$를 단순화한 $L_\text{simple}$이 성능이 좋음을 관찰하였다. 이런 훈련 절차와 샘플링 절차는 denoising score matching model과 동일하다고 한다. 다음으로, 조금 더 나은 diffusion model을 설명하게 되는데 기존 DDPM에서는 reverse process에서의 분산 $\Sigma_\theta(x_t, t)$ 을 고정된 값으로 설정하였는데, 이런 고정된 분산이 샘플링 단계 수가 적을 때 성능이 낮아질 수 있다. 따라서 $\Sigma_\theta(x_t, t)$ 를 파라미터화 하여 해결하려고 했고, 훈련 loss 또한 $L_\text{vlb}$ 과 $L_\text{simple}$ 를 함께 사용하는 hybrid objective로 해결한다. 본 논문에서도 해당 objecive와 parameterization을 사용한다. 또한 DDIM의 Non-Markovian 과정으로 인한 샘플링 스텝을 줄이는 방법 또한 본 논문에서 사용한다. 마지막으로 샘플 품질을 평가하는 metrics에 관한 설명으로 이어진다. Metrics 중 Inception Score(IS)는 ImageNet 클래스 분포를 얼마나 잘 학습했는지를 측정하는 메트릭이다. 개별 샘플이 특정 클래스의 예제를 얼마나 그럴듯하게 평가하면서도, 모델이 전체 dataset 클래스 분포를 잘 반영했는지 측정한다. 이런 IS도 한계점이 있는데, 아래는 IS의 한계점을 설명한 것이다. 모든 클래스에 대한 전체 분포를 얼마나 잘 커버하는지 평가하지 못한다. 데이터셋의 일부를 단순히 암기한 모델도 높은 IS 점수를 가질 수 있다. Fréchet Inception Distance (FID)는 IS보다 더 다양성을 잘 평가할 수 있는 방법이다. Inception-V3 모델의 latent space에서 두 이미지 분포 간 거리를 측정하여 두 이미지 분포 간의 symmetric measure of distance를 측정하게 된다. sFID라는 변형 버전은 기존 FID보다 공간적 특성을 고려하여 더 정교한 평가가 가능하다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/paper/dm-gans/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-02-28T16:42:40+09:00" />
<meta property="article:modified_time" content="2025-02-28T16:42:40+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)"/>
<meta name="twitter:description" content="개요 GAN 대비 Diffusion Model(DM)의 이미지 생성 성능을 비교하며, DM이 GAN을 능가하는 이유를 분석한 Diffusion Models Beat GANs 논문을 리뷰할 것이다. Introduction 요즘의 생성 모델은 많이 발전해오고 있다. 하지만 그 중에도 발전 가능성이 아직 많다. 생성 모델이 더욱 발전 한다면, 사용할 수 있는 분야가 셀 수 없이 많다. GAN은 여러 평가 지표 (FID, Inception Score, Precision, &hellip; 등)에 의하여 image generation에서 SOTA를 달성하고 있다. 하지만 이 평가 지표는 다양성을 완전히 포착하지 않고, likelihood-based model보다 더 다양성을 포착하지 않는다. 또한, GAN은 최적의 하이퍼 파라미터와 정규화를 하지 않으면 모델이 붕괴하기 때문에 훈련이 어렵다. 이러한 GAN의 단점 때문에 다른 domain에 적용하기에도 어렵고, 확장시키기도 어려워졌다. 그 결과 likelihood-based model이 GAN의 sampling image quality와 비슷하게 발전해왔다. likelihood-based model도 결국 단점이 있었는데, sampling 시 GAN보다 매우 느리고 sample quality 또한 기대에 미치진 못한다. Likelihood-based model의 한 종류인 Diffusion model이 등장하여 확장성도 있고, 높은 품질을 만들어내는 성능을 보였주었다. CIFAR-10에서 SOTA를 달성했지만 다른 어려운 dataset(LSUN, ImageNet)에 대해서는 GAN에 밀려있었다. 논문 저자들이 Improved Denoising Diffusion Probabilistic Models에서 diffusion model의 신뢰성을 증가시키는 연구를 했지만 FID가 GAN에 비하여 경쟁력 있지는 않았었다. 본 논문에선 GAN과 Diffusion model이 차이가 나는 두 가지 요인에서 비롯된다고 가정한다. 최근 GAN 연구에서 사용된 모델 아키텍처는 광범위하게 탐색되고 최적화되었다. GAN은 다양성(diversity)과 정확성(fidelity) 사이에서 trade-off의 균형을 조절한다. 따라서 GAN은 높은 품질의 샘플을 생성하는 대신 전체 데이터 분포를 완전히 포괄하지는 못한다. GAN은 Generator, Discriminator로 나뉘어져 있으므로 둘 간 균형을 조정한다. 본 논문에선 이 두 가지의 요인을 Diffusion model에도 적용하고자 한다. 먼저 모델의 아키텍쳐를 개선하고, 이후 다양성과 정확성 간의 trade-off의 균형을 조절 할 수 있는 기법을 개발한다. 이 결과 본 논문에서 제안한 diffusion model이 새로운 SOTA를 달성하며, GAN을 이기게 되었다. Background 이번 단락에서는 Diffusion model 중 DDPM에 대한 간단한 배경을 설명하고 있다. Diffusion model에 대한 자세한 설명은 해당 링크에서 보면 될 것 같다. DDPM에서의 목표는 조금 더 덜 노이즈가 포함된 $x_\text{t-1}$ 를 $x_t$에서 생성하는 과정을 학습하게 된다. DDPM에서 학습하기 위한 loss로 실제 변분 하한(Variational Lower Bound) $L_\text{vlb}$를 단순화한 $L_\text{simple}$이 성능이 좋음을 관찰하였다. 이런 훈련 절차와 샘플링 절차는 denoising score matching model과 동일하다고 한다. 다음으로, 조금 더 나은 diffusion model을 설명하게 되는데 기존 DDPM에서는 reverse process에서의 분산 $\Sigma_\theta(x_t, t)$ 을 고정된 값으로 설정하였는데, 이런 고정된 분산이 샘플링 단계 수가 적을 때 성능이 낮아질 수 있다. 따라서 $\Sigma_\theta(x_t, t)$ 를 파라미터화 하여 해결하려고 했고, 훈련 loss 또한 $L_\text{vlb}$ 과 $L_\text{simple}$ 를 함께 사용하는 hybrid objective로 해결한다. 본 논문에서도 해당 objecive와 parameterization을 사용한다. 또한 DDIM의 Non-Markovian 과정으로 인한 샘플링 스텝을 줄이는 방법 또한 본 논문에서 사용한다. 마지막으로 샘플 품질을 평가하는 metrics에 관한 설명으로 이어진다. Metrics 중 Inception Score(IS)는 ImageNet 클래스 분포를 얼마나 잘 학습했는지를 측정하는 메트릭이다. 개별 샘플이 특정 클래스의 예제를 얼마나 그럴듯하게 평가하면서도, 모델이 전체 dataset 클래스 분포를 잘 반영했는지 측정한다. 이런 IS도 한계점이 있는데, 아래는 IS의 한계점을 설명한 것이다. 모든 클래스에 대한 전체 분포를 얼마나 잘 커버하는지 평가하지 못한다. 데이터셋의 일부를 단순히 암기한 모델도 높은 IS 점수를 가질 수 있다. Fréchet Inception Distance (FID)는 IS보다 더 다양성을 잘 평가할 수 있는 방법이다. Inception-V3 모델의 latent space에서 두 이미지 분포 간 거리를 측정하여 두 이미지 분포 간의 symmetric measure of distance를 측정하게 된다. sFID라는 변형 버전은 기존 FID보다 공간적 특성을 고려하여 더 정교한 평가가 가능하다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/paper/dm-gans/" /><link rel="prev" href="https://goodyoung.github.io/posts/paper/ldm/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/paper\/dm-gans\/"
        },"genre": "posts","keywords": "논문 리뷰, computer vision, Diffusion, DM-GANs, Classifier Guidance","wordcount":  1790 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/paper\/dm-gans\/","datePublished": "2025-02-28T16:42:40+09:00","dateModified": "2025-02-28T16:42:40+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/diffusion/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Diffusion</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-02-28">2025-02-28</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;1790 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;9 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#background">Background</a></li>
    <li><a href="#architecture-improvements">Architecture Improvements</a>
      <ul>
        <li><a href="#adaptive-group-normalizationadagn">Adaptive Group Normalization(AdaGN)</a></li>
      </ul>
    </li>
    <li><a href="#classifier-guidance">Classifier Guidance</a>
      <ul>
        <li><a href="#conditional-reverse-noising-process">Conditional Reverse Noising Process</a></li>
        <li><a href="#conditional-sampling-for-ddim">Conditional Sampling for DDIM</a></li>
        <li><a href="#scaling-classifier-gradients">Scaling Classifier Gradients</a></li>
      </ul>
    </li>
    <li><a href="#results">Results</a></li>
    <li><a href="#limitations-and-future-work">Limitations and Future Work</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- image format
<div style="text-align:center;">
<img src="/images/cs231n/lecture4/back-1.png" height="100%" width="80%"> </div>
 -->
<h2 id="개요">개요</h2>
<ol start="0">
<li><code>GAN</code> 대비 <code>Diffusion Model(DM)</code>의 이미지 생성 성능을 비교하며, <code>DM</code>이 <code>GAN</code>을 능가하는 이유를 분석한 <code>Diffusion Models Beat GANs</code> 논문을 리뷰할 것이다.</li>
</ol>
<hr>
<h2 id="introduction">Introduction</h2>
<ol>
<li>요즘의 <code>생성 모델</code>은 많이 발전해오고 있다. 하지만 그 중에도 <strong>발전 가능성이 아직 많다.</strong></li>
</ol>
<br>
<ol start="2">
<li><code>생성 모델</code>이 더욱 발전 한다면, 사용할 수 있는 분야가 셀 수 없이 많다.</li>
</ol>
<br>
<ol start="3">
<li><code>GAN</code>은 여러 평가 지표 (FID, Inception Score, Precision, &hellip; 등)에 의하여 <code>image generation</code>에서 <code>SOTA</code>를 달성하고 있다.</li>
</ol>
<br>
<ol start="4">
<li>하지만 이 평가 지표는 다양성을 완전히 포착하지 않고, <code>likelihood-based model</code>보다 더 다양성을 포착하지 않는다.</li>
</ol>
<br>
<ol start="5">
<li>또한, <code>GAN</code>은 최적의 <code>하이퍼 파라미터</code>와 <code>정규화</code>를 하지 않으면 <strong>모델이 붕괴하기 때문에 훈련이 어렵다.</strong></li>
</ol>
<br>
<ol start="6">
<li>이러한 <code>GAN</code>의 단점 때문에 다른 domain에 적용하기에도 어렵고, 확장시키기도 어려워졌다. 그 결과 <code>likelihood-based model</code>이 <code>GAN</code>의 <code>sampling image quality</code>와 비슷하게 발전해왔다.</li>
</ol>
<br>
<ol start="7">
<li><code>likelihood-based model</code>도 결국 <strong>단점</strong>이 있었는데, sampling 시 <code>GAN</code>보다 매우 느리고 <code>sample quality</code> 또한 <strong>기대에 미치진 못한다.</strong></li>
</ol>
<br>
<ol start="8">
<li><code>Likelihood-based model</code>의 한 종류인 <code>Diffusion model</code>이 등장하여 확장성도 있고, 높은 품질을 만들어내는 성능을 보였주었다.</li>
</ol>
<br>
<ol start="9">
<li><code>CIFAR-10</code>에서 <code>SOTA</code>를 달성했지만 다른 어려운 dataset(<code>LSUN</code>, <code>ImageNet</code>)에 대해서는 <code>GAN</code>에 밀려있었다.</li>
</ol>
<ul>
<li>논문 저자들이 <code>Improved Denoising Diffusion Probabilistic Models</code>에서 <code>diffusion model</code>의 신뢰성을 증가시키는 연구를 했지만 <code>FID</code>가 <code>GAN</code>에 비하여 <strong>경쟁력 있지는 않았었다.</strong></li>
</ul>
<br>
<ol start="10">
<li><code>본 논문</code>에선 <code>GAN</code>과 <code>Diffusion model</code>이 차이가 나는 <code>두 가지 요인</code>에서 비롯된다고 가정한다.</li>
</ol>
<ul>
<li>최근 <code>GAN</code> 연구에서 사용된 모델 아키텍처는 <strong>광범위하게 탐색되고 최적화</strong>되었다.</li>
<li><code>GAN</code>은 <code>다양성(diversity)</code>과 <code>정확성(fidelity)</code> 사이에서 trade-off의 균형을 조절한다. 따라서 <code>GAN</code>은 높은 품질의 샘플을 생성하는 대신 전체 데이터 분포를 완전히 포괄하지는 못한다.
<ul>
<li><code>GAN</code>은 <code>Generator</code>, <code>Discriminator</code>로 나뉘어져 있으므로 둘 간 균형을 조정한다.</li>
</ul>
</li>
</ul>
<br>
<ol start="11">
<li><code>본 논문</code>에선 이 두 가지의 요인을 <code>Diffusion model</code>에도 적용하고자 한다.</li>
</ol>
<ul>
<li>먼저 <strong>모델의 아키텍쳐를 개선</strong>하고, 이후 <code>다양성</code>과 <code>정확성</code> 간의 trade-off의 균형을 조절 할 수 있는 기법을 개발한다.</li>
</ul>
<br>
<ol start="12">
<li>이 결과 본 논문에서 제안한 <code>diffusion model</code>이 새로운 <code>SOTA</code>를 달성하며, <code>GAN</code>을 이기게 되었다.</li>
</ol>
<hr>
<h2 id="background">Background</h2>
<ol start="13">
<li>이번 단락에서는 <code>Diffusion model</code> 중 <code>DDPM</code>에 대한 <strong>간단한 배경</strong>을 설명하고 있다. Diffusion model에 대한 자세한 설명은 <a href="https://goodyoung.github.io/posts/paper/ddpm/" target="_blank" rel="noopener noreffer ">해당 링크</a>에서 보면 될 것 같다.</li>
</ol>
<br>
<ol start="14">
<li><code>DDPM</code>에서의 목표는 조금 더 덜 노이즈가 포함된 $x_\text{t-1}$ 를 $x_t$에서 <strong>생성하는 과정을 학습하게 된다.</strong></li>
</ol>
<br>
<ol start="15">
<li><code>DDPM</code>에서 학습하기 위한 loss로 실제 변분 하한(Variational Lower Bound) $L_\text{vlb}$를 단순화한 $L_\text{simple}$이 성능이 좋음을 관찰하였다.</li>
</ol>
<br>
<ol start="16">
<li>이런 훈련 절차와 샘플링 절차는 denoising score matching model과 동일하다고 한다.</li>
</ol>
<br>
<ol start="17">
<li>다음으로, 조금 더 나은 <code>diffusion model</code>을 설명하게 되는데 기존 <code>DDPM</code>에서는 <code>reverse process</code>에서의 분산 $\Sigma_\theta(x_t, t)$ 을 <strong>고정된 값</strong>으로 설정하였는데, 이런 <code>고정된 분산</code>이 샘플링 단계 수가 적을 때 <strong>성능이 낮아질 수 있다.</strong></li>
</ol>
<br>
<ol start="18">
<li>따라서 $\Sigma_\theta(x_t, t)$ 를 파라미터화 하여 해결하려고 했고, 훈련 loss 또한 $L_\text{vlb}$ 과 $L_\text{simple}$ 를 함께 사용하는 <code>hybrid objective</code>로 해결한다.</li>
</ol>
<ul>
<li><code>본 논문</code>에서도 <code>해당 objecive</code>와 <code>parameterization</code>을 사용한다.</li>
</ul>
<br>
<ol start="19">
<li>또한 <code>DDIM</code>의 <code>Non-Markovian</code> 과정으로 인한 <code>샘플링 스텝</code>을 줄이는 방법 또한 <code>본 논문</code>에서 사용한다.</li>
</ol>
<br>
<ol start="20">
<li>마지막으로 샘플 품질을 평가하는 <code>metrics</code>에 관한 설명으로 이어진다. <code>Metrics</code> 중 <code>Inception Score(IS)</code>는 <code>ImageNet 클래스 분포</code>를 얼마나 잘 학습했는지를 측정하는 메트릭이다.</li>
</ol>
<br>
<ol start="21">
<li><code>개별 샘플</code>이 특정 클래스의 예제를 얼마나 그럴듯하게 평가하면서도, 모델이 <code>전체 dataset 클래스 분포</code>를 잘 반영했는지 측정한다. 이런 <code>IS</code>도 한계점이 있는데, 아래는 <code>IS</code>의 한계점을 설명한 것이다.</li>
</ol>
<ul>
<li>모든 클래스에 대한 <code>전체 분포</code>를 얼마나 잘 커버하는지 평가하지 못한다.</li>
<li>데이터셋의 일부를 단순히 <code>암기한 모델</code>도 <strong>높은 IS 점수</strong>를 가질 수 있다.</li>
</ul>
<br>
<ol start="22">
<li><code>Fréchet Inception Distance (FID)</code>는 <code>IS</code>보다 더 다양성을 잘 평가할 수 있는 방법이다. <code>Inception-V3 모델</code>의 <code>latent space</code>에서 <strong>두 이미지 분포 간 거리를 측정</strong>하여 두 이미지 분포 간의 <code>symmetric measure of distance</code>를 측정하게 된다.</li>
</ol>
<ul>
<li><code>sFID</code>라는 변형 버전은 기존 <code>FID</code>보다 <code>공간적 특성</code>을 고려하여 더 정교한 평가가 가능하다.</li>
</ul>
<br>
<ol start="23">
<li>마지막 평가 방법인 <code>개선된 precision &amp; recall 기반 metrics</code>가 있다.</li>
</ol>
<br>
<ol start="24">
<li><code>본 논문</code>은 <code>FID</code>를 기본 평가 지표로 사용하고 <em>일관된 평가</em>를 위해 <code>동일한 공개 샘플, 코드베이스</code>를 사용하여 평가를 진행한다.</li>
</ol>
<hr>
<h2 id="architecture-improvements">Architecture Improvements</h2>
<div style="text-align:center;">
<img src="/images/paper/adm-g/model-1.png" height="70%" width="70%"> </div>
<ol start="25">
<li><code>Introduction 11번 글</code>에서 설명했듯이 <code>모델의 아키텍쳐</code>를 <code>개선한 방법론</code>을 먼저 설명을 할 것이다. 그 후 <code>다양성</code>과 <code>정확성</code>을 조절한 <code>Classifier Guidance</code>를 설명할 것이다.</li>
</ol>
<br>
<ol start="26">
<li><code>모델의 아키텍쳐</code>를 개선하기 위하여 <code>모델 구조</code>에 대하여 <code>ablation study</code>를 진행하였다. <code>DDPM</code>에서는 <code>UNet</code>을 사용하였고 해당 구조는 다음과 같은 구성을 가진다. 모든 <code>실험</code>은 <code>동일한</code> 데이터 셋에서 진행했다.</li>
</ol>
<ul>
<li><code>Residual layers</code>와 <code>다운샘플링 &amp; 업샘플링 convolutions</code>을 거쳐 피처를 <strong>압축 및 복원</strong></li>
<li><code>Skip connections</code>을 사용하여 같은 공간 크기(spatial size)를 갖는 레이어 간 정보를 직접 전달</li>
<li><code>16×16</code>에서 <code>단일 헤드</code>를 사용하는 <code>글로벌 어텐션(Global Attention) 레이어</code> 추가</li>
<li><code>각 Residual block</code>에 <code>timestep embedding</code> 투영(projection)</li>
</ul>
<br>
<ol start="27">
<li><code>본 논문</code>에서는 다음의 구성을 추가하여 모델 구조를 변형하여 실험한다.</li>
</ol>
<ul>
<li><code>깊이 vs. 너비</code> 조정 → 모델 크기를 유지하면서 깊이를 늘릴지, 너비를 늘릴지 실험
<ul>
<li>깊이를 증가시키면 <strong>성능이 향상되지만</strong>, 학습 시간이 <strong>길어지는 단점이 발생</strong></li>
<li>이후 실험에서 깊이를 증가시키는 실험을 사용하지 않기로 결정함</li>
</ul>
</li>
<li><code>attention head 수 증가</code> → 더 많은 <code>attention head</code>가 성능 향상에 기여하는지 실험
<ul>
<li>
<p><code>고정된 attention head 수</code> vs <code>고정된 채널 수</code>를 비교 실험을 하였다.</p>
  <div style="text-align:center;">
  <img src="/images/paper/adm-g/improve-2.png" height="70%" width="70%"> </div>
<ul>
<li>attention를 <strong>늘리거나</strong> head당 채널 수를 <strong>줄이는 것</strong>이 <code>FID 향상</code></li>
</ul>
</li>
</ul>
</li>
<li><code>attention 범위 확장</code> → 기존 16×16에서만 적용하던 <code>attention</code>을 <code>32×32</code>, <code>16×16</code>, <code>8×8</code>에도 적용</li>
<li><code>BigGAN residual block 활용</code> → 업샘플링 및 다운샘플링 과정에서 <code>BigGAN</code>의 <code>residual block</code> 사용</li>
<li><code>Residual connection rescaling</code> → 안정성을 위해 residual connection을 $\frac{1}{\sqrt{2}}$ 로 rescale</li>
</ul>
<br>
<div style="text-align:center;">
<img src="/images/paper/adm-g/improve-2.png" height="70%" width="70%"> </div>
<ol start="28">
<li>결론적으로 <code>residual connection</code>을 <code>rescale</code> 하는 것을 <strong>제외하고</strong> 이러한 실험 사항들이 서로 결합될 때 <code>더 큰 성능 향상</code>을 보였다.</li>
</ol>
<h3 id="adaptive-group-normalizationadagn">Adaptive Group Normalization(AdaGN)</h3>
<ol start="29">
<li><code>AdaGN</code>은 <code>Group Normalization</code> 후에 <code>timestep</code> 및 <code>class embedding</code>을 <code>residual block</code>에 적용하는 방식이다.</li>
</ol>
<br>
<p>$$\text{AdaGN}(h,y) =y_s\text{GroupNorm}(h) + y_b$$</p>
<ul>
<li>$h$: 첫 번째 <code>convolution</code> 이후의 <code>residual block</code>의 <code>중간 활성값(intermediate activations)</code></li>
<li>$y = [y_s, y_b]$: <code>timestep 및 class embedding</code>을 <code>선형 변환</code>하여 생성된 벡터</li>
</ul>
<br>
<ol start="30">
<li>이로 인하여 <code>각 residual block</code>에 <code>timestep</code>과 <code>클래스 정보</code>를 반영할 수 있음</li>
</ol>
<br>
<ol start="31">
<li>초기에 <code>AdaGN</code>을 적용한 <code>diffusion model</code>에서 성능 향상을 하여, 기본 값으로 사용을 한다.</li>
</ol>
<br>
<ol start="32">
<li>그래서 <code>모든 비교 실험</code>을 거쳐 <code>최종 모델 아키텍쳐</code>는 다음과 같다.</li>
</ol>
<ul>
<li>각 해상도당 <code>residual block</code> 2개</li>
<li>64채널 per head의 <code>다중 어텐션 구조</code></li>
<li>해상도 <code>32×32</code>, <code>16×16</code>, <code>8×8</code>에서 어텐션 적용</li>
<li><code>BigGAN residual blocks</code>을 업/다운샘플링에 사용</li>
<li><code>AdaGN</code> 적용</li>
</ul>
<hr>
<h2 id="classifier-guidance">Classifier Guidance</h2>
<ol start="33">
<li><code>conditional gan</code>은 잘 설계된 아키텍쳐와 함께 <code>class labels</code>을 활용한다. 이처럼 <code>Class 정보</code>가 생성 모델의 <strong>중요한 역할을 한다.</strong></li>
</ol>
<ul>
<li><code>GAN</code>에서는 <code>discriminator</code>를 통하여 <code>class 정보</code>를 반영한다.</li>
</ul>
<br>
<ol start="34">
<li>따라서 <code>conditional diffusion</code>도 연구해볼만한 가치가 있다.</li>
</ol>
<br>
<ol start="35">
<li>이미 <code>AdaGN</code>에서 <code>class 정보</code>를 <strong>포함하는 방법</strong>을 사용했는데, 이 방법 말고 <code>본 논문</code>은 <code>classifier</code> ($p(y|x)$)를 활용하여 <code>diffusion</code>의 <strong>성능을 향상</strong>시키는 것을 보여준다.</li>
</ol>
<br>
<ol start="36">
<li><code>이전 연구</code>에서는 <code>pretrained diffusion model</code>이 <code>classifier</code>의 <strong>gradient를 활용</strong>하여 조건부 생성이 가능함을 보였다.</li>
</ol>
<br>
<ol start="37">
<li>따라서 <code>본 논문</code>은 <code>noisy</code>한 $x_t$를 기반으로 분류기 $p_\phi(y|x_t,t)$를 학습시키고, 그 <strong>gradient를 활용</strong>   ($\nabla_{x_t}logp_\phi(y|x_t,t)$)하여 <code>diffusion sampling process</code>를 <code>특정 클래스</code> $y$로 유도되도록 할 것이다.</li>
</ol>
<br>
<ol start="38">
<li>앞으로 <code>분류기 조건부 샘플링 방법</code>에 대해 <code>두 가지</code>를 검토 한 후 샘플 품질 개선에 대해서 사용하는 방법을 설명 할 것이다.</li>
</ol>
<hr>
<h3 id="conditional-reverse-noising-process">Conditional Reverse Noising Process</h3>
<p>$$p_{\theta, \phi}(x_t \mid x_{t+1}, y) = Z p_{\theta}(x_t \mid x_{t+1}) p_{\phi}(y \mid x_t)$$</p>
<ul>
<li>$Z$는 정규화 상수</li>
</ul>
<details>
  <summary> 위 식 유도 </summary>
<div style="text-align:center;">
<img src="/images/paper/adm-g/proof-1.png" height="70%" width="70%"> </div>
<div style="text-align:center;">
<img src="/images/paper/adm-g/proof-2.png" height="70%" width="70%"> </div>
<ul>
<li><code>(59)</code>에서 $\hat{q}$가 $q$가 된 이유는 <code>(53)</code>때문이다.</li>
<li><code>(60)</code>에서 $\hat{q}(y \mid x_{t+1})$가 $x_t$에 영향을 받지 않으므로, 상수 ($Z$)취급이 가능하다.</li>
<li>해당 전개는 <code>본 논문 Appendix H</code>에 있는 내용을 가져온 것이다.</li>
</ul>
</details>
<br>
<ol start="39">
<li>기존 <code>Unconditional ddpm</code>의 <code>reverse process</code>의 식은 $p_\theta(x_t|x_\text{t+1})$을 사용한다. 이를 <code>class label</code> $y$에 대해 <code>condition</code> 하게 변경한다면 <code>위 식</code>으로 변경이 가능하다.</li>
</ol>
<br>
<p>$$
\begin{aligned}
\log p_{\phi}(y \mid x_t) &amp;\approx \log p_{\phi}(y \mid x_t) \Big|_ {x_t = \mu} + (x_t - \mu) \nabla_{x_t} \log p_{\phi}(y \mid x_t) \Big|_{x_t = \mu} \\
&amp;= (x_t - \mu) g + C_1 \\
g &amp;= \nabla _ {x_t} \log p _ {\phi}(y \mid x_t)
\end{aligned}
$$</p>
<br>
<ol start="40">
<li>따라서 $p_{\phi}$를 계산하면 <code>sampling 식</code>을 <strong>다시 유도</strong> 할 수 있을 것이다. 위는 $p_{\phi}$을 <code>테일러 정리</code>를 활용하여 전개한 것이다.</li>
</ol>
<br>
<p>$$
\begin{aligned}
\log \big( p_{\theta}(x_t \mid x_{t+1}) p_{\phi}(y \mid x_t) \big) &amp;\approx -\frac{1}{2} (x_t - \mu)^T \Sigma^{-1} (x_t - \mu) + (x_t - \mu) g + C_2\\
&amp;= -\frac{1}{2} (x_t - \mu - \Sigma g)^T \Sigma^{-1} (x_t - \mu - \Sigma g) + \frac{1}{2} g^T \Sigma g + C_2 \\
&amp;= -\frac{1}{2} (x_t - \mu - \Sigma g)^T \Sigma^{-1} (x_t - \mu - \Sigma g) + C_3 \\
&amp;= \log p(z) + C_4, \quad z \sim \mathcal{N}(\mu + \Sigma g, \Sigma)
\end{aligned}
$$</p>
<br>
<ol start="41">
<li>$p_{\phi}$과 기존 <code>ddpm</code>에서 나온 $p_{\theta}(x_t \mid x_{t+1})$의 식을 활용하여 <code>위 식</code>을 전개할 수 있다.</li>
</ol>
<br>
<ol start="42">
<li><code>맨 마지막 식</code>을 확인을 하면 $log p(z)$의 $z$는 $\mathcal{N}(\mu + \Sigma g, \Sigma)$에서 <code>sampling</code>이 된 것을 확인 할 수 있다.</li>
</ol>
<br>
<ol start="43">
<li>이는 <code>unconditional</code> 에서 평균($\mu$)을 $\Sigma g$만큼 <strong>shift한 것이다.</strong> 따라서 $g$는 classifier의 gradient의 값이므로 이 gradient를 활용하여 conditional을 만족하게 된다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/paper/adm-g/algo-1.png" height="70%" width="70%"> </div>
<ul>
<li>정규분포에서 $x_t$추출</li>
<li>T부터 1까지 <code>sampling</code> 시작</li>
<li>t시점에서 $x_t$의 평균과 분산 계산</li>
<li>기존 <code>ddpm gaussian distribution의 평균</code>에 <code>scaling factor * classifier gradient</code>의 값을 <strong>shift</strong>한 분포에서 sampling</li>
<li>t가 1이 될 때까지 <strong>반복</strong></li>
</ul>
<br>
<ol start="44">
<li>위 알고리즘은 <code>Classifier guided</code>를 <code>stochastic diffusion</code>에 <code>sampling</code>을 과정이다. 여기의 <code>scaling factor</code>($s$)는 <code>gradient scale</code>이라는 <strong>하이퍼 파라미터</strong>이다.</li>
</ol>
<hr>
<h3 id="conditional-sampling-for-ddim">Conditional Sampling for DDIM</h3>
<ol start="45">
<li>위의 <code>conditional sampling</code>은 stochastic한 상황만 가능하다. 따라서 <code>DDIM</code>을 이용한 <code>deterministic sampling method</code>는 적용될 수 없게 된다.</li>
</ol>
<br>
<ol start="46">
<li>따라서 <code>score-based conditioning trick</code>에 의하여 식 전개를 하게 된다. 따라서 <code>score function</code>이 아래가 된다.</li>
</ol>
<p>$$\nabla_{x_t} \log q(x_t) = - \frac{\epsilon_{\theta}(x_t)}{\sqrt{1 - \bar{\alpha}_t}}$$</p>
<br>
<ol start="47">
<li>이 식을 활용하여 $log q(x_t,y)$를 정의할 수 있게 된다.</li>
</ol>
<br>
<p>$$
\begin{aligned}
\nabla_{x_t} \log q(x_t, y) &amp;= \nabla_{x_t} \log q(x_t) + \nabla_{x_t} \log q(y \mid x_t) \\
&amp;\approx -\frac{1}{\sqrt{1 - \bar{\alpha_t}}} \epsilon_{\theta}(x_t, t) + \nabla_{x_t} \log p_{\phi}(y \mid x_t) \\
-\frac{1}{\sqrt{1 - \bar{\alpha_t}}} \hat{\epsilon}(x_t) &amp;= -\frac{1}{\sqrt{1 - \bar{\alpha_t}}} \left( \epsilon_{\theta}(x_t, t) - \sqrt{1 - \bar{\alpha_t}} \nabla_{x_t} \log p_{\phi}(y \mid x_t) \right) \\ \\
\hat{\epsilon} _ {\theta}(x_t, t) &amp;= \epsilon_{\theta}(x_t, t) - \sqrt{1 - \bar{\alpha_t}} \nabla_{x_t} \log p_{\phi}(y \mid x_t)
\end{aligned}
$$</p>
<br>
<ol start="48">
<li>따라서 새로운 <code>classifier-guided predictor</code> $\hat{\epsilon_\theta}$가 도출되게 된다.</li>
</ol>
<div style="text-align:center;">
<img src="/images/paper/adm-g/algo-2.png" height="70%" width="70%"> </div>
<ul>
<li><code>정규분포</code>에서 $x_t$ 추출</li>
<li>T부터 1까지 <code>sampling</code> 시작</li>
<li>t시점에서 $\epsilon_\theta$에 <code>classifier gradient</code>를 반영하여 $\hat{\epsilon}$을 구함</li>
<li><code>그 값</code>을 활용하여 기존 DDIM에서 $x_\text{t-1}$을 구하는 식으로 계산</li>
<li>t가 1이 될 때까지 <strong>반복</strong></li>
</ul>
<br>
<ol start="49">
<li>위 그림은 <code>ddim 샘플링</code>일 때의 샘플링 알고리즘을 나타낸다. <code>Conditional Reverse Noising Process</code>와 다른점은 <code>classifier gradient</code>를 통해  $\epsilon$ <strong>모델 자체를 업데이트에 활용</strong> 한다는 점이다.</li>
</ol>
<hr>
<h3 id="scaling-classifier-gradients">Scaling Classifier Gradients</h3>
<ol start="50">
<li><code>Large scale generative task</code>에 <code>classifier</code>를 적용시키기 위해 ImageNet을 가지고 모델 구조를 <code>UNet</code>의 <code>downsampling</code> 부분을 통해 학습 시켰다.</li>
</ol>
<ul>
<li><code>8*8 layer</code>에 <code>attention pool</code>을 적용시켜 <strong>최종 결과를 나오게 했다.</strong></li>
<li><code>Diffusion model</code>에서 사용한 noise와 <strong>동일한 분포로 학습시켰다.</strong></li>
</ul>
<br>
<ol start="51">
<li><code>44번 줄</code>의 <code>샘플링 알고리즘</code>을 보면 <code>gradient scale</code>의 하이퍼 파라미터가 나온다. 이것이 있는 이유는 <code>scale = 1</code>로 설정을 한 경우 샘플링의 결과가 <strong>class에 할당된 확률이 50% 근처</strong>이고, <code>1보다 큰 scale factor</code>를 사용한 경우엔 특정 <strong>class에 속할 확률이 100% 부근의 값</strong>으로 수렴하게 된다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/paper/adm-g/classifier.png" height="70%" width="70%"> </div>
<br>
<ol start="52">
<li><code>위 사진</code>을 보면 왼쪽의 <code>scale factor = 1</code>이고 오른쪽은 <code>10</code>을 사용하고, <code>&quot;Pembroke Welsh corgi&quot;</code>라는 class를 부여했을 때의 샘플링 결과이다. 결과를 보면 확실히 <strong>오른쪽의 샘플링 결과가 더욱 좋은 것을 확인할 수 있다.</strong></li>
</ol>
<ul>
<li>수치적으로도 <code>FID</code>가 <strong>많이 개선이 되었다.</strong></li>
</ul>
<details>
  <summary> scale factor가 1보다 커지면 좋은 수식적인 이유 </summary>
<p>$$s \cdot \nabla_x \log p(y \mid x) = \nabla_x \log \left( \frac{1}{C} p(y \mid x)^s \right)$$</p>
<ul>
<li>임의의 상수 $C$에 의하여 <strong>위 수식이 성립하게 된다.</strong></li>
<li>따라서 <code>s&gt;1</code>의 경우에 분포 $p(y|x)$는 더 sharp해지고, 이에 대한 <code>class gradient scale</code>을 증가한다.</li>
<li>s가 커질수록 <code>특정 class</code>에 대해 높아지고, <code>나머지 class</code>에 대해서는 낮아진다.
<ul>
<li>이는 곧, sample의 <code>diversity</code>는 낮아지고, <code>fidelity(품질)</code>이 높아지게 된다.</li>
</ul>
</li>
</ul>
</details>
<br>
<div style="text-align:center;">
<img src="/images/paper/adm-g/result-1.png" height="70%" width="70%"> </div>
<br>
<ol start="53">
<li>지금까지 <code>diffusion model</code>이 <code>unconditional</code>이라는 가정을 가지고 왔다. 본 논문에선 <code>conditional diffusion model</code>과 <code>unconditional + classifier guidance</code>의 <code>FID</code> 차이가 많이 나지 않다고 한다.</li>
</ol>
<ul>
<li><code>conditional model</code>은 직접 이미지에 대한 <code>class</code>를 <strong>넣어주는 방식을 의미한다.</strong></li>
</ul>
<br>
<ol start="54">
<li>위 사진을 보면 물론, <code>conditional model</code>을 <code>guided</code>하면 <code>FID</code>가 더욱 향상이 되는 것을 확인 할 수 있다.</li>
</ol>
<div style="text-align:center;">
<img src="/images/paper/adm-g/result-2.png" height="70%" width="70%"> </div>
<ol start="55">
<li>또한 <code>위 그래프</code>는 <code>scale factor</code>에 따른 <code>평가 지표</code>들을 비교한 그래프이다.</li>
</ol>
<br>
<ol start="56">
<li>scale 값이 커질수록 <code>quality (IS,precision)</code>은 증가하지만, <code>diversity(recall)</code>은 감소하는 것을 확인할 수 있다.</li>
</ol>
<br>
<ol start="57">
<li>그 아래 그래프는 <code>classifier guidance</code>로 인해 <code>BigGAN-deep</code>의 성능을 뛰어 넘은 것을 확인 할 수 있다.</li>
</ol>
<hr>
<h2 id="results">Results</h2>
<ol start="58">
<li>본 논문이 제안하는 모델은 <code>Ablated Diffusion Model(ADM)</code>이고 <code>classifier guidance</code>를 사용하면 <code>(ADM-G)</code>이다.</li>
</ol>
<br>
<ol start="59">
<li>다양한 데이터셋 및 해상도에서 <code>GAN</code>을 뛰어넘는 <code>우수한 성능</code>을 보여준다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/paper/adm-g/result-3.png" height="70%" width="70%"> </div>
<ol start="60">
<li>위 결과는 <code>BigGAN-deep 모델</code>과 <code>제일 좋은 성능의 ADM</code>의 <code>sampling 이미지</code>들이다. 대게 이미지의 <code>quality</code>는 비슷하지만 <code>ADM</code>이 <strong>더 다양한 이미지</strong>를 만들어내는 것을 확인할 수 있다.</li>
</ol>
<hr>
<h2 id="limitations-and-future-work">Limitations and Future Work</h2>
<ol start="61">
<li>여전히 <code>denoising step</code>을 사용하기 때문에 샘플링에서 <code>GAN</code>보다 느리다.</li>
</ol>
<br>
<ol start="62">
<li><code>Single step model</code>의 샘플은 이전 <code>likelihood-based model</code>보다는 좋고, <code>GAN</code>과 <strong>경쟁력은 아직 없다.</strong></li>
</ol>
<br>
<ol start="63">
<li>또한 이때는 <code>classifier guidance</code>는 <strong>레이블이 있어야 사용이 가능</strong>한 한계를 가지고 있다.</li>
</ol>
<ul>
<li><code>Unlabeled data</code>로 확장이 가능하다.</li>
</ul>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=Y3kmXf9aT78" target="_blank" rel="noopener noreffer ">https://www.youtube.com/watch?v=Y3kmXf9aT78</a></li>
<li><a href="https://www.youtube.com/watch?v=gN1FQhQsUTE" target="_blank" rel="noopener noreffer ">https://www.youtube.com/watch?v=gN1FQhQsUTE</a></li>
<li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#score" target="_blank" rel="noopener noreffer ">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#score</a></li>
<li><a href="https://velog.io/@philiplee_235/Cold-Diffusion-Inverting-Arbitrary-Image-Transforms-Without-Noise" target="_blank" rel="noopener noreffer ">https://velog.io/@philiplee_235/Cold-Diffusion-Inverting-Arbitrary-Image-Transforms-Without-Noise</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-02-28</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/paper/dm-gans/" data-title="[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)" data-hashtags="논문 리뷰,computer vision,Diffusion,DM-GANs,Classifier Guidance"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/paper/dm-gans/" data-hashtag="논문 리뷰"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/paper/dm-gans/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/paper/dm-gans/" data-title="[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/paper/dm-gans/" data-title="[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/paper/dm-gans/" data-title="[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/paper/dm-gans/" data-title="[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/diffusion/">Diffusion</a>,&nbsp;<a href="/tags/dm-gans/">DM-GANs</a>,&nbsp;<a href="/tags/classifier-guidance/">Classifier Guidance</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/paper/ldm/" class="prev" rel="prev" title="[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)</a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
