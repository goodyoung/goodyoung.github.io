<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT) - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)" />
<meta property="og:description" content="개요 Vision분야에서 Transformer를 사용한 ViT에 관한 논문에 대한 리뷰를 할 것이다. Introduction Self-Attention기반 archtecture인 Transformer는 NLP에서 중요한 역할을 하고 있다. 이때의 주요 접근 방법은 아주 큰 text data로 학습 시킨 사전 훈련 모델을 fine tuning을 하는 것이다. Transformer의 계산 효율성과 확장성으로 인해 전레없는 크기의 모델을 훈련하는 것이 가능해졌다. Model과 dataset이 커져도 성능의 포화는 보이지 않는다. 하지만 Vision에서는 CNN 구조가 지배적이었다. NLP의 성공 후, attention을 사용하려는 여러 시도들이 있었지만 특수한 attention pattern의 사용 때문에 효과적으로 사용되진 않았다. CNN &#43; self-attention의 구조 CNN을 완전히 대체하는 구조 따라서 이런 상황속에서 최소한의 수정으로 image를 직접적으로 Transformer에 넣는 실험을 했다. Image를 패치로 나누고 이 패치를 linear 한 embedding의 연속으로 나눈 후 이것을 Transformer에 바로 넣는 방식이다. 그렇게 되면 Image patches는 NLP의 관점으론 token(word)가 되는 것이다. 이 ViT를 ImageNet과 같은 mid-size dataset으로 학습을 했다. 이땐 ResNet보다 몇 퍼센트 낮은 정확도를 보였다. 이는 예상된 결과이다. 왜냐하면 기존 CNN은 inductive biases를 가지고 있어 적은 양의 데이터로도 translation equivariance, locality를 잘하여 일반화 성능이 좋다.
inductive biases: 모델이 학습된 데이터 외의 데이터에 대해 얼마나 잘 일반화할 수 있는지에 사용하는 가정
머신러닝의 최종 목표는 generalization, 즉 학습 데이터로 학습시킨 모델이 본 적 없는 데이터에 대해서도 예측(prediction, approximation)을 잘 해내는 것이다. 본 적 없는 상황을 예측하기 위해서는 학습된 가정 이외에 추가적인 가정이 필요한데, 이것이 바로 inductive bias이다. translation equivariance: 입력 데이터가 일정한 변환을 받을 때, 그 변환이 모델의 출력에도 동일하게 반영되는 성질
CNN의 합성곱 층에서는 이미지의 특정 패턴이 위치를 옮겨도 그 패턴을 감지하는 필터의 반응이 동일하게 이동한다. 즉, 입력 이미지의 패턴이 이동하면, 해당 패턴을 인식하는 뉴런의 활성화 위치도 동일하게 이동한다. 어떠한 사물이 들어 있는 이미지를 제공해줄 때 사물의 위치가 바뀌면 CNN과 같은 연산의 activation 위치 또한 바뀌게 된다. Locality: 이미지 내의 픽셀들이 인접한 다른 픽셀들과 더 밀접한 관계를 가지는 특성
그에 반해, Transformer는 inductive biases가 부족하여 데이터가 충분하지 않을 때 일반화가 되지 않는다. 그러나 더 큰 데이터 셋으로 훈련을 했을 경우 모델의 성능이 크게 향상이 된다. 따라서 대규모 훈련이 inductive bias를 능가하는 것을 확인을 하였다. Method 위 사진은 ViT의 전체적인 구성에 대한 그림이다. 본 논문에서는 최대한 가능한 원래의 Transformer구조를 따를려고 했다. 그렇게 되면 확장 가능한 NLP Transformer 아키텍처와 그 효율적인 구현을 거의 그대로 사용할 수 있다는 장점이 있기 때문이다. Vision Transformer (ViT) ViT의 전체적인 구조를 설명하고 있다. 크게 이미지 입력 처리, 패치 임베딩, [class] 토큰 추가, position 임베딩, Transformer Encoder 구조 순으로 설명할 것이다. 먼저 이미지 입력 처리에서 입력 이미지는 $ H * W * C $ (H,W 이미지 높이와 너비, C: 채널 수) 크기를 가지지만 1D 시퀀스의 입력을 받는 Transformer의 구조에 맞게 변형이 필요하다. 따라서 $ H * W * C $ 에서 $ N * (P^2 * C) $ ($N = \frac{H*W}{P^2}$, $P^2$은 각 패치의 크기, C: 채널 수)로 변환을 해야한다. 또한 이 각 패치는 입력에 맞게 linear projection되어 고정된 크기 $D$의 벡터로 매핑이 되어 이 과정을 통해 입력 데이터를 학습 가능하게 변환한다. Linear projcetion: 입력 벡터에 선형 변환을 적용하여 다른 차원의 벡터로 변환하는 과정. $y = W * x &#43; b$ 일 때 $y$는 출력 백터로 고정된 크기의 $D$차원의 벡터이고, W는 가중치 행렬로 학습 가능한 행렬이다. 이미지 패치가 $16 * 16 * 3$일 때 이 패치를 펼쳐서 $16 * 16 * 3 = 768$크기의 벡터로 나타낼 수 있다 이때 Linear projection을 통해 $D = 512$ 크기의 벡터로 변환하려면, $512 * 768$크기의 학습 가능한 가중치 행렬 $W$를 사용해 Linear projection을 사용하여 다른 차원의 벡터로 변환하는 것이다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/paper/vit/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-31T19:28:04+09:00" />
<meta property="article:modified_time" content="2024-07-31T19:28:04+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)"/>
<meta name="twitter:description" content="개요 Vision분야에서 Transformer를 사용한 ViT에 관한 논문에 대한 리뷰를 할 것이다. Introduction Self-Attention기반 archtecture인 Transformer는 NLP에서 중요한 역할을 하고 있다. 이때의 주요 접근 방법은 아주 큰 text data로 학습 시킨 사전 훈련 모델을 fine tuning을 하는 것이다. Transformer의 계산 효율성과 확장성으로 인해 전레없는 크기의 모델을 훈련하는 것이 가능해졌다. Model과 dataset이 커져도 성능의 포화는 보이지 않는다. 하지만 Vision에서는 CNN 구조가 지배적이었다. NLP의 성공 후, attention을 사용하려는 여러 시도들이 있었지만 특수한 attention pattern의 사용 때문에 효과적으로 사용되진 않았다. CNN &#43; self-attention의 구조 CNN을 완전히 대체하는 구조 따라서 이런 상황속에서 최소한의 수정으로 image를 직접적으로 Transformer에 넣는 실험을 했다. Image를 패치로 나누고 이 패치를 linear 한 embedding의 연속으로 나눈 후 이것을 Transformer에 바로 넣는 방식이다. 그렇게 되면 Image patches는 NLP의 관점으론 token(word)가 되는 것이다. 이 ViT를 ImageNet과 같은 mid-size dataset으로 학습을 했다. 이땐 ResNet보다 몇 퍼센트 낮은 정확도를 보였다. 이는 예상된 결과이다. 왜냐하면 기존 CNN은 inductive biases를 가지고 있어 적은 양의 데이터로도 translation equivariance, locality를 잘하여 일반화 성능이 좋다.
inductive biases: 모델이 학습된 데이터 외의 데이터에 대해 얼마나 잘 일반화할 수 있는지에 사용하는 가정
머신러닝의 최종 목표는 generalization, 즉 학습 데이터로 학습시킨 모델이 본 적 없는 데이터에 대해서도 예측(prediction, approximation)을 잘 해내는 것이다. 본 적 없는 상황을 예측하기 위해서는 학습된 가정 이외에 추가적인 가정이 필요한데, 이것이 바로 inductive bias이다. translation equivariance: 입력 데이터가 일정한 변환을 받을 때, 그 변환이 모델의 출력에도 동일하게 반영되는 성질
CNN의 합성곱 층에서는 이미지의 특정 패턴이 위치를 옮겨도 그 패턴을 감지하는 필터의 반응이 동일하게 이동한다. 즉, 입력 이미지의 패턴이 이동하면, 해당 패턴을 인식하는 뉴런의 활성화 위치도 동일하게 이동한다. 어떠한 사물이 들어 있는 이미지를 제공해줄 때 사물의 위치가 바뀌면 CNN과 같은 연산의 activation 위치 또한 바뀌게 된다. Locality: 이미지 내의 픽셀들이 인접한 다른 픽셀들과 더 밀접한 관계를 가지는 특성
그에 반해, Transformer는 inductive biases가 부족하여 데이터가 충분하지 않을 때 일반화가 되지 않는다. 그러나 더 큰 데이터 셋으로 훈련을 했을 경우 모델의 성능이 크게 향상이 된다. 따라서 대규모 훈련이 inductive bias를 능가하는 것을 확인을 하였다. Method 위 사진은 ViT의 전체적인 구성에 대한 그림이다. 본 논문에서는 최대한 가능한 원래의 Transformer구조를 따를려고 했다. 그렇게 되면 확장 가능한 NLP Transformer 아키텍처와 그 효율적인 구현을 거의 그대로 사용할 수 있다는 장점이 있기 때문이다. Vision Transformer (ViT) ViT의 전체적인 구조를 설명하고 있다. 크게 이미지 입력 처리, 패치 임베딩, [class] 토큰 추가, position 임베딩, Transformer Encoder 구조 순으로 설명할 것이다. 먼저 이미지 입력 처리에서 입력 이미지는 $ H * W * C $ (H,W 이미지 높이와 너비, C: 채널 수) 크기를 가지지만 1D 시퀀스의 입력을 받는 Transformer의 구조에 맞게 변형이 필요하다. 따라서 $ H * W * C $ 에서 $ N * (P^2 * C) $ ($N = \frac{H*W}{P^2}$, $P^2$은 각 패치의 크기, C: 채널 수)로 변환을 해야한다. 또한 이 각 패치는 입력에 맞게 linear projection되어 고정된 크기 $D$의 벡터로 매핑이 되어 이 과정을 통해 입력 데이터를 학습 가능하게 변환한다. Linear projcetion: 입력 벡터에 선형 변환을 적용하여 다른 차원의 벡터로 변환하는 과정. $y = W * x &#43; b$ 일 때 $y$는 출력 백터로 고정된 크기의 $D$차원의 벡터이고, W는 가중치 행렬로 학습 가능한 행렬이다. 이미지 패치가 $16 * 16 * 3$일 때 이 패치를 펼쳐서 $16 * 16 * 3 = 768$크기의 벡터로 나타낼 수 있다 이때 Linear projection을 통해 $D = 512$ 크기의 벡터로 변환하려면, $512 * 768$크기의 학습 가능한 가중치 행렬 $W$를 사용해 Linear projection을 사용하여 다른 차원의 벡터로 변환하는 것이다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/paper/vit/" /><link rel="prev" href="https://goodyoung.github.io/posts/cs231n/lecture9/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/paper\/vit\/"
        },"genre": "posts","keywords": "Self-Attention, 논문 리뷰, computer vision, Transformer, ViT","wordcount":  996 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/paper\/vit\/","datePublished": "2024-07-31T19:28:04+09:00","dateModified": "2024-07-31T19:28:04+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-07-31">2024-07-31</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;996 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;5 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#method">Method</a>
      <ul>
        <li><a href="#vision-transformer-vit">Vision Transformer (ViT)</a></li>
        <li><a href="#inductive-bias">Inductive bias</a></li>
        <li><a href="#hybrid-architecture">Hybrid Architecture</a></li>
      </ul>
    </li>
    <li><a href="#experiments">Experiments</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- image format
<div style="text-align:center;">
<img src="/images/cs231n/lecture4/back-1.png" height="100%" width="80%"> </div>
 -->
<h2 id="개요">개요</h2>
<ol start="0">
<li><code>Vision</code>분야에서 <code>Transformer</code>를 사용한 <code>ViT</code>에 관한 논문에 대한 리뷰를 할 것이다.</li>
</ol>
<hr>
<h2 id="introduction">Introduction</h2>
<ol>
<li><code>Self-Attention</code>기반 archtecture인 <code>Transformer</code>는 <code>NLP</code>에서 중요한 역할을 하고 있다.</li>
</ol>
<br>
<ol start="2">
<li>이때의 <code>주요 접근 방법</code>은 <code>아주 큰 text data로 학습</code> 시킨 사전 훈련 모델을 <code>fine tuning</code>을 하는 것이다.</li>
</ol>
<br>
<ol start="3">
<li><code>Transformer</code>의 계산 효율성과 확장성으로 인해 <strong>전레없는 크기의 모델을 훈련하는 것이 가능해졌다.</strong></li>
</ol>
<br>
<ol start="4">
<li><strong>Model과 dataset이 커져도 성능의 포화는 보이지 않는다.</strong></li>
</ol>
<br>
<ol start="5">
<li>하지만 <code>Vision</code>에서는 CNN 구조가 <strong>지배적이었다</strong>. NLP의 성공 후, attention을 사용하려는 여러 시도들이 있었지만 <strong>특수한 attention pattern의 사용 때문에</strong> 효과적으로 사용되진 않았다.
<ul>
<li>CNN + self-attention의 구조</li>
<li>CNN을 완전히 대체하는 구조</li>
</ul>
</li>
</ol>
<br>
<ol start="6">
<li>따라서 이런 상황속에서 최소한의 수정으로 <strong>image를 직접적으로</strong> Transformer에 넣는 실험을 했다.</li>
</ol>
<br>
<ol start="7">
<li><code>Image</code>를 패치로 나누고 이 <strong>패치를 linear 한 embedding의 연속으로 나눈 후</strong> 이것을 Transformer에 <strong>바로 넣는 방식</strong>이다.
<ul>
<li>그렇게 되면 <strong>Image patches는 NLP의 관점으론 token(word)가 되는 것이다.</strong></li>
</ul>
</li>
</ol>
<br>
<ol start="8">
<li>이 <code>ViT</code>를 <code>ImageNet</code>과 같은 <code>mid-size dataset</code>으로 학습을 했다. 이땐 ResNet보다 <strong>몇 퍼센트 낮은 정확도를 보였다.</strong></li>
</ol>
<br>
<ol start="9">
<li>
<p>이는 예상된 결과이다. 왜냐하면 기존 CNN은 <code>inductive biases</code>를 가지고 있어 <code>적은 양의 데이터</code>로도 <code>translation equivariance</code>, <code>locality</code>를 잘하여 <strong>일반화 성능이 좋다.</strong></p>
<ul>
<li>
<p><code>inductive biases</code>: 모델이 학습된 <strong>데이터 외의 데이터</strong>에 대해 얼마나 잘 일반화할 수 있는지에 사용하는 가정</p>
<ul>
<li>머신러닝의 최종 목표는 <code>generalization</code>, 즉 학습 데이터로 학습시킨 <strong>모델이 본 적 없는 데이터에 대해서도 예측</strong>(prediction, approximation)을 잘 해내는 것이다. 본 적 없는 상황을 예측하기 위해서는 학습된 가정 이외에 추가적인 가정이 필요한데, 이것이 바로 <code>inductive bias</code>이다.</li>
</ul>
</li>
<li>
<p><code>translation equivariance</code>: 입력 데이터가 일정한 변환을 받을 때, 그 변환이 <strong>모델의 출력에도 동일하게 반영되는 성질</strong></p>
<ul>
<li>CNN의 합성곱 층에서는 이미지의 특정 패턴이 위치를 옮겨도 그 패턴을 감지하는 필터의 반응이 동일하게 이동한다. 즉, 입력 이미지의 패턴이 이동하면, 해당 패턴을 인식하는 <strong>뉴런의 활성화 위치도 동일하게 이동한다</strong>.</li>
<li>어떠한 사물이 들어 있는 이미지를 제공해줄 때 사물의 위치가 바뀌면 CNN과 같은 연산의 activation 위치 또한 바뀌게 된다.</li>
</ul>
</li>
<li>
<p><code>Locality</code>: 이미지 내의 픽셀들이 인접한 다른 픽셀들과 더 밀접한 관계를 가지는 특성</p>
</li>
</ul>
</li>
</ol>
<br>
<ol start="10">
<li>그에 반해, <code>Transformer</code>는 <code>inductive biases</code>가 부족하여 <strong>데이터가 충분하지 않을 때 일반화가 되지 않는다.</strong></li>
</ol>
<br>
<ol start="11">
<li>그러나 <strong>더 큰 데이터 셋</strong>으로 훈련을 했을 경우 모델의 성능이 크게 향상이 된다. <strong>따라서 대규모 훈련이 inductive bias를 능가하는 것을 확인을 하였다.</strong></li>
</ol>
<hr>
<h2 id="method">Method</h2>
<div style="text-align:center;">
<img src="/images/paper/ViT/overall.png" height="100%" width="80%"> </div>
<ol start="12">
<li><code>위 사진</code>은 <code>ViT</code>의 전체적인 구성에 대한 그림이다. 본 논문에서는 최대한 가능한 원래의 <code>Transformer</code>구조를 따를려고 했다.</li>
</ol>
<br>
<ol start="13">
<li>그렇게 되면 확장 가능한 <code>NLP Transformer</code> 아키텍처와 그 <strong>효율적인 구현을 거의 그대로 사용할 수 있다는 장점</strong>이 있기 때문이다.</li>
</ol>
<h3 id="vision-transformer-vit">Vision Transformer (ViT)</h3>
<ol start="14">
<li><code>ViT</code>의 전체적인 구조를 설명하고 있다. 크게 <code>이미지 입력 처리</code>, <code>패치 임베딩</code>, <code>[class] 토큰 추가</code>, <code>position 임베딩</code>, <code>Transformer Encoder 구조</code> 순으로 설명할 것이다.</li>
</ol>
<br>
<ol start="15">
<li>먼저 <code>이미지 입력 처리</code>에서 입력 이미지는 $ H * W * C $ (<code>H,W</code> 이미지 높이와 너비, <code>C</code>: 채널 수) 크기를 가지지만 1D 시퀀스의 입력을 받는 <code>Transformer</code>의 구조에 맞게 변형이 필요하다.</li>
</ol>
<br>
<ol start="16">
<li>따라서 $ H * W * C $ 에서 $ N * (P^2 * C) $ ($N = \frac{H*W}{P^2}$, $P^2$은 각 패치의 크기, C: 채널 수)로 변환을 해야한다. 또한 <strong>이 각 패치는 입력에 맞게 linear projection되어</strong> 고정된 크기 $D$의 벡터로 매핑이 되어 이 과정을 통해 <strong>입력 데이터를 학습 가능하게 변환한다.</strong></li>
</ol>
<ul>
<li><code>Linear projcetion</code>: <u>입력 벡터에 선형 변환을 적용하여 다른 차원의 벡터로 변환하는 과정.</u>
<ul>
<li>$y = W * x + b$ 일 때 $y$는 출력 백터로 고정된 크기의 $D$차원의 벡터이고, W는 가중치 행렬로 학습 가능한 행렬이다.</li>
<li>이미지 패치가 $16 * 16 * 3$일 때 이 패치를 펼쳐서 $16 * 16 * 3 = 768$크기의 벡터로 나타낼 수 있다</li>
<li>이때 <code>Linear projection</code>을 통해 $D = 512$ 크기의 벡터로 변환하려면, $512 * 768$크기의 학습 가능한 가중치 행렬 $W$를 사용해 <code>Linear projection</code>을 사용하여 <strong>다른 차원의 벡터로 변환하는 것이다.</strong></li>
</ul>
</li>
</ul>
<br>
<ol start="17">
<li>이렇게 projection을 통해 매핑된 벡터들은 <code>patch embedding(패치 임베딩)</code>이라고 불리운다.</li>
</ol>
<br>
<ol start="18">
<li>그 다음 <code>[class] 토큰 추가</code>으로는 <code>BERT</code> 모델처럼, [class] 토큰을 이미지 패치 임베딩 시퀀스의 맨 앞에 추가한다. (맨 앞 일땐  $z^0_0$)</li>
</ol>
<br>
<ol start="19">
<li>이 시퀀스가 <code>Transformer</code> 인코더를 거쳐 나온 [class] 토큰의 최종 출력 $z^0_L$은 이미지 전체를 표현하는 벡터로 사용된다.</li>
</ol>
<br>
<ol start="20">
<li>이 벡터에 <code>Normalization</code>을 적용하면 이미지 표현 $y$를 얻게 된다.</li>
</ol>
<ul>
<li><strong>즉, 최종 output으로 이 token이 나왔을 때 이미지에 대한 1차원 representation vector로써의 역할을 수행하게 된다.</strong></li>
</ul>
<br>
<ol start="21">
<li>그 다음 <code>position 임베딩</code>은 Transformer에 위치 정보를 함께 주기 위하여 패치 임베딩에 위치 정보를 추가해서 준다.</li>
</ol>
<div style="text-align:center;">
<img src="/images/paper/ViT/transformer.png" height="100%" width="80%"> </div>
<ol start="22">
<li>마지막으로 <code>Transformer Encoder 구조</code>는 교차적으로 반복되는 <code>Multi-Head Self-Attention</code>와 <code>MLP</code> 블록으로 구성되어 있다.</li>
</ol>
<ul>
<li><strong>위 그림처럼 Class token만이 <code>MLP</code>블록을 거친다.</strong></li>
<li>이때 나온 $y$가 1차원 이미지의 대한 representation vector로써의 역할을 수행하기 때문이다.</li>
</ul>
<br>
<p>$$z_0 = [x_{\text{class}}; x_p^1 E; x_p^2 E; \dots; x_p^N E] + E_{\text{pos}}, \quad E \in \mathbb{R}^{(P^2 \cdot C) \times D}, \quad E_{\text{pos}} \in \mathbb{R}^{(N+1) \times D}$$</p>
<ol start="23">
<li>다음은 본 논문에 나온 <code>(1)</code>에 대한 이해이다.</li>
</ol>
<br>
<ol start="24">
<li>위 식에서 $z_0$은 <code>Transformer</code>의 인코더에 첫 번째 레이어에 입력으로 들어가는 시퀀스이다.</li>
</ol>
<br>
<ol start="25">
<li>첫 번째일 때 $x_\text{class}$는 학습 가능한 <code>[class]토큰</code>이다. 또한 그 뒤의 $x_p^iE$는 $i$번째의 패치 임베딩이다. 이때 $E$는 학습 가능한 linear projection으로 입력 패치를 고정된 크기의 $D$의 벡터로 변환하는 것을 확인 할 수 있다.</li>
</ol>
<ul>
<li>이때 $x_p^i$는 $ x_p^i \in \mathbb {R}^{N \times (P^2 \cdot C)}$ 으로 $E$와 계산을 하게 되면 $D$차원으로 변경이 되는 것을 확인할 수 있게 된다.</li>
</ul>
<h3 id="inductive-bias">Inductive bias</h3>
<ol start="26">
<li>다음으로 <code>ViT</code>는 <code>CNN</code>에 비해 <code>inductive bias</code>가 작다. <code>CNN</code>에서는 <code>locality</code>, <code>translational equivariance</code>을 가지고 있어 공간 정보를 잘 사용할 수 있게 한다.</li>
</ol>
<br>
<ol start="27">
<li>그에 반해, <code>ViT</code>는 <code>MLP</code>만이 <code>local</code>, <code>translational equivariance</code>를 가지고 있고 <code>self-attention</code>은 <code>global</code>한 정보를 가지고 있다.</li>
</ol>
<br>
<ol start="28">
<li>
<p><code>ViT</code>는 이미지를 패치로 나누어 <code>positional embedding</code>만을 사용하여 조정하기 때문에 <code>locality</code>가 매우 제한적이다. 따라서 모든 spatial 관계는 모델이 학습을 해야한다.</p>
<pre tabindex="0"><code>Inductive bias: 주어지지 않은 입력의 출력을 예측가능하도록 모델이 가지고 있는 가정들의 집합
즉, 일반화의 성능을 높이기 위해서 만약의 상황에 대한 추가적인 가정이다.

머신러닝은 특정 문제를 풀기 위해 학습 데이터에 대해서 가장 loss가 작은 Hypothesis를 찾는다.
하지만 Hypothesis의 제한이 없다면 overfitting이 일어나므로 제한을 걸어주는데 이 제한이 바로 Inductive bias이다.
Inductive bias와 generalization은 서로 Trade-off이다.

그리고 Inductive Bias가 강할수록, 작은 데이터셋에 대해 학습 성능이 더 좋아지는 경향이 있다.
Transformer가 부족한 Inductive bias 때문에 성능 향상을 
위해 많은 양의 데이터셋이 필요한 대신, robust하게 동작한다. (generalization 성능이 뛰어나서)
</code></pre></li>
</ol>
<h3 id="hybrid-architecture">Hybrid Architecture</h3>
<ol start="29">
<li>기존에는 <code>CNN</code>의 <code>feature map</code>에서 입력 시퀀스를 형성할 수 있었지만 본 논문에서 소개하는 구조에서는 <code>CNN</code>의 <code>feature map</code>에서 추출한 patch에 대해 <code>patch embedding</code>을 적용한다.</li>
</ol>
<br>
<ol start="30">
<li>패치가 $1*1$일 땐 단순히 feature map의 공간적 차원을 평탄화하고 Transformer로 투영하여 입력 시퀀스를 얻을 수 있다.</li>
</ol>
<hr>
<h2 id="experiments">Experiments</h2>
<div style="text-align:center;">
<img src="/images/paper/ViT/result.png" height="100%" width="80%"> </div>
<div style="text-align:center;">
<img src="/images/paper/ViT/visual.png" height="100%" width="80%"> </div>
<ol start="31">
<li><code>위 그림</code>을 보면 더 큰 데이터로 학습을 했을 때 성능이 훨씬 뛰어난 것으로 확인할 수 있게 된다.</li>
</ol>
<br>
<ol start="32">
<li>그 <code>밑 그림</code>을 보면 왼쪽부터 각각 <code>embedding filter</code>, <code>position embedding</code>, <code>attention distance</code>를 보인다.</li>
</ol>
<br>
<ol start="33">
<li><code>positional embedding</code>을 보면 각각의 이미지에 대한 위치를 잘 학습한 것을 볼 수 있고 <code>attnetion distance</code> 그림을 보면 <strong>network가 깊어질 수록 image가 통합되는 모습을 확인할 수 있다.</strong></li>
</ol>
<ul>
<li><code>self-attention</code>은 ViT를 <strong>통합된 정보를</strong> 만들도록 한다.</li>
</ul>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li>
<p><a href="https://hipgyung.tistory.com/entry/%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-ViTVision-Transformer-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale" target="_blank" rel="noopener noreffer ">https://hipgyung.tistory.com/entry/%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-ViTVision-Transformer-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale</a></p>
</li>
<li>
<p><a href="https://wikidocs.net/137253" target="_blank" rel="noopener noreffer ">https://wikidocs.net/137253</a></p>
</li>
<li>
<p><a href="https://velog.io/@qtly_u/ViT%EC%9D%98-Inductive-Bias%EA%B0%80-%EB%8F%84%EB%8C%80%EC%B2%B4-%EC%96%B4%EB%96%BB%EB%8B%A4%EB%8A%94-%EA%B1%B0%EC%95%BC" target="_blank" rel="noopener noreffer ">https://velog.io/@qtly_u/ViT%EC%9D%98-Inductive-Bias%EA%B0%80-%EB%8F%84%EB%8C%80%EC%B2%B4-%EC%96%B4%EB%96%BB%EB%8B%A4%EB%8A%94-%EA%B1%B0%EC%95%BC</a></p>
</li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2024-07-31</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/paper/vit/" data-title="[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)" data-hashtags="Self-Attention,논문 리뷰,computer vision,Transformer,ViT"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/paper/vit/" data-hashtag="Self-Attention"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/paper/vit/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/paper/vit/" data-title="[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/paper/vit/" data-title="[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/paper/vit/" data-title="[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/paper/vit/" data-title="[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/self-attention/">Self-Attention</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/transformer/">Transformer</a>,&nbsp;<a href="/tags/vit/">ViT</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/cs231n/lecture9/" class="prev" rel="prev" title="[CS231n] 09.CNN Architectures"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[CS231n] 09.CNN Architectures</a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
