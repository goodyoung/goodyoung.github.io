<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet) - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)" />
<meta property="og:description" content="개요 Image Segmentation 문제를 해결하기 위해 제안된 대표적인 딥러닝 모델인 UNet에 대한 논문 리뷰를 해보려고 한다. Abstract 딥러닝 네트워크에선 성공적인 훈련을 위해선 수많은 annotated training samples들이 필요한데 이것들을 효율적으로 사용할 수 있는 강력한 data augmentation 방법에 대해서 소개한다. 그 다음으론 Unet의 구조에 대해서 간략한 소개가 나온다. Unet의 구조는 context들을 포착할 수 있는 contracting path와 localization을 정확하게 해주는 expanding path가 서로 대칭적으로 이루어져 있다는 것을 알려준다. 그리고 또한 Unet은 적은 양의 image로 학습을 할 수 있고 ISBI challenge에서 그 전 모델보다 더 좋은 성능을 보여주었다. 이런 Unet에 대해서 좀 더 자세히 Introduction의 파트에서 설명하게 될 것이다. Introduction 본 논문에서는 지난 2년 간 deep convolutional net이 성공을 이룬 이유는 Krizhevsky의 연구(Alex Net) 때문이라고 설명했다. 보통 convolutional network의 사용은 classification task(하나의 이미지를 single class label) 였지만 biomedical image task는 localization(class label이 픽셀 단위로 분류) 이 필요했다. 또한 biomedical tasks에서는 수많은 양의 dataset들을 구하기 어려운 한계도 있었다. [그림 1] sliding-window 따라서 위 그림 처럼 이전에 sliding-window 방식으로 픽셀의 주변 patch가 한 단위가 되어 pixel predict를 하여 localization을 수행하는 방법이 있었지만 두가지의 단점이 있었다. 모든 patch 별로 연산을 하므로 시간이 오래 걸린다. 수많은 중복된 패치들이 많다. patch 크기가 크면 max-pooling layer가 많아져서 localization accuracy가 떨어지고 반대로 patch 크기가 작아지면 littel context를 하여서 localization accuracy와 use of context 간의 trade off가 있다. 따라서 이러한 단점들을 해결하기 위해 Fully convolutional network의 구조를 변형하여 더욱 elegant한 구조로 모델을 만들었다. 적은 데이터셋으로도 work가 가능하고 더욱 더 정밀한 segmentation이 가능해짐 [그림 2] unet main idea Unet의 main idea는 contracting layer(downsampling) 에서 pooling operator가 upsampling operator로 교체되는 연속적인 layer들을 보완하는 것이다. contracting path로 부터 high resolution feature들이 upsampled output과 합쳐진다. 그런 다음 연속적인 합성곱 층들이 더 precise한 output을 조합한다. output의 해상도를 증가시킨다. Upsampling part에서 더 많은 feature channel들을 가지고 있게 된다. 이는 context 정보가 high resolution으로 잘 전달이 되게 한 것이다. 따라서 contracting path와 expansive path가 U-shaped architecture가 된다. 또한 Unet은 fully connected layer를 사용하지 않고 오직 각각의 convolution의 필요한 부분만 사용을 하였다. 이것은 overlap-tile strategy으로 임의의 큰 이미지의 seamless segmentation을 가능하게 한다. 다른 방식은 GPU memory의 한계 때문에 거대한 이미지를 처리할 수 없었다. Sliding Window 방식과 유사하지만 1개의 픽셀만 classification을 하는 것이 아닌 패치 영역을 classification을 한다. 또한 전체 영역을 다 예측하는 것이 아닌 572의 입력을 받아 388 크기의 아웃풋을 생성한다. fully connected layer를 사용하지 않고 오직 convolution만 사용하여 임의의 이미지를 입력받아도 문제가 없다. 큰 이미지를 한정된 GPU에서 학습하게 된다. Border region을 예측하기 위해서 missing context는 input image를 mirroring 하여 extrapolated(나타나게 만들 수 없는 부분을 예측)로 예측이 된다. 위의 오른쪽 그림을 보면 노란색 박스 영역을 예측 하려면 파란색 박스 영역의 이미지(노란 부분보다 더 큰 크기)가 필요한데 이렇게 누락되는 부분은 mirroring의 전략을 가져간다. 실제로 오른쪽 그림의 왼쪽을 자세히 보면 테두리의 이미지가 원본 이미지랑 대칭임을 볼 수 있게 된다. 또한 그 다음 그림은 단계로 넘어가는 과정인데 이전에 예측에 사용했던 patch와 겹치기 때문에 overlap-tile이라고 불린다. 또 만약 572의 입력이 들어올 때 388로 예측을 하기 때문에 외각 부분은 예측할 수 없어서 이 때문에 mirroring 전략을 취하는 것이다. [그림 3] elastic deformation Unet은 little training data에서도 가능했다. 모델 학습 시 data augmentation을 image의 invariance(이미지 불변성)를 학습하는 elastic deformation방법을 적용하였다. deformation: 변형
image를 다양하게 변형하여 augmentation을 하는 방법이다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/paper/unet/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-17T12:59:43+09:00" />
<meta property="article:modified_time" content="2024-05-17T12:59:43+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)"/>
<meta name="twitter:description" content="개요 Image Segmentation 문제를 해결하기 위해 제안된 대표적인 딥러닝 모델인 UNet에 대한 논문 리뷰를 해보려고 한다. Abstract 딥러닝 네트워크에선 성공적인 훈련을 위해선 수많은 annotated training samples들이 필요한데 이것들을 효율적으로 사용할 수 있는 강력한 data augmentation 방법에 대해서 소개한다. 그 다음으론 Unet의 구조에 대해서 간략한 소개가 나온다. Unet의 구조는 context들을 포착할 수 있는 contracting path와 localization을 정확하게 해주는 expanding path가 서로 대칭적으로 이루어져 있다는 것을 알려준다. 그리고 또한 Unet은 적은 양의 image로 학습을 할 수 있고 ISBI challenge에서 그 전 모델보다 더 좋은 성능을 보여주었다. 이런 Unet에 대해서 좀 더 자세히 Introduction의 파트에서 설명하게 될 것이다. Introduction 본 논문에서는 지난 2년 간 deep convolutional net이 성공을 이룬 이유는 Krizhevsky의 연구(Alex Net) 때문이라고 설명했다. 보통 convolutional network의 사용은 classification task(하나의 이미지를 single class label) 였지만 biomedical image task는 localization(class label이 픽셀 단위로 분류) 이 필요했다. 또한 biomedical tasks에서는 수많은 양의 dataset들을 구하기 어려운 한계도 있었다. [그림 1] sliding-window 따라서 위 그림 처럼 이전에 sliding-window 방식으로 픽셀의 주변 patch가 한 단위가 되어 pixel predict를 하여 localization을 수행하는 방법이 있었지만 두가지의 단점이 있었다. 모든 patch 별로 연산을 하므로 시간이 오래 걸린다. 수많은 중복된 패치들이 많다. patch 크기가 크면 max-pooling layer가 많아져서 localization accuracy가 떨어지고 반대로 patch 크기가 작아지면 littel context를 하여서 localization accuracy와 use of context 간의 trade off가 있다. 따라서 이러한 단점들을 해결하기 위해 Fully convolutional network의 구조를 변형하여 더욱 elegant한 구조로 모델을 만들었다. 적은 데이터셋으로도 work가 가능하고 더욱 더 정밀한 segmentation이 가능해짐 [그림 2] unet main idea Unet의 main idea는 contracting layer(downsampling) 에서 pooling operator가 upsampling operator로 교체되는 연속적인 layer들을 보완하는 것이다. contracting path로 부터 high resolution feature들이 upsampled output과 합쳐진다. 그런 다음 연속적인 합성곱 층들이 더 precise한 output을 조합한다. output의 해상도를 증가시킨다. Upsampling part에서 더 많은 feature channel들을 가지고 있게 된다. 이는 context 정보가 high resolution으로 잘 전달이 되게 한 것이다. 따라서 contracting path와 expansive path가 U-shaped architecture가 된다. 또한 Unet은 fully connected layer를 사용하지 않고 오직 각각의 convolution의 필요한 부분만 사용을 하였다. 이것은 overlap-tile strategy으로 임의의 큰 이미지의 seamless segmentation을 가능하게 한다. 다른 방식은 GPU memory의 한계 때문에 거대한 이미지를 처리할 수 없었다. Sliding Window 방식과 유사하지만 1개의 픽셀만 classification을 하는 것이 아닌 패치 영역을 classification을 한다. 또한 전체 영역을 다 예측하는 것이 아닌 572의 입력을 받아 388 크기의 아웃풋을 생성한다. fully connected layer를 사용하지 않고 오직 convolution만 사용하여 임의의 이미지를 입력받아도 문제가 없다. 큰 이미지를 한정된 GPU에서 학습하게 된다. Border region을 예측하기 위해서 missing context는 input image를 mirroring 하여 extrapolated(나타나게 만들 수 없는 부분을 예측)로 예측이 된다. 위의 오른쪽 그림을 보면 노란색 박스 영역을 예측 하려면 파란색 박스 영역의 이미지(노란 부분보다 더 큰 크기)가 필요한데 이렇게 누락되는 부분은 mirroring의 전략을 가져간다. 실제로 오른쪽 그림의 왼쪽을 자세히 보면 테두리의 이미지가 원본 이미지랑 대칭임을 볼 수 있게 된다. 또한 그 다음 그림은 단계로 넘어가는 과정인데 이전에 예측에 사용했던 patch와 겹치기 때문에 overlap-tile이라고 불린다. 또 만약 572의 입력이 들어올 때 388로 예측을 하기 때문에 외각 부분은 예측할 수 없어서 이 때문에 mirroring 전략을 취하는 것이다. [그림 3] elastic deformation Unet은 little training data에서도 가능했다. 모델 학습 시 data augmentation을 image의 invariance(이미지 불변성)를 학습하는 elastic deformation방법을 적용하였다. deformation: 변형
image를 다양하게 변형하여 augmentation을 하는 방법이다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/paper/unet/" /><link rel="prev" href="https://goodyoung.github.io/posts/cs231n/lecture2/" /><link rel="next" href="https://goodyoung.github.io/posts/paper/deeplab-v1/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/paper\/unet\/"
        },"genre": "posts","keywords": "UNet, Implement, 논문 리뷰, computer vision, segmentation","wordcount":  1019 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/paper\/unet\/","datePublished": "2024-05-17T12:59:43+09:00","dateModified": "2024-05-17T12:59:43+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-05-17">2024-05-17</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;1019 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;5 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#network-architecture">Network Architecture</a></li>
    <li><a href="#training">Training</a>
      <ul>
        <li><a href="#data-augmentation">Data Augmentation</a></li>
      </ul>
    </li>
    <li><a href="#experiments">Experiments</a></li>
    <li><a href="#implement">Implement</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="개요">개요</h2>
<ol start="0">
<li>Image Segmentation 문제를 해결하기 위해 제안된 대표적인 딥러닝 모델인 <code>UNet</code>에 대한 논문 리뷰를 해보려고 한다.</li>
</ol>
<hr>
<h2 id="abstract">Abstract</h2>
<ol>
<li>딥러닝 네트워크에선 성공적인 훈련을 위해선 수많은 <code>annotated training samples</code>들이 필요한데 이것들을 효율적으로 사용할 수 있는 <code>강력한 data augmentation</code> 방법에 대해서 소개한다.</li>
</ol>
<br>
<ol start="2">
<li>그 다음으론 Unet의 구조에 대해서 간략한 소개가 나온다.</li>
</ol>
<br>
<ol start="3">
<li>Unet의 구조는 context들을 포착할 수 있는 <code>contracting path</code>와 localization을 정확하게 해주는 <code>expanding path</code>가 서로 대칭적으로 이루어져 있다는 것을 알려준다.</li>
</ol>
<br>
<ol start="4">
<li>그리고 또한 <strong>Unet은 적은 양의 image로 학습을 할 수 있고</strong> ISBI challenge에서 그 전 모델보다 더 좋은 성능을 보여주었다.</li>
</ol>
<br>
<ol start="5">
<li>이런 Unet에 대해서 좀 더 자세히 Introduction의 파트에서 설명하게 될 것이다.</li>
</ol>
<hr>
<h2 id="introduction">Introduction</h2>
<ol start="6">
<li>본 논문에서는 지난 2년 간 deep convolutional net이 성공을 이룬 이유는 <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" target="_blank" rel="noopener noreffer ">Krizhevsky의 연구(Alex Net)</a> 때문이라고 설명했다.</li>
</ol>
<br>
<ol start="7">
<li>보통 convolutional network의 사용은 <code>classification task</code>(하나의 이미지를 single class label) 였지만 biomedical image task는 <code>localization</code>(class label이 픽셀 단위로 분류) 이 필요했다.</li>
</ol>
<br>
<ol start="8">
<li>또한 biomedical tasks에서는 수많은 양의 dataset들을 구하기 어려운 한계도 있었다.</li>
</ol>
<br>
<figure><a class="lightgallery" href="/images/paper/unet/sliding-window.png" title="/images/paper/unet/sliding-window.png" data-thumbnail="/images/paper/unet/sliding-window.png" data-sub-html="<h2>[그림 1] sliding-window</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/paper/unet/sliding-window.png"
            data-srcset="/images/paper/unet/sliding-window.png, /images/paper/unet/sliding-window.png 1.5x, /images/paper/unet/sliding-window.png 2x"
            data-sizes="auto"
            alt="/images/paper/unet/sliding-window.png" width="20%" />
    </a><figcaption class="image-caption">[그림 1] sliding-window</figcaption>
    </figure>
<ol start="9">
<li>따라서 <code>위 그림</code> 처럼 이전에 sliding-window 방식으로 픽셀의 주변 patch가 한 단위가 되어 pixel predict를 하여 <code>localization</code>을 수행하는 방법이 있었지만 두가지의 단점이 있었다.
<ul>
<li>모든 patch 별로 연산을 하므로 시간이 오래 걸린다.</li>
<li>수많은 중복된 패치들이 많다.</li>
<li>patch 크기가 크면 max-pooling layer가 많아져서 localization accuracy가 떨어지고 반대로 patch 크기가 작아지면 littel context를 하여서 <strong>localization accuracy와 use of context 간의 trade off가 있다.</strong></li>
</ul>
</li>
</ol>
<br>
<ol start="10">
<li>따라서 이러한 단점들을 해결하기 위해 <code>Fully convolutional network</code>의 구조를 변형하여 더욱 elegant한 구조로 모델을 만들었다.
<ul>
<li>적은 데이터셋으로도 work가 가능하고</li>
<li>더욱 더 정밀한 segmentation이 가능해짐</li>
</ul>
</li>
</ol>
<br>
<figure><a class="lightgallery" href="/images/paper/unet/unet-main-idea.png" title="/images/paper/unet/unet-main-idea.png" data-thumbnail="/images/paper/unet/unet-main-idea.png" data-sub-html="<h2>[그림 2] unet main idea</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/paper/unet/unet-main-idea.png"
            data-srcset="/images/paper/unet/unet-main-idea.png, /images/paper/unet/unet-main-idea.png 1.5x, /images/paper/unet/unet-main-idea.png 2x"
            data-sizes="auto"
            alt="/images/paper/unet/unet-main-idea.png" width="20%" />
    </a><figcaption class="image-caption">[그림 2] unet main idea</figcaption>
    </figure>
<ol start="11">
<li><code>Unet</code>의 main idea는 <code>contracting layer</code>(downsampling) 에서 pooling operator가 upsampling operator로 교체되는 연속적인 layer들을 보완하는 것이다.
<ul>
<li>contracting path로 부터 high resolution feature들이 upsampled output과 합쳐진다.</li>
<li>그런 다음 연속적인 합성곱 층들이 더 precise한 output을 조합한다.</li>
<li>output의 해상도를 증가시킨다.</li>
</ul>
</li>
</ol>
<br>
<ol start="12">
<li><code>Upsampling part</code>에서 더 많은 feature channel들을 가지고 있게 된다. 이는 context 정보가 <code>high resolution</code>으로 잘 전달이 되게 한 것이다.
<ul>
<li>따라서 contracting path와 expansive path가 <code>U-shaped architecture</code>가 된다.</li>
</ul>
</li>
</ol>
<br>
<ol start="13">
<li>또한 <code>Unet</code>은 <code>fully connected layer</code>를 사용하지 않고 오직 각각의 convolution의 필요한 부분만 사용을 하였다.</li>
</ol>
<br>
<img src="/images/paper/unet/overlap-tile.png" height="50%" width="48.4%">
<img src="/images/paper/unet/overlap-tile2.png" height="50%" width="50%">
<ol start="14">
<li>이것은 <code>overlap-tile strategy</code>으로 <strong>임의의 큰 이미지</strong>의 <code>seamless segmentation</code>을 가능하게 한다.
<ul>
<li>다른 방식은 GPU memory의 한계 때문에 거대한 이미지를 처리할 수 없었다.</li>
<li>Sliding Window 방식과 유사하지만 1개의 픽셀만 classification을 하는 것이 아닌 패치 영역을 classification을 한다.</li>
<li>또한 전체 영역을 다 예측하는 것이 아닌 572의 입력을 받아 388 크기의 아웃풋을 생성한다.
<ul>
<li>fully connected layer를 사용하지 않고 오직 convolution만 사용하여 임의의 이미지를 입력받아도 문제가 없다.</li>
<li>큰 이미지를 한정된 GPU에서 학습하게 된다.</li>
</ul>
</li>
</ul>
</li>
</ol>
<br>
<img src="/images/paper/unet/overlap-tile3.png" height="50%" width="65%">
<ol start="15">
<li>Border region을 예측하기 위해서 missing context는 input image를 mirroring 하여 <code>extrapolated</code>(나타나게 만들 수 없는 부분을 예측)로 예측이 된다.
<ul>
<li><code>위의 오른쪽 그림</code>을 보면 노란색 박스 영역을 예측 하려면 파란색 박스 영역의 이미지(노란 부분보다 더 큰 크기)가 필요한데 이렇게 누락되는 부분은 mirroring의 전략을 가져간다.
<ul>
<li>실제로 <code>오른쪽 그림의 왼쪽</code>을 자세히 보면 테두리의 이미지가 원본 이미지랑 대칭임을 볼 수 있게 된다.</li>
</ul>
</li>
<li>또한 <code>그 다음 그림</code>은 단계로 넘어가는 과정인데 이전에 예측에 사용했던 patch와 겹치기 때문에 overlap-tile이라고 불린다.</li>
<li>또 만약 572의 입력이 들어올 때 388로 예측을 하기 때문에 외각 부분은 예측할 수 없어서 이 때문에 mirroring 전략을 취하는 것이다.
<br></li>
</ul>
</li>
</ol>
<figure><a class="lightgallery" href="/images/paper/unet/elastic-deformations.png" title="/images/paper/unet/elastic-deformations.png" data-thumbnail="/images/paper/unet/elastic-deformations.png" data-sub-html="<h2>[그림 3] elastic deformation</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/paper/unet/elastic-deformations.png"
            data-srcset="/images/paper/unet/elastic-deformations.png, /images/paper/unet/elastic-deformations.png 1.5x, /images/paper/unet/elastic-deformations.png 2x"
            data-sizes="auto"
            alt="/images/paper/unet/elastic-deformations.png" width="20%" />
    </a><figcaption class="image-caption">[그림 3] elastic deformation</figcaption>
    </figure>
<ol start="16">
<li><code>Unet</code>은 <code>little training data</code>에서도 가능했다. 모델 학습 시 data augmentation을 image의 invariance(이미지 불변성)를 학습하는 <code>elastic deformation</code>방법을 적용하였다.
<ul>
<li>
<blockquote>
<p>deformation: 변형</p>
</blockquote>
</li>
<li>image를 다양하게 변형하여 augmentation을 하는 방법이다.</li>
<li><a href="https://goodyoung.github.io/posts/paper/unet/#data-augmentation" target="_blank" rel="noopener noreffer ">Data Augmentation 파트</a>에서 자세히 다루게 된다.</li>
</ul>
</li>
</ol>
<br>
<ol start="17">
<li>이 방법은 <strong>label이 있는 image를 학습하는 방법(annotated image corpus)이 아니어도</strong> 이런 deformation을 통해 <strong>image의 invariance를</strong> 학습하면서 더욱 네트워크가 <code>robust</code>하게 된다.</li>
</ol>
<br>
<ol start="18">
<li>또한 weighted loss를 사용하여 <strong>접촉이 있는 세포들 사이의 background label은</strong> <strong>loss function에서 큰 가중치</strong>를 얻게 하여 접촉되어 있는 객체를 구분하였다고 나온다.
<ul>
<li><a href="https://goodyoung.github.io/posts/paper/unet/#training" target="_blank" rel="noopener noreffer ">Training 파트</a>에서 자세히 다루게 된다.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="network-architecture">Network Architecture</h2>
<div style="text-align:center;">
  <img src="/images/paper/unet/unet-architecture.png" height="50%" width="70%">
</div>
<ol start="19">
<li>이번엔 <code>UNet</code>의 Network 구조에 대해서 알아보도록 하겠다. 기본적으로 <code>UNet</code>은 <code>Contracting path</code>와 <code>Expansive path</code>로 나뉘는 구조를 가진다.</li>
</ol>
<br>
<ol start="20">
<li><code>Contracting path</code>는 일반 convolutional network랑 동일하다.</li>
</ol>
<br>
<ol start="21">
<li><code>Expansive path</code>는 up-sample을 하고 <code>Contracting path</code>에서 추출한 feature map을 <code>copy and crop</code>을 하여 붙인 후 2번의 convolution을 거친다.
<ul>
<li>이때 crop을 하는 이유는 <code>contracting path</code>와 <code>concatenation을 하려는 feature map</code>과 해상도를 맞춰야 하기 때문이다.</li>
</ul>
</li>
</ol>
<br>
<ol start="22">
<li>마지막 layer에서는 class의 개수로 mapping하기 위해 <code>1*1 convolution</code>이 사용된다.</li>
</ol>
<br>
<ol start="23">
<li>over-lap tile을 수행하기 위하여 max pooling에 들어가는 x,y가 모두 짝수가 되도록 input tile size를 생각해줘야 한다고 나와 있다.</li>
</ol>
<ul>
<li><strong>둘 다 짝수여야 각 영역이 정확히 나누어 떨어지게 되기 때문이다.</strong></li>
</ul>
<br>
<ol start="24">
<li>더 자세한 코드에 대한 구현은 <a href="https://github.com/Sbeom12/basic-for-Segmentation/tree/main/practice/models" target="_blank" rel="noopener noreffer ">여기</a>에서 보면 될 것 같다.</li>
</ol>
<hr>
<h2 id="training">Training</h2>
<ol start="25">
<li>다음으론 <code>UNet</code>을 어떻게 학습을 시켰는지에 대해서 나온다.</li>
</ol>
<br>
<ol start="26">
<li>
<p>본 논문에서는 오버헤드를 최소화 하고 GPU자원을 최대한 활용하기 위해 <strong>많은 작은 이미지를 한 번에 처리하는 대신</strong>, <strong>큰 크기의 이미지를 하나씩 처리하도록 배치 크기를 줄이는 것이다.</strong></p>
<ul>
<li>고해상도 이미지를 다루는 작업에서는 mini batch 큰 이미지를 하나씩 처리하는 전략을 취한다.
<br></li>
</ul>
</li>
<li>
<p>모멘텀을 0.99로 설정하여 이전에 조정한 가중치를 현재 시점의 가중치를 조정하는데 많이 반영하도록 했다.
<img src="/images/paper/unet/sgd-momentum.png" height="50%" width="70%"></p>
<ul>
<li>모멘텀 계수가 높을수록 과거의 기울기 정보가 현재의 가중치 업데이트에 더 큰 영향을 미치게 된다.</li>
<li>진행하던 속도에 관성이 적용되니까 <code>local minima</code>에 빠지더라도 그 지점을 벗어날 가능성이 있게 된다.</li>
</ul>
</li>
</ol>
<br>
<figure><a class="lightgallery" href="/images/paper/unet/soft-entropy.png" title="/images/paper/unet/soft-entropy.png" data-thumbnail="/images/paper/unet/soft-entropy.png" data-sub-html="<h2>[그림 4] energy function</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/paper/unet/soft-entropy.png"
            data-srcset="/images/paper/unet/soft-entropy.png, /images/paper/unet/soft-entropy.png 1.5x, /images/paper/unet/soft-entropy.png 2x"
            data-sizes="auto"
            alt="/images/paper/unet/soft-entropy.png" width="20%" />
    </a><figcaption class="image-caption">[그림 4] energy function</figcaption>
    </figure>
<ol start="28">
<li>그리고 <code>energy function</code>을 사용했다. 이는 맨 마지막 layer에 pixel별로 soft-max(p_l(x))를 수행하고 여기에 <code>cross entropy loss</code>를 적용하는 식이다.</li>
</ol>
<br>
<ol start="29">
<li>또한 <code>weight map</code>을 <code>Cross Entropy</code>에 곱하였는데, 더 중요한 pixel을 더욱 더 강조하기 위해서이다. <code>weight map</code>수식은 아래 그림과 같다.</li>
</ol>
<br>
<figure><a class="lightgallery" href="/images/paper/unet/weight-map.png" title="/images/paper/unet/weight-map.png" data-thumbnail="/images/paper/unet/weight-map.png" data-sub-html="<h2>[그림 5] weight map</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/paper/unet/weight-map.png"
            data-srcset="/images/paper/unet/weight-map.png, /images/paper/unet/weight-map.png 1.5x, /images/paper/unet/weight-map.png 2x"
            data-sizes="auto"
            alt="/images/paper/unet/weight-map.png" width="20%" />
    </a><figcaption class="image-caption">[그림 5] weight map</figcaption>
    </figure>
<ol start="30">
<li><code>weight map</code>에서 특정 클래스가 가지는 fixel freuency 차이를 보완해주고, 접촉하는 cell간의 <code>small separation border</code>를 학습을 시킨다.</li>
</ol>
<ul>
<li>touching cell의 경우에 그 두 클래스 간의 픽셀 수에 불균형이 있을 경우, 모델은 빈도가 많은 클래스에 집중할 가능성이 있게 된다.
따라서 이런 차이를 보상해주는 것이 중요하게 된다.</li>
</ul>
<br>
<ol start="31">
<li>이런 <code>separation border</code>는 <code>morphological operation</code>을 사용하여 계산이 된다.</li>
</ol>
<br>
<ol start="32">
<li><code>Wc</code>는 Class Frequency의 balance를 위한 것이고, <code>d1</code>은 근처 cell의 border 거리, <code>d2</code>는 두번째로 가까운 cell의 border 거리이다.</li>
</ol>
<br>
<ol start="33">
<li>따라서 모델이 다양해지면서 <code>weight initialization</code>이 굉장히 중요해졌다.
<ul>
<li>그렇지 않으면 일부가 지나치게 활성화되어 다른 부분은 전혀 기여하지 않을 수 있다.</li>
<li>본 논문에선 Gaussian Distribution을 사용하였다.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="data-augmentation">Data Augmentation</h3>
<ol start="34">
<li><code>Data Augmentation</code>은 few training sample일 때 모델이 <code>특정 invariance</code>을 알게 되고 <code>robust</code>해지기 위해 중요하다.</li>
</ol>
<br>
<ol start="35">
<li><code>microscopical image</code>들은 특히 이런 Data augmentation이 필요하게 되는데 <code>UNet</code>은 <code>shift</code>, <code>rotation</code>등을 수행하였다.</li>
</ol>
<br>
<ol start="36">
<li>또한 <code>random elastic deformation</code><a href="http://goodyoung.github.io/posts/paper/unet/#training" target="_blank" rel="noopener noreffer ">그림 3</a>을 사용한 것이 <code>key concept</code>이다.
<ul>
<li>
<blockquote>
<p>elastic(탄성): 속체에서 어떤 힘이나 시간 흐름으로 인해 변화가 발생하는 경우. 이 힘이 제거된 후 변형이 원래처럼 돌아오게 되면 이 변형을 탄성이라고 합니다. 이렇게 탄성이 있는 경우는 같은 물체라 해도 촬영 방법이나 각도 등에 의해서 다른 결과를 가져올 수 있으므로 이럴 때 사용하면 좋다고 하나 이 외의 경우에도 사용하게 된다.</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<br>
<ol start="37">
<li><code>contracting path</code>의 end 부분에 <code>drop-out layer</code>을 추가하여 더욱 절대적인 data augmentation을 수행하였다.</li>
</ol>
<hr>
<h2 id="experiments">Experiments</h2>
<img src="/images/paper/unet/eval1.png" height="50%" width="41%">
<img src="/images/paper/unet/eval2.png" height="50%" width="58%">
<ol start="38">
<li><code>UNet</code>은 왼쪽 그림의 EM segmentation challenge에서 Warping Error에 대해 가장 좋은 성능을 보였다.</li>
</ol>
<br>
<ol start="39">
<li>또한 오른쪽 그림의 Cell Segmentation Task(ISBI cell tracking challenge)에서의 <strong>IOU</strong>가 2등과 <strong>확연한 차이</strong>를 보여주어 우승을 하게 하였다.</li>
</ol>
<h2 id="implement">Implement</h2>
<ol start="35">
<li>코드에 대한 구현은 <a href="https://github.com/Sbeom12/basic-for-Segmentation/blob/main/practice/models/UNet.py" target="_blank" rel="noopener noreffer ">여기</a>에 해두었다.</li>
</ol>
<br>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1505.04597" target="_blank" rel="noopener noreffer ">https://arxiv.org/pdf/1505.04597</a></li>
<li><a href="https://stat-cbc.tistory.com/28" target="_blank" rel="noopener noreffer ">https://stat-cbc.tistory.com/28</a></li>
<li><a href="https://deep-math.tistory.com/23" target="_blank" rel="noopener noreffer ">https://deep-math.tistory.com/23</a></li>
<li><a href="https://velog.io/@minkyu4506/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-U-Net-%EB%A6%AC%EB%B7%B0" target="_blank" rel="noopener noreffer ">https://velog.io/@minkyu4506/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-U-Net-%EB%A6%AC%EB%B7%B0</a></li>
<li><a href="https://blog.kubwa.co.kr/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-unet-convolutional-networks-for-biomedical-image-segmentation-2015-unet%EA%B5%AC%ED%98%84-%EC%8B%A4%EC%8A%B5-w-pytorch-40f9e54890d3" target="_blank" rel="noopener noreffer ">https://blog.kubwa.co.kr/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-unet-convolutional-networks-for-biomedical-image-segmentation-2015-unet%EA%B5%AC%ED%98%84-%EC%8B%A4%EC%8A%B5-w-pytorch-40f9e54890d3</a></li>
<li><a href="https://eda-ai-lab.tistory.com/546" target="_blank" rel="noopener noreffer ">https://eda-ai-lab.tistory.com/546</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2024-05-17</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/paper/unet/" data-title="[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)" data-hashtags="UNet,Implement,논문 리뷰,computer vision,segmentation"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/paper/unet/" data-hashtag="UNet"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/paper/unet/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/paper/unet/" data-title="[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/paper/unet/" data-title="[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/paper/unet/" data-title="[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/paper/unet/" data-title="[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/unet/">UNet</a>,&nbsp;<a href="/tags/implement/">Implement</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/segmentation/">segmentation</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/cs231n/lecture2/" class="prev" rel="prev" title="[CS231n] 02.Image Classification"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[CS231n] 02.Image Classification</a>
            <a href="/posts/paper/deeplab-v1/" class="next" rel="next" title="[Paper Review]Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs(DeepLab V1)">[Paper Review]Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs(DeepLab V1)<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.121.2">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2022 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://goodyoung.github.io" target="_blank">GoodYoung</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
