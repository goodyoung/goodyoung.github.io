<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM) - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)" />
<meta property="og:description" content="개요 DDPM 이후, 고해상도 이미지 생성을 위해 효율적인 latent space에서 확산 과정을 수행하는 LDM에 대해 논문 리뷰를 할 것이다. Introduction 이미지 합성(Image synthesis)은 최근 가장 빠르게 발전이 되어왔지만, 많은 컴퓨터 계산 비용이 크다. 특히 고해상도 복원 문제는 AR(Autoregressive) 기반 모델들이 자주 사용하지만 모델 수십억 개의 파라미터를 요구한다. GAN은 학습 방식의 한계 때문에 multi-modal 분포에서는 모델링 하는데 한계가 있다. 이러한 가운데, Diffusion model이 이미지 합성의 여러 분야에서 뛰어난 성과(SOTA)를 보여줬다. Diffusion 모델은 다른 모델들과 다르게 모델 붕괴(model collapse), 학습 불안정성, 많은 파라미터에서 강점을 지닌다. 이러한 Diffusion 모델도 문제점이 있는데, Diffusion 모델은 mode-covering 성질을 갖고 있다. 그래서 데이터의 모든 세부 패턴을 학습하려는 경향이 있어서 많은 연산 자원을 필요로 하게 된다. Reweighted Variational Objective 방법이 연산을 줄이려고 하지만 여전히 계산량이 많다. A100 기준으로 50,000개의 샘플을 생성하는데 5일이 걸린다. 이러한 문제는 두 가지 영향을 보여준다. 첫 번째로는 훈련 시 거대한 컴퓨팅 자원을 필요로 하므로 일반 연구자나 소규모 연구팀에게 접근성이 낮다. 두 번째로는 추론 시 높은 비용과 시간을 소모하여 학습 뿐 아니라 샘플링 시에도 매우 비효율적이다.&#39; 따라서 이 두 가지의 문제를 해결하기 위한 것이 핵심이다. 본 논문은 pixel space에서 이미 학습된 diffusion 모델을 분석하는 것 부터 시작한다. 기존의 DM은 픽셀 단위에서 학습을 진행 하였다. 이미지 자체에 대해서 훈련하는 방식이였다. 위 그림과 같이 모든 likelihood-based 모델들의 학습 과정은 두 단계로 나뉠 수 있다. 첫 번째로 높은 주파수 영역의 세부사항을 제거하며 압축을 수행하는 지각적 압축 단계이고, 두 번째로는 실제 생성 모델이 데이터의 의미적이고 개념적인 구성을 학습하는 단계이다. 본 논문은 위 두 단계와 동일하지만, 계산적으로는 더욱 효율적인 공간을 사용하는 모델을 제안한다. 본 논문에서 모델은 두 단계를 제안한다. 첫 번째로 autoencoder를 학습하여 pixel space와 지각적으로 동일하지만, 더욱 효율적인 저차원의 잠재 공간을 만든다. 두 번째로는 추가적인 공간 압축에 의존할 필요 없이 잠재 공간에서 diffusion 모델을 학습시켜 공간적 차원성(spatial dimensionality)에 대해 더 나은 확장성을 갖고 있다. 이미지 자체에 노이즈를 추가하여 학습을 했던 방식과 달리, 잠재 공간에서 학습을 하자는 것이다. 이러한 복잡성 감소 때문에 단 한 번의 네트워크 실행으로도 효율적인 이미지 생성을 할 수 있고, 본 논문은 이 모델을 Latent Diffsion Models(LDMs) 라고 부른다. 이 방법의 장점은 방대한 autoencoder를 한 번만 학습을 하게 된다면, 이를 통해 나온 latent space를 여러 DM 모델의 훈련에 사용할 수 있게 된다. 이는 곧 여러 task에서도 재사용할 수 있게 된다는 것이다. 마지막으로 본 논문의 주요 contribution에 대해 정리한다. Transformer만으로 이루어진 접근법들과 달리 더 높은 차원의 데이터에도 효율적으로 적용할 수 있다.
여러 task(inpainting 등)에서 계산 비용을 크게 감소 시키면서 경쟁력 있는 성능을 달성하였다.
기존 연구는 재구성(reconstruction)과 생성(generative)능력 사이의 차이를 조절하는 것이 중요했지만, 본 논문의 모델은 그것이 필요 없다.
두 가지를 분리해서 해결했기 때문이다. Autoencoder는 오직 **재구성(압축과 복원)**만 담당한다. Diffusion 모델은 오직 **이미지 생성(새로운 이미지 합성)**만 담당한다. 초해상도와 같은 고밀도 작업에서도 적용이 가능하다.
cross attention을 기반으로 하는 매커니즘을 개발하여 multi-modal data에도 사용할 수 있다.
Method Introduction에서도 설명이 되어있듯이, 기존 DM은 pixel space에서 매우 비용이 큰 연산을 수행해야한다는 단점이 있다. 이를 해결하기 위하여 압축과 생성 단계를 분리 하였고, 압축 단계는 계산 비용이 작은 autoencoder를 사용한다. 이를 통해 계산 효율성이 증가하고, 반복적으로 사용할 수 있는 latent space를 제공하여 범용적인 압축 모델이 된다. 또한 본 논문은 UNet의 Inductive bias를 활용하여 공간적 구조를 잘 표현하기 때문에, 과도하게 압축하지 않고도 효과적으로 이미지를 잘 생성해낼 수 있게 된다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/paper/ldm/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-02-17T17:57:23+09:00" />
<meta property="article:modified_time" content="2025-02-17T17:57:23+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)"/>
<meta name="twitter:description" content="개요 DDPM 이후, 고해상도 이미지 생성을 위해 효율적인 latent space에서 확산 과정을 수행하는 LDM에 대해 논문 리뷰를 할 것이다. Introduction 이미지 합성(Image synthesis)은 최근 가장 빠르게 발전이 되어왔지만, 많은 컴퓨터 계산 비용이 크다. 특히 고해상도 복원 문제는 AR(Autoregressive) 기반 모델들이 자주 사용하지만 모델 수십억 개의 파라미터를 요구한다. GAN은 학습 방식의 한계 때문에 multi-modal 분포에서는 모델링 하는데 한계가 있다. 이러한 가운데, Diffusion model이 이미지 합성의 여러 분야에서 뛰어난 성과(SOTA)를 보여줬다. Diffusion 모델은 다른 모델들과 다르게 모델 붕괴(model collapse), 학습 불안정성, 많은 파라미터에서 강점을 지닌다. 이러한 Diffusion 모델도 문제점이 있는데, Diffusion 모델은 mode-covering 성질을 갖고 있다. 그래서 데이터의 모든 세부 패턴을 학습하려는 경향이 있어서 많은 연산 자원을 필요로 하게 된다. Reweighted Variational Objective 방법이 연산을 줄이려고 하지만 여전히 계산량이 많다. A100 기준으로 50,000개의 샘플을 생성하는데 5일이 걸린다. 이러한 문제는 두 가지 영향을 보여준다. 첫 번째로는 훈련 시 거대한 컴퓨팅 자원을 필요로 하므로 일반 연구자나 소규모 연구팀에게 접근성이 낮다. 두 번째로는 추론 시 높은 비용과 시간을 소모하여 학습 뿐 아니라 샘플링 시에도 매우 비효율적이다.&#39; 따라서 이 두 가지의 문제를 해결하기 위한 것이 핵심이다. 본 논문은 pixel space에서 이미 학습된 diffusion 모델을 분석하는 것 부터 시작한다. 기존의 DM은 픽셀 단위에서 학습을 진행 하였다. 이미지 자체에 대해서 훈련하는 방식이였다. 위 그림과 같이 모든 likelihood-based 모델들의 학습 과정은 두 단계로 나뉠 수 있다. 첫 번째로 높은 주파수 영역의 세부사항을 제거하며 압축을 수행하는 지각적 압축 단계이고, 두 번째로는 실제 생성 모델이 데이터의 의미적이고 개념적인 구성을 학습하는 단계이다. 본 논문은 위 두 단계와 동일하지만, 계산적으로는 더욱 효율적인 공간을 사용하는 모델을 제안한다. 본 논문에서 모델은 두 단계를 제안한다. 첫 번째로 autoencoder를 학습하여 pixel space와 지각적으로 동일하지만, 더욱 효율적인 저차원의 잠재 공간을 만든다. 두 번째로는 추가적인 공간 압축에 의존할 필요 없이 잠재 공간에서 diffusion 모델을 학습시켜 공간적 차원성(spatial dimensionality)에 대해 더 나은 확장성을 갖고 있다. 이미지 자체에 노이즈를 추가하여 학습을 했던 방식과 달리, 잠재 공간에서 학습을 하자는 것이다. 이러한 복잡성 감소 때문에 단 한 번의 네트워크 실행으로도 효율적인 이미지 생성을 할 수 있고, 본 논문은 이 모델을 Latent Diffsion Models(LDMs) 라고 부른다. 이 방법의 장점은 방대한 autoencoder를 한 번만 학습을 하게 된다면, 이를 통해 나온 latent space를 여러 DM 모델의 훈련에 사용할 수 있게 된다. 이는 곧 여러 task에서도 재사용할 수 있게 된다는 것이다. 마지막으로 본 논문의 주요 contribution에 대해 정리한다. Transformer만으로 이루어진 접근법들과 달리 더 높은 차원의 데이터에도 효율적으로 적용할 수 있다.
여러 task(inpainting 등)에서 계산 비용을 크게 감소 시키면서 경쟁력 있는 성능을 달성하였다.
기존 연구는 재구성(reconstruction)과 생성(generative)능력 사이의 차이를 조절하는 것이 중요했지만, 본 논문의 모델은 그것이 필요 없다.
두 가지를 분리해서 해결했기 때문이다. Autoencoder는 오직 **재구성(압축과 복원)**만 담당한다. Diffusion 모델은 오직 **이미지 생성(새로운 이미지 합성)**만 담당한다. 초해상도와 같은 고밀도 작업에서도 적용이 가능하다.
cross attention을 기반으로 하는 매커니즘을 개발하여 multi-modal data에도 사용할 수 있다.
Method Introduction에서도 설명이 되어있듯이, 기존 DM은 pixel space에서 매우 비용이 큰 연산을 수행해야한다는 단점이 있다. 이를 해결하기 위하여 압축과 생성 단계를 분리 하였고, 압축 단계는 계산 비용이 작은 autoencoder를 사용한다. 이를 통해 계산 효율성이 증가하고, 반복적으로 사용할 수 있는 latent space를 제공하여 범용적인 압축 모델이 된다. 또한 본 논문은 UNet의 Inductive bias를 활용하여 공간적 구조를 잘 표현하기 때문에, 과도하게 압축하지 않고도 효과적으로 이미지를 잘 생성해낼 수 있게 된다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/paper/ldm/" /><link rel="prev" href="https://goodyoung.github.io/posts/paper/ddim/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/paper\/ldm\/"
        },"genre": "posts","keywords": "논문 리뷰, computer vision, Diffusion, LDM","wordcount":  1468 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/paper\/ldm\/","datePublished": "2025-02-17T17:57:23+09:00","dateModified": "2025-02-17T17:57:23+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/diffusion/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Diffusion</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-02-17">2025-02-17</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;1468 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;7 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#method">Method</a>
      <ul>
        <li><a href="#perceptual-image-compression">Perceptual Image Compression</a></li>
        <li><a href="#latent-diffusion-models">Latent Diffusion Models</a></li>
        <li><a href="#conditioning-mechanisms">Conditioning Mechanisms</a></li>
      </ul>
    </li>
    <li><a href="#experiments">Experiments</a></li>
    <li><a href="#limitation">Limitation</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- image format
<div style="text-align:center;">
<img src="/images/cs231n/lecture4/back-1.png" height="100%" width="80%"> </div>
 -->
<h2 id="개요">개요</h2>
<ol start="0">
<li><code>DDPM</code> 이후, <code>고해상도 이미지</code> 생성을 위해 효율적인 <code>latent space</code>에서 <code>확산 과정</code>을 수행하는 <code>LDM</code>에 대해 <code>논문 리뷰</code>를 할 것이다.</li>
</ol>
<hr>
<h2 id="introduction">Introduction</h2>
<ol>
<li><code>이미지 합성(Image synthesis)</code>은 최근 가장 빠르게 발전이 되어왔지만, 많은 컴퓨터 계산 비용이 크다.</li>
</ol>
<ul>
<li>특히 고해상도 복원 문제는 <code>AR(Autoregressive) 기반 모델</code>들이 자주 사용하지만 모델 수십억 개의 파라미터를 요구한다.</li>
</ul>
<br>
<ol start="2">
<li><code>GAN</code>은 학습 방식의 한계 때문에 <code>multi-modal 분포</code>에서는 모델링 하는데 한계가 있다.</li>
</ol>
<br>
<ol start="3">
<li>이러한 가운데, <code>Diffusion model</code>이 <code>이미지 합성</code>의 여러 분야에서 뛰어난 성과(SOTA)를 보여줬다.</li>
</ol>
<br>
<ol start="4">
<li><code>Diffusion 모델</code>은 다른 모델들과 다르게 <code>모델 붕괴(model collapse)</code>, <code>학습 불안정성</code>, <code>많은 파라미터</code>에서 강점을 지닌다.</li>
</ol>
<br>
<ol start="5">
<li>이러한 <code>Diffusion 모델</code>도 문제점이 있는데, <code>Diffusion 모델</code>은 <code>mode-covering</code> 성질을 갖고 있다. 그래서 데이터의 모든 세부 패턴을 학습하려는 경향이 있어서 <strong>많은 연산 자원을 필요로 하게 된다.</strong></li>
</ol>
<ul>
<li><code>Reweighted Variational Objective</code> 방법이 연산을 줄이려고 하지만 여전히 계산량이 많다.</li>
<li>A100 기준으로 50,000개의 샘플을 생성하는데 5일이 걸린다.</li>
</ul>
<br>
<ol start="6">
<li>이러한 문제는 두 가지 영향을 보여준다. <code>첫 번째로는</code> 훈련 시 거대한 컴퓨팅 자원을 필요로 하므로 <strong>일반 연구자나 소규모 연구팀에게 접근성이 낮다.</strong></li>
</ol>
<br>
<ol start="7">
<li><code>두 번째로는</code> 추론 시 높은 비용과 시간을 소모하여 학습 뿐 아니라 <code>샘플링 시</code>에도 <strong>매우 비효율적이다.</strong>'</li>
</ol>
<br>
<ol start="8">
<li>따라서 이 <code>두 가지의 문제</code>를 해결하기 위한 것이 핵심이다.</li>
</ol>
<div style="text-align:center;">
<img src="/images/paper/ldm/model-1.png" height="70%" width="50%"> </div>
<ol start="9">
<li><code>본 논문</code>은 <code>pixel space</code>에서 이미 학습된 <code>diffusion 모델</code>을 분석하는 것 부터 시작한다.</li>
</ol>
<ul>
<li>기존의 <code>DM</code>은 <code>픽셀 단위</code>에서 학습을 진행 하였다.</li>
<li><strong>이미지 자체</strong>에 대해서 훈련하는 방식이였다.</li>
</ul>
<br>
<ol start="10">
<li><code>위 그림</code>과 같이 모든 <code>likelihood-based</code> 모델들의 학습 과정은 두 단계로 나뉠 수 있다.</li>
</ol>
<br>
<ol start="11">
<li><code>첫 번째로</code> 높은 주파수 영역의 세부사항을 제거하며 압축을 수행하는 <code>지각적 압축</code> 단계이고, <code>두 번째로는</code> 실제 생성 모델이 데이터의 의미적이고 <code>개념적인 구성</code>을 학습하는 단계이다.</li>
</ol>
<br>
<ol start="12">
<li><code>본 논문</code>은 위 <code>두 단계</code>와 <strong>동일하지만</strong>, 계산적으로는 더욱 효율적인 공간을 사용하는 모델을 제안한다.</li>
</ol>
<br>
<ol start="13">
<li>본 논문에서 모델은 <code>두 단계</code>를 제안한다. <code>첫 번째</code>로 <code>autoencoder</code>를 학습하여 <code>pixel space</code>와 <strong>지각적으로 동일하지만</strong>, 더욱 효율적인 <code>저차원의 잠재 공간</code>을 만든다.</li>
</ol>
<br>
<ol start="14">
<li><code>두 번째</code>로는 추가적인 공간 압축에 의존할 필요 없이 <code>잠재 공간</code>에서 <code>diffusion 모델</code>을 학습시켜 <code>공간적 차원성(spatial dimensionality)</code>에 대해 더 나은 확장성을 갖고 있다.</li>
</ol>
<ul>
<li>이미지 자체에 노이즈를 추가하여 학습을 했던 방식과 달리, <strong>잠재 공간에서 학습을 하자는 것이다.</strong></li>
</ul>
<br>
<ol start="15">
<li>이러한 복잡성 감소 때문에 <strong>단 한 번의 네트워크 실행으로도 효율적인 이미지 생성</strong>을 할 수 있고, 본 논문은 이 모델을 <code>Latent Diffsion Models(LDMs) </code>라고 부른다.</li>
</ol>
<br>
<ol start="16">
<li>이 방법의 장점은 방대한 <code>autoencoder</code>를 <strong>한 번만</strong> 학습을 하게 된다면, 이를 통해 나온 <code>latent space</code>를 <code>여러 DM 모델의 훈련</code>에 사용할 수 있게 된다.</li>
</ol>
<ul>
<li>이는 곧 <code>여러 task</code>에서도 <code>재사용</code>할 수 있게 된다는 것이다.</li>
</ul>
<br>
<ol start="17">
<li>마지막으로 본 논문의 <code>주요 contribution</code>에 대해 정리한다.</li>
</ol>
<ul>
<li>
<p><code>Transformer만</code>으로 이루어진 접근법들과 달리 <code>더 높은 차원의 데이터</code>에도 효율적으로 적용할 수 있다.</p>
</li>
<li>
<p>여러 task(inpainting 등)에서 계산 비용을 <code>크게 감소</code> 시키면서 <code>경쟁력 있는 성능</code>을 달성하였다.</p>
</li>
<li>
<p>기존 연구는 <code>재구성(reconstruction)</code>과 <code>생성(generative)</code>능력 사이의 차이를 조절하는 것이 중요했지만, <code>본 논문</code>의 모델은 그것이 필요 없다.</p>
<ul>
<li><strong>두 가지를 분리해서 해결했기 때문이다.</strong>
<ul>
<li><code>Autoencoder</code>는 오직 **재구성(압축과 복원)**만 담당한다.</li>
<li><code>Diffusion</code> 모델은 오직 **이미지 생성(새로운 이미지 합성)**만 담당한다.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>초해상도와 같은 고밀도 작업에서도 적용이 가능하다.</p>
</li>
<li>
<p><code>cross attention</code>을 기반으로 하는 <strong>매커니즘</strong>을 개발하여 <code>multi-modal data</code>에도 사용할 수 있다.</p>
</li>
</ul>
<hr>
<h2 id="method">Method</h2>
<ol start="18">
<li><code>Introduction</code>에서도 설명이 되어있듯이, <code>기존 DM</code>은 <code>pixel space</code>에서 매우 비용이 큰 연산을 수행해야한다는 <strong>단점이 있다.</strong></li>
</ol>
<br>
<ol start="19">
<li>이를 해결하기 위하여 <strong>압축과 생성 단계를 분리</strong> 하였고, 압축 단계는 계산 비용이 작은 <code>autoencoder</code>를 사용한다.</li>
</ol>
<br>
<ol start="20">
<li>이를 통해 계산 효율성이 증가하고, 반복적으로 사용할 수 있는 <code>latent space</code>를 제공하여 범용적인 압축 모델이 된다.</li>
</ol>
<br>
<ol start="21">
<li>또한 본 논문은 <code>UNet의 Inductive bias</code>를 활용하여 공간적 구조를 잘 표현하기 때문에, <strong>과도하게 압축하지 않고도</strong> 효과적으로 이미지를 <strong>잘 생성해낼 수 있게 된다.</strong></li>
</ol>
<ul>
<li>
<p>본 논문에서는 <code>UNet 구조</code>를 사용하겠다라는 의미이다.</p>
<details>
  <summary> Inductive bias란? </summary>
<ul>
<li>
<p><code>Inductive biases</code>: 모델이 학습된 <strong>데이터 외의 데이터</strong>에 대해 얼마나 잘 일반화할 수 있는지에 사용하는 가정</p>
<ul>
<li>머신러닝의 최종 목표는 <code>generalization</code>, 즉 학습 데이터로 학습시킨 <strong>모델이 본 적 없는 데이터에 대해서도 예측</strong>(prediction, approximation)을 잘 해내는 것이다. 본 적 없는 상황을 예측하기 위해서는 학습된 가정 이외에 추가적인 가정이 필요한데, 이것이 바로 <code>inductive bias</code>이다.</li>
</ul>
</li>
<li>
<p><a href="https://goodyoung.github.io/posts/paper/vit/" target="_blank" rel="noopener noreffer ">VIT 9번</a>에 나와있는 내용이다.</p>
</li>
</ul>
</details>
</li>
</ul>
<hr>
<h3 id="perceptual-image-compression">Perceptual Image Compression</h3>
<ol start="22">
<li>이 파트에선 본 논문에서 <code>AutoEncoder(AE)</code>를 사용하여 <code>perceptual Image compression</code>을 잘 하게 된 배경에 대해서 설명한다.</li>
</ol>
<br>
<ol start="23">
<li>먼저 <code>AE</code>는 <code>perceptual loss</code>와 <code>patch-based adversarial objective</code>를 사용하여 훈련을 진행한다.</li>
</ol>
<ul>
<li>두 가지 방법의 장점이 있다.</li>
<li>첫째로 <code>local realism</code>을 유지하도록 강제하여 재구성된 이미지가 <strong>원본 이미지의 분포내에 있게 보장한다.</strong></li>
<li>둘째로 단순한 픽셀 공간의 손실에 의존시 발생하는 <strong>흐릿함(bluriness)을 방지한다.</strong></li>
</ul>
<br>
<ol start="23">
<li>또한 <code>AE</code>의 구조를 설명한다. <code>AE</code>는 $x \in \mathbb{R}^{H \times W \times 3}$ 에 대해:</li>
</ol>
<ul>
<li>인코더 $E$는 $x$를 <code>latent representation</code>($z$)으로 변환한다. ($z = E(x)$)
<ul>
<li>$E$는 원본 이미지($x$)를 특정 배율 $f$만큼 다운 샘플링을 진행한다.</li>
<li>비율은 $f = \frac{H}{h} = \frac{W}{w}$와 같고 본 논문은 $f = 2^m$ 으로 실험을 진행한다.</li>
</ul>
</li>
<li>디코더 $D$는 z에서 다시 원본 이미지를 복원한다. ($\tilde{x} = D(z) = D(E(x))$)</li>
<li><code>latent representation</code>는 $z \in \mathbb{R}^{h \times w \times c}$에 속한다.</li>
</ul>
<br>
<ol start="24">
<li>잠재 공간(<code>latent representation</code>)의 <strong>과도한 분산 문제</strong>를 해결하기 위하여 본 논문은 <code>두 가지 정규화 기법</code>을 도입한다.</li>
</ol>
<br>
<ol start="25">
<li>첫 번째로 <code>KL-reg</code>이다. 이는 학습된 잠재 표현이 <strong>표준 정규 분포를 따르도록</strong> <code>KL-penalty</code>를 부여한다.</li>
</ol>
<br>
<ol start="26">
<li>두 번째로 <code>VQ-reg</code>이다. 이는 벡터 양자화를 디코더에 포함시켜 <strong>양자화 층을 디코더 층에 흡수 시킨다.</strong></li>
</ol>
<br>
<ol start="27">
<li>이러한 압축 방식들은 기존 방식(<code>VQ-VAE</code>, <code>VQ-GAN</code>)보다 더 낫다. 왜냐하면 기존 방식은 잠재 공간을 1D로 변환해서 <code>자기회귀</code> 방식을 사용한다. <strong>상당한 z의 구조가 무시되는 경향이 있다.</strong></li>
</ol>
<br>
<ol start="28">
<li>하지만 <code>본 논문</code>은 <code>잠재 공간</code>을 <code>2D 구조</code>로 가져가 <strong>구조를 유지한채로 낮은 압축률</strong>로도 높은 품질의 이미지를 생성해낼 수 있게 된다.</li>
</ol>
<hr>
<h3 id="latent-diffusion-models">Latent Diffusion Models</h3>
<ol start="29">
<li>
<p>기존의 <code>DM</code>은 <code>변분 하한(variational lower bound)</code> 기반의 <code>재가중치(reweighted)</code>된 변형을 사용한다. 이는 <strong>디노이징 스코어 매칭(denoising score-matching)</strong> 과 유사한 방식으로 동작한다.</p>
<details>
  <summary> reweighted variant of the variational lower bound란? </summary>
<ul>
<li>학습 효율성을 높이기 위해 특정 $t$ 단계(예: 중간 정도의 노이즈)에 <code>noise schedule</code>을 조절하여 <code>더 높은 가중치</code>를 부여하는 방식이다.</li>
<li>난 <code>노이즈 스케줄 조정 = 특정 스텝 가중치 조정 = Reweighting</code>으로 이해했다.</li>
</ul>
</details>
</li>
</ol>
<br>
<ol start="30">
<li>본 논문은 이 <code>DM</code>을 <code>디노이징 오토인코더(denoising autoencoders)</code> $\epsilon_\theta(x_t, t)$들의 <strong>동일한 가중치를 가진 연속적인 집합</strong>으로 볼 수 있다고 설명한다. 아래는 이런 DM의 <code>간단한 목적식</code>을 나타낸 것이다.</li>
</ol>
<ul>
<li>이때 이 <code>오토인코더</code>는 노이즈가 포함된 $x_t$에 대해 디노이징된 $x_\text{t-1}$을 예측하도록 학습된다.</li>
</ul>
<p>$$L_\text{DM} = \mathbb{E_{x, \epsilon \sim \mathcal{N}(0,1), t}} \left[ \left| \epsilon - \epsilon_ {\theta}(x_t, t) \right|_2^2 \right]$$</p>
<br>
<ol start="31">
<li><code>본 논문</code>에선 <code>perceptual compression 모델</code>인 <code>AE</code>를 사용하여 효율적인 저차원 <code>latent space</code>를 얻게 된다.</li>
</ol>
<ul>
<li>반복적인 얘기지만, <code>AE</code>를 사용하면 pixel-based와 비교했을 때 데이터의 중요한 <code>semantic</code>적인 요소에 집중할 수 있다.</li>
<li>또한 저차원의 공간을 사용하면서, <strong>계산 효율성이 증대된다.</strong></li>
</ul>
<br>
<ol start="32">
<li>기존에 <code>latent space</code>에서 <code>autoregressive</code> 및 <code>transformer</code> 기반의 모델을 사용한 <strong>반면</strong>, <code>본 논문</code>은 <code>UNet</code>구조로 2D Conv 구조를 중심으로 가져간다.</li>
</ol>
<br>
<ol start="33">
<li><code>UNet 구조</code>를 채택하여 이미지 특화된 <code>inductive bias</code>를 활용할 수 있다. 또한 이를 활용하여 <code>재가중치된 변분 하한</code>을 사용하여 지각적으로 중요한 정보에 집중할 수 있게 된다.</li>
</ol>
<br>
<ol start="34">
<li>따라서 아래는 본 논문에서 제안하고 있는 <code>목적식</code>을 나타낸 것이다.</li>
</ol>
<p>$$L_{LDM} := \mathbb{E_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1), t} }\left[ \left| \epsilon - \epsilon_{\theta}(z_t, t) \right|_2^2 \right].$$</p>
<ul>
<li>기존 <code>DM 목적식</code>과 다른 것은 인코더 $\mathcal{E}(x)$ 를 통해 나온 $z_t$가 denoising 모델인 $\epsilon_{\theta}(z_t, t)$의 input으로 들어간다는 점이다.</li>
</ul>
<br>
<div style="text-align:center;">
<img src="/images/paper/ldm/model-2.png" height="70%" width="50%"> </div>
<ol start="35">
<li><code>위 사진</code>은 본 논문의 전체적인 구조를 나타내고 있다.</li>
</ol>
<br>
<ol start="36">
<li><code>forward pass</code>는 <strong>고정되어</strong> 있기 때문에 학습 시 $z_t$ 를 <strong>효율적으로</strong> 얻을 수 있고, <code>샘플링 시</code> 에도 $p(z)$ 에서 샘플을 뽑아 디코더 $D$의 <code>single pass</code>로 이미지 복원이 가능하다.</li>
</ol>
<hr>
<h3 id="conditioning-mechanisms">Conditioning Mechanisms</h3>
<ol start="37">
<li>다른 생성 모델과 마찬가지로 <code>DM</code>은 조건부 확률 분포로 이루어질 수 있다. ($p(z|y)$)</li>
</ol>
<br>
<ol start="38">
<li>이를 구현하는 방법은 <code>conditional denosing autoencoder</code> $\epsilon_\theta(z_t, t, y)$를 사용하는 것이다.</li>
</ol>
<ul>
<li>이를 통해 입력 $y$가 다양한 task에 사용될 수 있게 된다.</li>
</ul>
<br>
<ol start="39">
<li><code>본 논문</code>에서는 <code>conditional DM</code>을 만들기 위해 <code>Unet</code>에 <code>cross-attention mechanism</code>을 부착한 구조를 제안한다.</li>
</ol>
<br>
<ol start="40">
<li>그렇게 하기 위하여 <code>domain-specific encoder</code>($\tau_\theta$) 를 도입한다. $\tau_\theta(y)$는 <code>UNet</code>의 <code>중간 표현</code>의 <code>cross-attention layer</code>를 통해 연결 된다.</li>
</ol>
<br>
<ol start="41">
<li>아래는 <code>cross-attention</code>에 condition을 적용하는 식이다.</li>
</ol>
<p>$$
\text{Attention}(Q,K,V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d}} \right) \cdot V \\
\textbf{쿼리(Query)}: ( Q = W_Q^{(i)} \cdot \varphi_i(z_t) ) \\
\textbf{키(Key)}: ( K = W_K^{(i)} \cdot \tau_{\theta}(y) ) \\
\textbf{값(Value)}: ( V = W_V^{(i)} \cdot \tau_{\theta}(y) )
$$</p>
<ul>
<li>$ \varphi_i(z_t) \in \mathbb{R}^{N \times d_{\epsilon}^{i}}$ 는 <code>UNet</code>에서 <strong>Flatten된 중간 표현을 의미한다.</strong>
<ul>
<li>더 쉽게 설명하자면, 기존의 <code>DM</code>에서 시간 $t$에서 추출된 표현($z_t)$을 <code>attention mechanism</code>에 <strong>적용</strong>하기 위하여 <code>Flatten</code>한 것이다.
<details>
  <summary> Query에만 $\varphi_i$가 곱해지는 이유? </summary>
- `Cross-attention`에서 `쿼리(Q)`는 모델이 주어진 입력을 기반으로 **어텐션을 적용할 요소를 결정하는 역할**이다.
- 즉, `쿼리(Q)`는 `DM`이 현재 생성 중인 **이미지의 정보를 포함하는 특징 맵(feature map)**이 되기 때문에 쿼리에 $\varphi_i$를 곱하는 것이다.
</details>
</li>
</ul>
</li>
</ul>
<br>
<ol start="42">
<li>아래는 <code>본 논문</code>에서 제안하는 <code>LDM</code>의 최종 목적식이다.</li>
</ol>
<p>$$L_{LDM} := \mathbb{E_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0,1), t} }\left[ \left| \epsilon - \epsilon_{\theta}(z_t, t, \tau_\theta(y)) \right|_2^2 \right].$$</p>
<ul>
<li>기존 모델에 <code>condition</code>을 부여할 수 있게 $y$와 학습 가능한 $\tau_\theta$가 추가되어 있는 것을 볼 수 있다.</li>
</ul>
<hr>
<h2 id="experiments">Experiments</h2>
<div style="text-align:center;">
<img src="/images/paper/ldm/result-1.png" height="70%" width="50%"> </div>
<ol start="43">
<li><code>위 그래프</code>는 <code>ImageNet</code>을 학습 할 때 step에 대한 <code>샘플 품질(FID, IS)</code>를 나타낸다.</li>
</ol>
<ul>
<li>
<p>작은 downsampling factor는 학습을 느리게 하는 것을 확인 할 수 있다.</p>
</li>
<li>
<p>큰 factor는 비교적 적은 step에서 샘플 품질이 정체가 되는 것을 확인 할 수 있다.</p>
<details>
  <summary> FID Score란? </summary>
- `FID(Fréchet Inception Distance) Score`는 `생성된 이미지`와 `실제 이미지` 간의 **품질 차이**를 측정하는 평가 지표이다.
- `실제 데이터셋`과 `생성된 이미지`의 `특징 벡터`를 추출하여, 두 분포의 `평균`과 `분산`을 계산하고 두 정규 분포 간의 **Fréchet Distance (Wasserstein-2 거리)**를 구한다.
- `FID Score` **값이 낮을수록** 더 좋은 성능을 의미한다.
- 
</details>
</li>
</ul>
<br>
<div style="text-align:center;">
<img src="/images/paper/ldm/result-2.png" height="70%" width="50%"> </div>
<ol start="44">
<li><code>위 그래프</code>는 <code>CelebA-HQ</code>와 <code>ImageNet</code>으로 학습한 LDM의 <code>샘플링 속도</code>와 <code>FID</code>를 비교한 그래프이다.</li>
</ol>
<ul>
<li><code>LDM-4</code>, <code>LDM-8</code>이 성능이 제일 좋다는 것을 알 수 있게 된다.</li>
</ul>
<br>
<ol start="45">
<li><code>LDM</code>은 기존 <code>DM</code>, <code>생성 모델</code>들 대비 좋은 성능을 내보였으며 <code>여러 Task</code>에서도 좋은 성능을 보였다.</li>
</ol>
<hr>
<h2 id="limitation">Limitation</h2>
<ol start="46">
<li><code>LDM</code>은 <code>Pixel-based 방법</code>과 비교하여 <code>계산량</code>을 줄일 수 있지만 <code>샘플링 과정</code>이 순차적으로 이루어지기 때문에 <code>GAN</code>보다는 속도가 느리다.</li>
</ol>
<br>
<ol start="47">
<li>또한 <code>High Precision</code>을 원할 경우 <code>LDM</code>의 활용이 제한될 수 있다.</li>
</ol>
<ul>
<li>본 논문에서 제안한 $f = 4$인 <code>AE</code>의 경우에 loss가 매우 작다.</li>
<li>하지만 <code>픽셀 수준</code>에서의 <code>정밀한 복원</code>이 필요한 작업에서는 <code>재구성 성능(reconstruction capability)</code>이 <code>병목(bottleneck)</code>이 될 수도 있다.</li>
</ul>
<br>
<ol start="48">
<li>또한 논문에서는 생성 모델에 대한 문제점에 대해서 설명하고 있다.</li>
</ol>
<ul>
<li>조작된 데이터의 생성을 할 수 있는 <code>사회적 영향 문제</code></li>
<li>학습 데이터 유출 가능성이 있는 <code>데이터 프라이버시 문제</code></li>
<li><code>DM</code>은 데이터 분포를 그대로 학습하기 때문에 학습 데이터에서 분포가 충분하지 않거나 편향되어있다면 생성 데이터에 대한 정당성 또한 편향적일 수 있는 <code>편향(bias)문제</code></li>
</ul>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener noreffer ">https://arxiv.org/abs/2112.10752</a></li>
<li><a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/ldm/" target="_blank" rel="noopener noreffer ">https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/ldm/</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-02-17</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/paper/ldm/" data-title="[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)" data-hashtags="논문 리뷰,computer vision,Diffusion,LDM"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/paper/ldm/" data-hashtag="논문 리뷰"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/paper/ldm/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/paper/ldm/" data-title="[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/paper/ldm/" data-title="[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/paper/ldm/" data-title="[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/paper/ldm/" data-title="[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/diffusion/">Diffusion</a>,&nbsp;<a href="/tags/ldm/">LDM</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/paper/ddim/" class="prev" rel="prev" title="[Paper Review]Denoising Diffusion Implicit Models(DDIM)"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[Paper Review]Denoising Diffusion Implicit Models(DDIM)</a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
