<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[CS236] 9. GANs - 1 - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[CS236] 9. GANs - 1" />
<meta property="og:description" content="개요 이번 포스트에서는 CS236 강의의 8강을 설명한다. 우리는 지금까지 여러 생성 모델들을 배웠다. AutoRegreesive Model, Variational Autoencoder, Normalizing Flow. 이들은 모두 실제 데이터 분포 $p_\text{data}$ 에 제일 가까운 $p_\theta$ 를 찾으려고 했다. 가장 가까운 $\theta$ 를 찾기 위하여 Maximum log-likelihood(MLE)를 학습 목표로 삼았다. 그렇다면, 높은 log-likelihood가 무조건 좋은 품질의 생성을 의미할까? 아니다. likelihood가 낮더라도 sampling의 품질은 꽤 좋을 수 있다. 그래서 이번 lecture에서는 MLE에 기반하지 않는 다양한 종류의 훈련 목적 함수를 알아볼 것이다. 높은 likelihood -&gt; 나쁜 품질은 어떻게 하면 될까? 자세한 내용은 아래 toggle을 확인하면 된다.
높은 likelihood -&gt; 나쁜 품질 예시 아래의 슬라이드를 보면, 99퍼센트의 noise를 추출하는 $p_\theta$ 이지만 높은 차원으로 갈 수록이 모델은 실제 $p_\text{data}$ 를 나타내는 예시를 볼 수 있다. 그렇게 되면 이 모델은 MLE는 매우 좋을 것이며, 샘플의 품질은 매우 나쁠 것이다. 그렇다면 낮은 likelihood -&gt; 좋은 품질은 어떻게 하면 될까? 그 방법은 모델을 overfitting을 하면 된다.
결론적으로 기존과 달리, 우리는 위 그림의 $d(p_\text{data}||p_\theta)$ 의 다른 대안을 알아볼 것이다. Two-Sample Test 그렇다면 어떤 다른 방법이 있을까. 바로 두 개의 분포에서 생성한 결과를 가지고 그 생성한 결과가 서로 같다면 귀무가설(두 분포가 같다)을 받아드리고, 다르다면 기각하는 방식으로 MLE 없이 두 개의 분포 유사도를 측정할 수 있는 Two-Sample Test 방식이 있다. 두 분포가 같음을 측정할 때 만약 두 분포의 평균만 사용한다면, 우리는 분포의 확률조차도 구할 필요가 없는 것이다.
하지만, 통계적으로 고차원의 데이터에서 단지 평균만으로 측정한다면, 올바른 측정이 어렵다. 위와 같이 생각해야할 것들이 많기 때문이다. (단지, 평균만 같아도 두 분포가 같다고 할 수 없는 것(왼쪽 첫번째 그림) 처럼)
그렇다면 자동적으로 이 두 분포의 차이를 어떻게 알까? 방법은 분류기를 학습하는 것이다. (GAN에서 Discriminator 역할) 본질적으로는, 우리가 딥러닝 분류기의 역할은 두(여러) 그룹의 샘플을 구별하고 구분할 수 있는 특징을 자동적으로 분류하는 것이다. 이를 Two-Sample Test에 적용시키겠다는 것이다. Generative Adversarial Network (GAN) Discriminator 1일 때 real, 0일 때 fake 를 구별하는 2진 분류기가 있다고 하자. 그렇게 되면 우리가 사용할 통계량은 이 분류기의 loss일 것이다.(loss가 적으면 잘 구별한다는 것이고, 높으면 구별하기 어렵다는 것으로 해석할 수 있다.) 땨라서 이 분류기의 목표는 이 통계량을 최대화하거나 loss를 최소화하는 것이다. $$ \begin{aligned} \max_{D_\phi} V(p_\theta, D_\phi) &amp;= \mathbb{E_{x \sim p_{\text{data}}}}[\log D_\phi(x)] &#43; \mathbb{E_{x \sim p_\theta}}[\log(1 - D_\phi(x))] \\ &amp;\approx \sum_{x \in S_1} \log D_\phi(x) &#43; \sum_{x \in S_2} \log(1 - D_\phi(x)) \end{aligned} $$
$p_\theta$: Fixed generative model $p_\text{data}$: 데이터 셋 $D_\phi(x)$: Discriminator 이것은 고정된 생성 모델이 있을 때, 분류기의 목적 함수이다. 따라서 오직 분류기의 최적화 관점만 생각해야한다. 이 분류기는 $S1$에 대해서 1(real, 진짜로 인식)로 , $S2$에 대해서 0(fake, 가짜로 인식)로 잘 분류할 수 있게 학습하도록 한다. 이렇기 때문에 $D_\phi(x)$ 의 값은 샘플 $x$ 가 실제 데이터 분포에 속할 확률을 나타내는 것으로 해석할 수 있다. (데이터 분포와 유사하면 1, 아니면 0이기 때문이다.) $$ D_\theta^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) &#43; p_\theta(x)} $$
그래서 위의 Discriminator 식을 최적화 한다면 위와 같은 식으로 표현할 수 있는데, 이는 x가 Discriminator에 들어왔을 때 전체 분포의 확률 중에 실제 데이터 분포일 확률을 나타낸다. 따라서, 만약 $p_\text{data} = p_\theta$ 라면($p_\text{data}$와 $p_\theta$의 분포가 같다면) 값은 1/2이 나올 것이다. Generator 그렇다면 Discriminator를 속이기 위해 Generator ($p_\theta$)를 최적화 하는 방법을 정의해보자. Flow-model 처럼 유사하게 동작하지만, 역변환은 필요 없다. 쉬운 분포 p(z)에서 z를 뽑고 $G_\theta$ 에 넣어 x를 생성하는 것에 초점이 맞춰져 있다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/cs236/lecture9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-08-27T19:15:17+09:00" />
<meta property="article:modified_time" content="2025-08-27T19:15:17+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[CS236] 9. GANs - 1"/>
<meta name="twitter:description" content="개요 이번 포스트에서는 CS236 강의의 8강을 설명한다. 우리는 지금까지 여러 생성 모델들을 배웠다. AutoRegreesive Model, Variational Autoencoder, Normalizing Flow. 이들은 모두 실제 데이터 분포 $p_\text{data}$ 에 제일 가까운 $p_\theta$ 를 찾으려고 했다. 가장 가까운 $\theta$ 를 찾기 위하여 Maximum log-likelihood(MLE)를 학습 목표로 삼았다. 그렇다면, 높은 log-likelihood가 무조건 좋은 품질의 생성을 의미할까? 아니다. likelihood가 낮더라도 sampling의 품질은 꽤 좋을 수 있다. 그래서 이번 lecture에서는 MLE에 기반하지 않는 다양한 종류의 훈련 목적 함수를 알아볼 것이다. 높은 likelihood -&gt; 나쁜 품질은 어떻게 하면 될까? 자세한 내용은 아래 toggle을 확인하면 된다.
높은 likelihood -&gt; 나쁜 품질 예시 아래의 슬라이드를 보면, 99퍼센트의 noise를 추출하는 $p_\theta$ 이지만 높은 차원으로 갈 수록이 모델은 실제 $p_\text{data}$ 를 나타내는 예시를 볼 수 있다. 그렇게 되면 이 모델은 MLE는 매우 좋을 것이며, 샘플의 품질은 매우 나쁠 것이다. 그렇다면 낮은 likelihood -&gt; 좋은 품질은 어떻게 하면 될까? 그 방법은 모델을 overfitting을 하면 된다.
결론적으로 기존과 달리, 우리는 위 그림의 $d(p_\text{data}||p_\theta)$ 의 다른 대안을 알아볼 것이다. Two-Sample Test 그렇다면 어떤 다른 방법이 있을까. 바로 두 개의 분포에서 생성한 결과를 가지고 그 생성한 결과가 서로 같다면 귀무가설(두 분포가 같다)을 받아드리고, 다르다면 기각하는 방식으로 MLE 없이 두 개의 분포 유사도를 측정할 수 있는 Two-Sample Test 방식이 있다. 두 분포가 같음을 측정할 때 만약 두 분포의 평균만 사용한다면, 우리는 분포의 확률조차도 구할 필요가 없는 것이다.
하지만, 통계적으로 고차원의 데이터에서 단지 평균만으로 측정한다면, 올바른 측정이 어렵다. 위와 같이 생각해야할 것들이 많기 때문이다. (단지, 평균만 같아도 두 분포가 같다고 할 수 없는 것(왼쪽 첫번째 그림) 처럼)
그렇다면 자동적으로 이 두 분포의 차이를 어떻게 알까? 방법은 분류기를 학습하는 것이다. (GAN에서 Discriminator 역할) 본질적으로는, 우리가 딥러닝 분류기의 역할은 두(여러) 그룹의 샘플을 구별하고 구분할 수 있는 특징을 자동적으로 분류하는 것이다. 이를 Two-Sample Test에 적용시키겠다는 것이다. Generative Adversarial Network (GAN) Discriminator 1일 때 real, 0일 때 fake 를 구별하는 2진 분류기가 있다고 하자. 그렇게 되면 우리가 사용할 통계량은 이 분류기의 loss일 것이다.(loss가 적으면 잘 구별한다는 것이고, 높으면 구별하기 어렵다는 것으로 해석할 수 있다.) 땨라서 이 분류기의 목표는 이 통계량을 최대화하거나 loss를 최소화하는 것이다. $$ \begin{aligned} \max_{D_\phi} V(p_\theta, D_\phi) &amp;= \mathbb{E_{x \sim p_{\text{data}}}}[\log D_\phi(x)] &#43; \mathbb{E_{x \sim p_\theta}}[\log(1 - D_\phi(x))] \\ &amp;\approx \sum_{x \in S_1} \log D_\phi(x) &#43; \sum_{x \in S_2} \log(1 - D_\phi(x)) \end{aligned} $$
$p_\theta$: Fixed generative model $p_\text{data}$: 데이터 셋 $D_\phi(x)$: Discriminator 이것은 고정된 생성 모델이 있을 때, 분류기의 목적 함수이다. 따라서 오직 분류기의 최적화 관점만 생각해야한다. 이 분류기는 $S1$에 대해서 1(real, 진짜로 인식)로 , $S2$에 대해서 0(fake, 가짜로 인식)로 잘 분류할 수 있게 학습하도록 한다. 이렇기 때문에 $D_\phi(x)$ 의 값은 샘플 $x$ 가 실제 데이터 분포에 속할 확률을 나타내는 것으로 해석할 수 있다. (데이터 분포와 유사하면 1, 아니면 0이기 때문이다.) $$ D_\theta^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) &#43; p_\theta(x)} $$
그래서 위의 Discriminator 식을 최적화 한다면 위와 같은 식으로 표현할 수 있는데, 이는 x가 Discriminator에 들어왔을 때 전체 분포의 확률 중에 실제 데이터 분포일 확률을 나타낸다. 따라서, 만약 $p_\text{data} = p_\theta$ 라면($p_\text{data}$와 $p_\theta$의 분포가 같다면) 값은 1/2이 나올 것이다. Generator 그렇다면 Discriminator를 속이기 위해 Generator ($p_\theta$)를 최적화 하는 방법을 정의해보자. Flow-model 처럼 유사하게 동작하지만, 역변환은 필요 없다. 쉬운 분포 p(z)에서 z를 뽑고 $G_\theta$ 에 넣어 x를 생성하는 것에 초점이 맞춰져 있다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/cs236/lecture9/" /><link rel="prev" href="https://goodyoung.github.io/posts/cs236/lecture8/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[CS236] 9. GANs - 1",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/cs236\/lecture9\/"
        },"genre": "posts","keywords": "GAN, CS236","wordcount":  1180 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/cs236\/lecture9\/","datePublished": "2025-08-27T19:15:17+09:00","dateModified": "2025-08-27T19:15:17+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[CS236] 9. GANs - 1</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a>&nbsp;<a href="/categories/deep-generative-models/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Generative Models</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-08-27">2025-08-27</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;1180 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;6 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#two-sample-test">Two-Sample Test</a></li>
    <li><a href="#generative-adversarial-network-gan">Generative Adversarial Network (GAN)</a>
      <ul>
        <li><a href="#discriminator">Discriminator</a></li>
        <li><a href="#generator">Generator</a></li>
        <li><a href="#gan-learning-objective">GAN Learning Objective</a></li>
        <li><a href="#challenges">Challenges</a></li>
      </ul>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- image format
<div style="text-align:center;">
<img src="/images/cs231n/lecture4/back-1.png" height="100%" width="80%"> </div>
 -->
<h2 id="개요">개요</h2>
<ol start="0">
<li>이번 포스트에서는 <code>CS236</code> 강의의 <code>8강</code>을 설명한다.</li>
</ol>
<br>
<ol>
<li>우리는 지금까지 여러 생성 모델들을 배웠다. <code>AutoRegreesive Model</code>, <code>Variational Autoencoder</code>, <code>Normalizing Flow</code>. 이들은 모두 실제 데이터 분포 $p_\text{data}$ 에 제일 가까운 $p_\theta$ 를 찾으려고 했다. 가장 가까운 $\theta$ 를 찾기 위하여 <code>Maximum log-likelihood(MLE)</code>를 학습 목표로 삼았다.</li>
</ol>
<br>
<ol start="2">
<li>그렇다면, 높은 log-likelihood가 무조건 좋은 품질의 생성을 의미할까? <strong>아니다.</strong> likelihood가 <strong>낮더라도</strong> sampling의 품질은 <strong>꽤 좋을 수 있다.</strong> 그래서 이번 lecture에서는 <code>MLE</code>에 기반하지 않는 다양한 종류의 훈련 목적 함수를 알아볼 것이다.
<ul>
<li>
<p><code>높은 likelihood -&gt; 나쁜 품질</code>은 어떻게 하면 될까? 자세한 내용은 아래 toggle을 확인하면 된다.</p>
<details>
<summary> 높은 likelihood -> 나쁜 품질 예시 </summary>
<ul>
<li>아래의 슬라이드를 보면, 99퍼센트의 noise를 추출하는 $p_\theta$ 이지만 <strong>높은 차원으로 갈 수록</strong>이 모델은 실제 $p_\text{data}$ 를 나타내는 예시를 볼 수 있다. 그렇게 되면 이 모델은 <strong>MLE는 매우 좋을 것</strong>이며, <strong>샘플의 품질은 매우 나쁠 것</strong>이다.</li>
</ul>
<div style="text-align:center;"> <img src="/images/cs236/lecture9/MLE.png" height="80%" width="80%"></div>
</details>
</li>
<li>
<p>그렇다면 <code>낮은 likelihood -&gt; 좋은 품질</code>은 어떻게 하면 될까? 그 방법은 모델을 <strong>overfitting을 하면 된다.</strong></p>
</li>
</ul>
</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture9/MLE-1.png" height="50%" width="50%"></div>
<ol start="3">
<li>결론적으로 기존과 달리, 우리는 위 그림의 $d(p_\text{data}||p_\theta)$ 의 <strong>다른 대안</strong>을 알아볼 것이다.</li>
</ol>
<hr>
<h2 id="two-sample-test">Two-Sample Test</h2>
<ol start="4">
<li>그렇다면 어떤 다른 방법이 있을까. 바로 두 개의 분포에서 생성한 결과를 가지고 그 생성한 결과가 서로 같다면 귀무가설(두 분포가 같다)을 받아드리고, 다르다면 기각하는 방식으로 <code>MLE</code> <strong>없이</strong> 두 개의 분포 유사도를 측정할 수 있는 <code>Two-Sample Test</code> 방식이 있다.
<ul>
<li>
<p>두 분포가 같음을 측정할 때 만약 두 분포의 평균만 사용한다면, 우리는 분포의 <strong>확률조차도 구할 필요가 없는 것이다.</strong></p>
<img src="/images/cs236/lecture9/two-sample.png" height="100%" width="100%">
</li>
<li>
<p>하지만, 통계적으로 고차원의 데이터에서 <strong>단지 평균만으로 측정</strong>한다면, 올바른 측정이 어렵다. 위와 같이 생각해야할 것들이 많기 때문이다. (단지, <strong>평균만 같아도 두 분포가 같다고 할 수 없는 것</strong>(<code>왼쪽 첫번째 그림</code>) 처럼)</p>
</li>
</ul>
</li>
</ol>
<br>
<ol start="5">
<li>그렇다면 자동적으로 이 두 분포의 차이를 어떻게 알까? 방법은 <em>분류기</em>를 학습하는 것이다. (<code>GAN</code>에서 Discriminator 역할)
<ul>
<li>본질적으로는, 우리가 <code>딥러닝 분류기</code>의 역할은 두(여러) 그룹의 샘플을 구별하고 구분할 수 있는 <strong>특징을 자동적으로 분류</strong>하는 것이다. 이를 <code>Two-Sample Test</code>에 적용시키겠다는 것이다.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="generative-adversarial-network-gan">Generative Adversarial Network (GAN)</h2>
<h3 id="discriminator">Discriminator</h3>
<div style="text-align:center;">
<img src="/images/cs236/lecture9/discriminator.png" height="25%" width="25%"></div>
<ol start="6">
<li>1일 때 real, 0일 때 fake 를 구별하는 <strong>2진 분류기</strong>가 있다고 하자. 그렇게 되면 우리가 사용할 <code>통계량</code>은 <strong>이 분류기의 loss</strong>일 것이다.(loss가 적으면 잘 구별한다는 것이고, 높으면 구별하기 어렵다는 것으로 해석할 수 있다.)
땨라서 이 분류기의 목표는 <strong>이 통계량을 최대화하거나 loss를 최소화하는 것이다.</strong></li>
</ol>
<br>
<p>$$
\begin{aligned}
\max_{D_\phi} V(p_\theta, D_\phi)
&amp;= \mathbb{E_{x \sim p_{\text{data}}}}[\log D_\phi(x)] + \mathbb{E_{x \sim p_\theta}}[\log(1 - D_\phi(x))] \\
&amp;\approx \sum_{x \in S_1} \log D_\phi(x) + \sum_{x \in S_2} \log(1 - D_\phi(x))
\end{aligned}
$$</p>
<ul>
<li>$p_\theta$: Fixed generative model</li>
<li>$p_\text{data}$: 데이터 셋</li>
<li>$D_\phi(x)$: Discriminator</li>
</ul>
<br>
<ol start="7">
<li>이것은 고정된 생성 모델이 있을 때, 분류기의 목적 함수이다. 따라서 <strong>오직 분류기의 최적화 관점</strong>만 생각해야한다. 이 분류기는 $S1$에 대해서 1(<strong>real, 진짜로 인식</strong>)로 , $S2$에 대해서 0(<strong>fake, 가짜로 인식</strong>)로 잘 분류할 수 있게 학습하도록 한다. 이렇기 때문에 $D_\phi(x)$ 의 값은 샘플 $x$ 가 <strong>실제 데이터 분포에 속할 확률을 나타내는 것</strong>으로 해석할 수 있다. (데이터 분포와 유사하면 1, 아니면 0이기 때문이다.)</li>
</ol>
<br>
<p>$$ D_\theta^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_\theta(x)} $$</p>
<ol start="8">
<li>그래서 위의 <code>Discriminator 식</code>을 최적화 한다면 위와 같은 식으로 표현할 수 있는데, 이는 x가 <code>Discriminator</code>에 들어왔을 때 <strong>전체 분포의 확률 중에 실제 데이터 분포일 확률을 나타낸다.</strong> 따라서, 만약 $p_\text{data} = p_\theta$ 라면($p_\text{data}$와 $p_\theta$의 분포가 같다면) 값은 1/2이 나올 것이다.</li>
</ol>
<br>
<h3 id="generator">Generator</h3>
<div style="text-align:center;">
<img src="/images/cs236/lecture9/generator.png" height="25%" width="25%"></div>
<ol start="9">
<li>그렇다면 <code>Discriminator</code>를 속이기 위해 <code>Generator</code> ($p_\theta$)를 최적화 하는 방법을 정의해보자. <code>Flow-model</code> 처럼 유사하게 동작하지만, <strong>역변환은 필요 없다.</strong> 쉬운 분포 p(z)에서 z를 뽑고 $G_\theta$ 에 넣어 x를 생성하는 것에 초점이 맞춰져 있다. 또한 여기서 나온 $p_\theta(x)$ 의 <code>likelihood</code>도 <strong>계산할 필요가 없다.</strong> <code>Two-Sample Test</code>에서 모델을 비교하며 학습하기 때문에 신경 쓸 필요 없다.
(<code>VAE</code> 등의 다른 생성 모델은 $p(x,z)$를 <strong>학습하는데 초점이 맞춰져있다.</strong> )</li>
</ol>
<br>
<ol start="10">
<li>이 모델을 사용하여 얻은 <code>샘플 분포</code>와 <code>실제 데이터 분포</code>가 <strong>동일하다는 귀무 가설을 뒷받침하는 통계</strong>를 최소화 하려고 한다.
<ul>
<li><code>Discriminator</code>는 &ldquo;두 분포가 다르다&quot;는 <strong>증거(= test statistic)를 최대화하려고 한다.</strong></li>
<li><code>Generator</code>는 반대로 &ldquo;두 분포가 다르지 않다&quot;는 귀무가설을 성립시키고 싶기 때문에 그 <strong>증거를 최소화하려고 한다.</strong></li>
</ul>
</li>
</ol>
<br>
<h3 id="gan-learning-objective">GAN Learning Objective</h3>
<p>$$\min_G \max_D V(G, D) = \mathbb{E_{x \sim p_{\text{data}}}}[\log D(x)] + \mathbb{E_{x \sim p_G}}[\log(1 - D(x))]$$</p>
<ol start="11">
<li><code>위 식</code>은 최종 목적 식이다. <code>Discriminator</code>는 최대화 하는 방향으로 학습을 하고, <code>Generator</code>는 최소화 하는 방향으로 학습을 하는 것을 볼 수 있다. 위에서 나온 최적의 <code>Discriminator</code> ($D^*_{\theta}(x)$) 를 위의 최종 목적 식에 대입해보면 다음과 같다.</li>
</ol>
<br>
<p>$$
\begin{aligned}
V(G, D_G^*(x)) &amp;= \mathbb{E_{x \sim p_{\text{data}}}} \left[ \log \frac{p_{\text{data}}(x)}{p_{\text{data}}(x)+p_G(x)} \right] + \mathbb{E_{x \sim p_G}} \left[ \log \frac{p_G(x)}{p_{\text{data}}(x)+p_(x)} \right] \\
&amp;= \mathbb{E_{x \sim p_{\text{data}}}} \left[ \log \frac{p_{\text{data}}(x)}{\frac{p_{\text{data}}(x)+p_G(x)}{2}} \right] + \mathbb{E_{x \sim p_G}} \left[ \log \frac{p_G(x)}{\frac{p_{\text{data}}(x)+p_G(x)}{2}} \right] - \log 4 \\
&amp;= D_{\text{KL}}\left(p_{\text{data}} ;\middle|; \tfrac{p_{\text{data}} + p_G}{2}\right) + D_{\text{KL}}\left(p_G ;\middle|; \tfrac{p_{\text{data}} + p_G}{2}\right) - \log 4 \\
&amp;= 2 D_{\text{JSD}}\left[p_{\text{data}} \parallel p_G\right] - \log 4
\end{aligned}
$$</p>
<br>
<ol start="12">
<li>
<p>위의 긴 식을 차근 차근 풀어보겠다.</p>
<ul>
<li>
<p>2번째 줄에 <code>Normalizing</code>을 하기위하여 2로 나누고 $\log4$ 를 빼주었다.</p>
<ul>
<li>$p_\text{data}, p_G$ 는 각각 독립적인 확률분포이므로 2로 나누어 정규화를 해주어야 한다.</li>
</ul>
</li>
<li>
<p>그렇게 되면 3번째 줄에 <code>Expectation</code>을 <code>KL-Divergence 식</code>으로 표현할 수 있다.</p>
<ul>
<li>$D_{KL}(p \parallel q) = \sum_x p(x) \log \frac{p(x)}{q(x)}= \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q(x)}\right]$</li>
</ul>
</li>
<li>
<p>마지막으로, 4번째 줄에, <code>KL 식</code>을 <code>젠슨 - 섀넌 divergence(JSD)</code>으로 다시 표현해줄 수 있게 된다.</p>
</li>
<li>
<p>그럼 결국, <code>GAN</code>을 학습하는 것은 <code>실제 데이터 분포</code>와 <code>모델의 샘플링 분포</code>의 차이를 나타내는 <code>JSD</code>를 <strong>최적화 하는 문제와 동일해진다.</strong></p>
</li>
</ul>
<details>
<summary> 젠슨-섀넌 Divergence란? </summary>
$$D_{JSD}[p, q] = \frac{1}{2} \left( D_{KL}\!\left[p \,\bigg\|\, \tfrac{p+q}{2}\right] + D_{KL}\!\left[q \,\bigg\|\, \tfrac{p+q}{2}\right] \right)$$
<ul>
<li>보통 <code>데이터 분포</code>와 <code>모델 분포</code> 사이의 <strong>차이</strong>를 <code>젠슨 - 섀넌 divergence</code> (<strong>대칭적</strong> KL-divergence라고도 불린다.)</li>
<li>이 식은 몇가지 특징이 있다.
<ul>
<li>0보다 크다는 특징 (KL이 0보다 커서)</li>
<li>$D_{JSD}[p, q] = D_{JSD}[q, p]$, KL과는 <strong>다른 특징</strong>이다. (KL은 바뀌면 같지 않다.)</li>
<li>분포가 동일할 때만 <code>global optimum</code>을 달성한다.</li>
<li>따라서, 만약 두 분포가 같다면 $V(G^*, D_G^*(x)) = - \log4$ 일 것이다.</li>
</ul>
</li>
</ul>
</details>
</li>
</ol>
<br>
<ol start="13">
<li>따라서, 우리는 기존과 달리 <strong>KL을 사용하지 않고</strong>, <code>JSD</code>를 사용하여 <code>데이터 분포</code>와 <code>모델 분포</code> 를 비교하는 방식을 사용한다. 이 점이 <strong>다른 생성 모델과 차별점</strong>이라고 할 수 있다. 또한 모델 손실을 평가하고 최적화하는데, $p_\theta$ 의 <strong>likelihood는 필요 없고</strong> 오직 샘플만 필요하다. 그렇기 때문에 <code>GAN</code>은 <code>neural network</code> $G_\theta$ 의 <strong>구조에 많은 유연성</strong>이 있다. 왜냐하면 우리는 그저 랜덤 노이즈를 넣고 <strong>샘플링만</strong> 하면 되는 $G_\theta$ 를 구하면 되기 때문이다. 그래서 <code>single forward pass</code>이기 때문에, <strong>샘플링이 빠르다.</strong></li>
</ol>
<br>
<ol start="14">
<li>하지만 <code>GAN</code>은 문제점도 있다. 쉬워보이지만 현실에서 <strong>훈련되기 어렵고 까다롭다.</strong>(최소,최대 문제이기 때문에)</li>
</ol>
<br>
<ol start="15">
<li><code>GAN</code>의 전체 훈련 흐름을 한번 살펴보자.
<ul>
<li>
<p><strong>Step 1</strong>:  m개의 Minibatch 훈련 데이터, ($x^{(1)}, x^{(2)}, \ldots, x^{(m)} \sim \mathcal{D}$)</p>
</li>
<li>
<p><strong>Step 2</strong>: m개의 Noise 데이터, ($z^{(1)}, z^{(2)}, \ldots, z^{(m)} \sim p_z$)</p>
</li>
<li>
<p><strong>Step 3</strong>: Discriminator 파라미터 $\phi$ 최적화 by <strong>stochastic gradient ascent</strong>
$$\nabla_\phi V(G_\theta, D_\phi)
= \frac{1}{m} \nabla_\phi \sum_{i=1}^m
\Big[ \log D_\phi(x^{(i)}) + \log (1 - D_\phi(G_\theta(z^{(i)}))) \Big]$$</p>
<ul>
<li><code>Real, Fake</code>를 잘 분류하기 위한 훈련</li>
</ul>
</li>
<li>
<p><strong>Step 4</strong>: Generator 파라미터 $\theta$ 최적화 by <strong>stochastic gradient descent</strong> (첫번째 항은 필요 없음)
$$\nabla_\theta V(G_\theta, D_\phi)
= \frac{1}{m} \nabla_\theta \sum_{i=1}^m
\log \big(1 - D_\phi(G_\theta(z^{(i)}))\big)$$</p>
<ul>
<li><code>Discriminator</code>를 속이기 위한 훈련</li>
</ul>
</li>
<li>
<p><strong>Step 5</strong>: Epoch마다 반복</p>
</li>
</ul>
</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture9/gan.png" height="78%" width="78%">
</div>
<ol start="16">
<li><code>위 그림</code>은 전체 훈련 흐름을 진행하면서 x,z 벡터가 업데이트 되는 방향을 시각적으로 나타낸다.
<ul>
<li><code>a-&gt;b</code> 로 갈 떄 <strong>Discriminator를 최적화 되면서</strong> 왼쪽은 real일 확률이 높고, 오른쪽으로 갈수록 fake일 확률이 높다고 판단하며 학습이 된다.</li>
<li><code>b-&gt;c</code> 로 갈때는 <code>Generator</code>가 최적화 되면서 점점 <strong>실제 분포</strong>(Data Distribution)으로 <strong>가까워지는 방향으로</strong> 옮겨지는 것을 확인할 수 있다.</li>
<li><code>c-&gt;d</code>일 땐 <code>Discriminator</code>가 더이상 분류를 못하여 모든 데이터 포인트에서 <code>1/2</code>의 <strong>확률을 내뱉는 것</strong>을 볼 수 있다.</li>
</ul>
</li>
</ol>
<br>
<h3 id="challenges">Challenges</h3>
<ol start="17">
<li>이런 <code>GAN은</code> 특정 task에서 매우 성공적이었지만, <strong>몇가지 단점</strong>도 발견되었다.
<ul>
<li>
<p><strong>Unstable Optimization</strong></p>
<ul>
<li>훈련 중 <code>loss</code>를 보면, 매끄럽게 수렴하는 것이 아니라 <code>min,max</code> 문제이기 때문에 <strong>훈련 loss 그래프가 진동하는 현상</strong></li>
<li>또한 어느 특정 시점에서 멈춰야하는지에 대한 <strong>기준(criteria)가 없다.</strong></li>
<li>따라서 <code>GAN</code>은 훈련이 매우 어렵다고 평가받는다.</li>
</ul>
</li>
<li>
<p><strong>Mode Collapse</strong></p>
<img src="/images/cs236/lecture9/problem.png" height="78%" width="78%">
<ul>
<li><code>위 그림</code>을 보면 생성한 샘플링의 벡터 위치를 2차원으로 나타낸 것인데, target의 <code>모든 부분</code>을 감싸는 것이 아니라 훈련을 진행할 때, <strong>특정 부분만 보는 것을 확인할 수 있다.</strong>
<ul>
<li>이는, <code>Generator</code>가 <code>Discriminator</code>를 속이는 방향에 따라 최적화가 진행되는 특성에 내재한 문제라고 볼 수 있다.</li>
</ul>
</li>
<li>따라서 샘플링 시 <code>Generator</code>가 <code>Discriminator</code>를 속이지만, 특정 부분만 반복적으로 생성하는 <code>Mode Collapse 문제</code>가 있다.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://deepgenerativemodels.github.io/" target="_blank" rel="noopener noreffer ">https://deepgenerativemodels.github.io/</a></li>
<li><a href="https://www.youtube.com/watch?v=3Zv-gokhLu8&amp;list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&amp;index=9" target="_blank" rel="noopener noreffer ">https://www.youtube.com/watch?v=3Zv-gokhLu8&list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&index=9</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-08-27</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/cs236/lecture9/" data-title="[CS236] 9. GANs - 1" data-hashtags="GAN,CS236"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/cs236/lecture9/" data-hashtag="GAN"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/cs236/lecture9/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/cs236/lecture9/" data-title="[CS236] 9. GANs - 1"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/cs236/lecture9/" data-title="[CS236] 9. GANs - 1"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/cs236/lecture9/" data-title="[CS236] 9. GANs - 1"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/cs236/lecture9/" data-title="[CS236] 9. GANs - 1" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/gan/">GAN</a>,&nbsp;<a href="/tags/cs236/">CS236</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/cs236/lecture8/" class="prev" rel="prev" title="[CS236] 8. Normalizing Flows - 2"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[CS236] 8. Normalizing Flows - 2</a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
