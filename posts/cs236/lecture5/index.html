<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[CS236] 5. Latent Variable Models-1 - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[CS236] 5. Latent Variable Models-1" />
<meta property="og:description" content="개요 이번 포스트에서는 CS236 강의의 5강 내용을 정리한다. Autoregressive 구조는 장/단점을 지닌다. 장점은 likelihood를 평가하여 maximum likelihood를 구할 수 있어 훈련이 비교적 쉽다. 반면, 단점은 순서대로 생성하여 생성 시간이 오래걸린다. 그리고 비지도 학습을 사용하여 데이터의 특징을 추출하는 것이 명확하지 않다. 이 점이 잠재 변수 모델을 사용해서 해결할 수 있는 일 중 하나이다. 이번 챕터에서는 이 latent variable(잠재 변수)가 있을 때 생성 모델이 추론과 학습을 수행하는 방법에 대해서 설명할 것이다. Latent Variable Model 위 그림과 같이 사람 이미지와 같은 이미지 데이터 안에는 단순히 픽셀 데이터가 아니라 그 안에 성별, 눈 색깔 등의 여러 변동성 큰 정보들이 포함되어 있다. 하지만 이 모든 정보들을 annotated 하여 이용하기 쉽지 않다. 그렇기 떄문에 latent variable에 원본 데이터 상에 숨겨진 특징들이 존재하게 된다. 만약 특정한 특징을 나타내는 latent variable(z)을 사용하여 생성하게 되면, 우린 더욱 더 유연한 생성 모델을 얻을 수 있을 것 이다. 그래서 이 latent variable(z)을 반영하여 모델의 확률 분포 $p(x,z)$를 구하는 Latent Variable Model에 대해서 알아볼 것이다. 왼쪽 그림은 Latent Variable Model을 간략하게 나타낸 구조이고 오른쪽 그림은 베이지안 네트워크일 때를 나타낸다. 우리는 $x$와 $z$의 결합 분포인 $p(x,z) = p(z)p(x|z)$ 를 구하게 될 것이고 이것이 베이지안 네트워크로 가면 $p(x,z) = p(z_1)p(x|z_1)&#43;&hellip;&#43;p(z_k)p(x|z_k)$ 가 될 것이다. 이렇게 잠재변수 z를 포함한 모델링을 사용하면 x만을 활용한 것보다 쉽다. 또한 만약 z의 특징을 각각 추출할 수 있다면, 그것을 이용해서 다른 종류의 task에도 이용이 가능하다. (ex)eyeColor = Blue만 식별 가능) 하지만 현실적으로 모든 z중에 특정 z만 따로 추출하는 것이 어렵다는 것이다.(이 말은 위에서 현실적으로 모든 z의 확률을 구할 수 없어서 베이지안 네트워크를 사용하기 어려울 수 있다는 말과 동일하다.) 따라서 우리는 이 문제를 해결하기 위하여 단순한 z(가우시안) 만을 가정하여 deep neural network(모든 각각의 관계의 정보를 따로따로 확인하지 않아도 모든 정보를 고려해줄 수 있다.) 를 사용하여 이 z를 예측하려고 한다. $z \sim \mathcal{N}(0,1)$ $p(x|z) = \mathcal{N}(\mu_\theta(z), \Sigma_\theta(z))$ 이 문제는 비지도학습의 representation learning이기 때문애 학습 시 z에 대해서 잘 학습(z의 특징을 잘 추출)되기를 희망하게 된다. (이 모델이 어떤 z를 추출할지 명확하지 않다.) 만약 학습이 잘된다면, z의 분포를 클러스터링 하여 우리가 원하는 클러스터에 해당하는 z의 값($p(z)$)을 구할 수 있고 이 z를 가지고 새로운 데이터 $p_\theta(x,z)$를 만들어낼 수 있게 된다. 다음 챕터에서 이에 대해 더 자세히 설명할 것이다. Mixture of Gaussians (GMM,VAE) 그렇다면 z를 어떻게 예측 하는 방법에 대해서 자세히 알아볼 필요가 있다. 데이터의 분포를 확인하면 위 그림과 같을 것이다. 그 후, $K$개의 가우시안 분포(혼합 성분)를 나눌 수 있다. 그럼 생성 과정에선 K개 중 하나를 선택하여 그 K에 해당하는 평균, 공분산을 따르는 가우시안 분포에서 x를 샘플링한다. (이는 VAE에서 생성 과정에서도 동일하다.) $\mathbf{z} \sim \text{Categorical}(1, \cdots, K)$ $\mathbf{x} \sim p(\mathbf{x} \mid \mathbf{z} = k) = \mathcal{N}(\mu_k, \Sigma_k)$ (likelihood) 이 방법은 생성 과정 뿐 아니라, x가 주어졌을 때 어떤 z에 속하는지 맞출 수 있는 추론 과정에도 (posterior $p(z|x)$) 사용할 수 있다. (clustering, unsupervised representation learning) 또한 Mixture of Gaussians의 전체 확률 ($p(x)$)도 구할 수 있는데 다음과 같은 식으로 가능하다. $p(\mathbf{x}) = \sum_{z} p(\mathbf{x}, z) = \sum_{z} p(z) \cdot p(\mathbf{x} \mid z) = \sum_{k=1}^{K} p(z=k) \cdot \mathcal{N}(\mathbf{x}; \mu_k, \Sigma_k)$ 이때 얻을 수 있는 직관은 simple한 $p(x|z)$를 활용하여 복잡한 혼합 모델($p(x)$)을 구할 수 있다는 것이다. 지금까진 z를 어떻게 예측을 할지에 대해서 알아보았다. 이 Mixture of Gaussians에서는 Neural Network(NN)를 사용하지 않아 z의 수가 많아지면 계산하기 어려운 단점이 있다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/cs236/lecture5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-07-14T17:24:25+09:00" />
<meta property="article:modified_time" content="2025-07-14T17:24:25+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[CS236] 5. Latent Variable Models-1"/>
<meta name="twitter:description" content="개요 이번 포스트에서는 CS236 강의의 5강 내용을 정리한다. Autoregressive 구조는 장/단점을 지닌다. 장점은 likelihood를 평가하여 maximum likelihood를 구할 수 있어 훈련이 비교적 쉽다. 반면, 단점은 순서대로 생성하여 생성 시간이 오래걸린다. 그리고 비지도 학습을 사용하여 데이터의 특징을 추출하는 것이 명확하지 않다. 이 점이 잠재 변수 모델을 사용해서 해결할 수 있는 일 중 하나이다. 이번 챕터에서는 이 latent variable(잠재 변수)가 있을 때 생성 모델이 추론과 학습을 수행하는 방법에 대해서 설명할 것이다. Latent Variable Model 위 그림과 같이 사람 이미지와 같은 이미지 데이터 안에는 단순히 픽셀 데이터가 아니라 그 안에 성별, 눈 색깔 등의 여러 변동성 큰 정보들이 포함되어 있다. 하지만 이 모든 정보들을 annotated 하여 이용하기 쉽지 않다. 그렇기 떄문에 latent variable에 원본 데이터 상에 숨겨진 특징들이 존재하게 된다. 만약 특정한 특징을 나타내는 latent variable(z)을 사용하여 생성하게 되면, 우린 더욱 더 유연한 생성 모델을 얻을 수 있을 것 이다. 그래서 이 latent variable(z)을 반영하여 모델의 확률 분포 $p(x,z)$를 구하는 Latent Variable Model에 대해서 알아볼 것이다. 왼쪽 그림은 Latent Variable Model을 간략하게 나타낸 구조이고 오른쪽 그림은 베이지안 네트워크일 때를 나타낸다. 우리는 $x$와 $z$의 결합 분포인 $p(x,z) = p(z)p(x|z)$ 를 구하게 될 것이고 이것이 베이지안 네트워크로 가면 $p(x,z) = p(z_1)p(x|z_1)&#43;&hellip;&#43;p(z_k)p(x|z_k)$ 가 될 것이다. 이렇게 잠재변수 z를 포함한 모델링을 사용하면 x만을 활용한 것보다 쉽다. 또한 만약 z의 특징을 각각 추출할 수 있다면, 그것을 이용해서 다른 종류의 task에도 이용이 가능하다. (ex)eyeColor = Blue만 식별 가능) 하지만 현실적으로 모든 z중에 특정 z만 따로 추출하는 것이 어렵다는 것이다.(이 말은 위에서 현실적으로 모든 z의 확률을 구할 수 없어서 베이지안 네트워크를 사용하기 어려울 수 있다는 말과 동일하다.) 따라서 우리는 이 문제를 해결하기 위하여 단순한 z(가우시안) 만을 가정하여 deep neural network(모든 각각의 관계의 정보를 따로따로 확인하지 않아도 모든 정보를 고려해줄 수 있다.) 를 사용하여 이 z를 예측하려고 한다. $z \sim \mathcal{N}(0,1)$ $p(x|z) = \mathcal{N}(\mu_\theta(z), \Sigma_\theta(z))$ 이 문제는 비지도학습의 representation learning이기 때문애 학습 시 z에 대해서 잘 학습(z의 특징을 잘 추출)되기를 희망하게 된다. (이 모델이 어떤 z를 추출할지 명확하지 않다.) 만약 학습이 잘된다면, z의 분포를 클러스터링 하여 우리가 원하는 클러스터에 해당하는 z의 값($p(z)$)을 구할 수 있고 이 z를 가지고 새로운 데이터 $p_\theta(x,z)$를 만들어낼 수 있게 된다. 다음 챕터에서 이에 대해 더 자세히 설명할 것이다. Mixture of Gaussians (GMM,VAE) 그렇다면 z를 어떻게 예측 하는 방법에 대해서 자세히 알아볼 필요가 있다. 데이터의 분포를 확인하면 위 그림과 같을 것이다. 그 후, $K$개의 가우시안 분포(혼합 성분)를 나눌 수 있다. 그럼 생성 과정에선 K개 중 하나를 선택하여 그 K에 해당하는 평균, 공분산을 따르는 가우시안 분포에서 x를 샘플링한다. (이는 VAE에서 생성 과정에서도 동일하다.) $\mathbf{z} \sim \text{Categorical}(1, \cdots, K)$ $\mathbf{x} \sim p(\mathbf{x} \mid \mathbf{z} = k) = \mathcal{N}(\mu_k, \Sigma_k)$ (likelihood) 이 방법은 생성 과정 뿐 아니라, x가 주어졌을 때 어떤 z에 속하는지 맞출 수 있는 추론 과정에도 (posterior $p(z|x)$) 사용할 수 있다. (clustering, unsupervised representation learning) 또한 Mixture of Gaussians의 전체 확률 ($p(x)$)도 구할 수 있는데 다음과 같은 식으로 가능하다. $p(\mathbf{x}) = \sum_{z} p(\mathbf{x}, z) = \sum_{z} p(z) \cdot p(\mathbf{x} \mid z) = \sum_{k=1}^{K} p(z=k) \cdot \mathcal{N}(\mathbf{x}; \mu_k, \Sigma_k)$ 이때 얻을 수 있는 직관은 simple한 $p(x|z)$를 활용하여 복잡한 혼합 모델($p(x)$)을 구할 수 있다는 것이다. 지금까진 z를 어떻게 예측을 할지에 대해서 알아보았다. 이 Mixture of Gaussians에서는 Neural Network(NN)를 사용하지 않아 z의 수가 많아지면 계산하기 어려운 단점이 있다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/cs236/lecture5/" /><link rel="prev" href="https://goodyoung.github.io/posts/cs236/lecture4/" /><link rel="next" href="https://goodyoung.github.io/posts/cs236/lecture6/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[CS236] 5. Latent Variable Models-1",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/cs236\/lecture5\/"
        },"genre": "posts","keywords": "Latent Variable Models, VAE, CS236","wordcount":  1485 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/cs236\/lecture5\/","datePublished": "2025-07-14T17:24:25+09:00","dateModified": "2025-07-14T17:24:25+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[CS236] 5. Latent Variable Models-1</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a>&nbsp;<a href="/categories/deep-generative-models/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Generative Models</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-07-14">2025-07-14</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;1485 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;7 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#latent-variable-model">Latent Variable Model</a></li>
    <li><a href="#mixture-of-gaussians-gmmvae">Mixture of Gaussians (GMM,VAE)</a></li>
    <li><a href="#maximum-marginal-likelihood">Maximum Marginal Likelihood</a></li>
    <li><a href="#naive-monte-carlo">Naive Monte Carlo</a></li>
    <li><a href="#importance-sampling">Importance Sampling</a></li>
    <li><a href="#evidence-lower-boundelbo---1">Evidence Lower Bound(ELBO) - 1</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- image format
<div style="text-align:center;">
<img src="/images/cs231n/lecture4/back-1.png" height="100%" width="80%"> </div>
 -->
<h2 id="개요">개요</h2>
<ol start="0">
<li>이번 포스트에서는 <code>CS236</code> 강의의 5강 내용을 정리한다.</li>
</ol>
<br>
<ol>
<li><code>Autoregressive</code> 구조는 장/단점을 지닌다. 장점은 likelihood를 평가하여 <code>maximum likelihood</code>를 구할 수 있어 훈련이 비교적 쉽다.</li>
</ol>
<br>
<ol start="2">
<li>반면, 단점은 순서대로 생성하여 <strong>생성 시간이 오래걸린다.</strong> 그리고 <code>비지도 학습</code>을 사용하여 데이터의 특징을 추출하는 것이 명확하지 않다. 이 점이 <code>잠재 변수 모델</code>을 사용해서 해결할 수 있는 일 중 하나이다.</li>
</ol>
<br>
<ol start="3">
<li>이번 챕터에서는 이 <code>latent variable(잠재 변수)</code>가 있을 때 생성 모델이 추론과 학습을 수행하는 방법에 대해서 설명할 것이다.</li>
</ol>
<hr>
<h2 id="latent-variable-model">Latent Variable Model</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture5/people.png" height="85%" width="80%"> </div>
<ol start="4">
<li><code>위 그림</code>과 같이 사람 이미지와 같은 이미지 데이터 안에는 단순히 픽셀 데이터가 아니라 그 안에 성별, 눈 색깔 등의 <strong>여러 변동성 큰 정보들</strong>이 포함되어 있다. 하지만 이 모든 정보들을 annotated <strong>하여 이용하기 쉽지 않다.</strong> 그렇기 떄문에 <code>latent variable</code>에 원본 데이터 상에 숨겨진 특징들이 존재하게 된다.</li>
</ol>
<br>
<ol start="5">
<li>만약 특정한 특징을 나타내는 <code>latent variable(z)</code>을 사용하여 생성하게 되면, 우린 더욱 더 <strong>유연한 생성 모델을 얻을 수</strong> 있을 것 이다.</li>
</ol>
<br>
<ol start="6">
<li>그래서 이 <code>latent variable(z)</code>을 반영하여 모델의 확률 분포 $p(x,z)$를 구하는 <code>Latent Variable Model</code>에 대해서 알아볼 것이다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture5/z-model.png" height="85%" width="80%"> </div>
<ol start="7">
<li>왼쪽 그림은 <code>Latent Variable Model</code>을 간략하게 나타낸 구조이고 오른쪽 그림은 베이지안 네트워크일 때를 나타낸다.</li>
</ol>
<br>
<ol start="8">
<li>우리는 $x$와 $z$의 결합 분포인 $p(x,z) = p(z)p(x|z)$ 를 구하게 될 것이고 이것이 베이지안 네트워크로 가면 $p(x,z) = p(z_1)p(x|z_1)+&hellip;+p(z_k)p(x|z_k)$ 가 될 것이다.</li>
</ol>
<br>
<ol start="9">
<li>이렇게 잠재변수 z를 포함한 모델링을 사용하면 x만을 <strong>활용한 것보다 쉽다</strong>. 또한 만약 z의 특징을 각각 추출할 수 있다면, 그것을 이용해서 <strong>다른 종류의 task에도 이용이 가능하다</strong>. (ex)eyeColor = Blue만 식별 가능)</li>
</ol>
<br>
<ol start="10">
<li>하지만 현실적으로 모든 z중에 <strong>특정 z만 따로 추출하는 것이 어렵다는</strong> 것이다.(이 말은 위에서 현실적으로 모든 z의 확률을 구할 수 없어서 <strong>베이지안 네트워크를 사용하기 어려울 수 있다는 말</strong>과 동일하다.)</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture5/z-model-2.png" height="50%" width="50%"> </div>
<ol start="11">
<li>따라서 우리는 이 문제를 해결하기 위하여 <strong>단순한 z(가우시안)</strong> 만을 가정하여 <code>deep neural network</code>(모든 각각의 관계의 정보를 따로따로 확인하지 않아도 모든 정보를 고려해줄 수 있다.) 를 사용하여 이 z를 예측하려고 한다.</li>
</ol>
<ul>
<li>$z \sim \mathcal{N}(0,1)$</li>
<li>$p(x|z) = \mathcal{N}(\mu_\theta(z), \Sigma_\theta(z))$</li>
<li>이 문제는 비지도학습의 <code>representation learning</code>이기 때문애 학습 시 z에 대해서 잘 학습(z의 특징을 잘 추출)되기를 희망하게 된다. (이 모델이 어떤 z를 추출할지 명확하지 않다.)</li>
<li>만약 <strong>학습이 잘된다면</strong>, z의 분포를 클러스터링 하여 우리가 원하는 클러스터에 해당하는 z의 값($p(z)$)을 구할 수 있고 이 z를 가지고 새로운 데이터 $p_\theta(x,z)$를 만들어낼 수 있게 된다. 다음 챕터에서 이에 대해 더 자세히 설명할 것이다.</li>
</ul>
<hr>
<h2 id="mixture-of-gaussians-gmmvae">Mixture of Gaussians (GMM,VAE)</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture5/gaussian.png" height="50%" width="50%"> </div>
<ol start="12">
<li>그렇다면 z를 어떻게 예측 하는 방법에 대해서 자세히 알아볼 필요가 있다. 데이터의 분포를 확인하면 <code>위 그림</code>과 같을 것이다. 그 후, $K$개의 가우시안 분포(혼합 성분)를 나눌 수 있다.
그럼 <code>생성 과정</code>에선 K개 중 하나를 선택하여 그 K에 해당하는 평균, 공분산을 따르는 가우시안 분포에서 x를 샘플링한다. (이는 <code>VAE</code>에서 <strong>생성 과정에서도 동일하다.</strong>)</li>
</ol>
<ul>
<li>$\mathbf{z} \sim \text{Categorical}(1, \cdots, K)$</li>
<li>$\mathbf{x} \sim p(\mathbf{x} \mid \mathbf{z} = k) = \mathcal{N}(\mu_k, \Sigma_k)$ (likelihood)</li>
</ul>
<br>
<ol start="13">
<li>이 방법은 <code>생성 과정</code> 뿐 아니라, x가 주어졌을 때 어떤 z에 속하는지 맞출 수 있는 <code>추론 과정</code>에도 (posterior $p(z|x)$) 사용할 수 있다. (<code>clustering</code>, <code>unsupervised representation learning</code>)</li>
</ol>
<br>
<ol start="14">
<li>또한 <code>Mixture of Gaussians</code>의 전체 확률 ($p(x)$)도 구할 수 있는데 다음과 같은 식으로 가능하다.</li>
</ol>
<ul>
<li>$p(\mathbf{x}) = \sum_{z} p(\mathbf{x}, z) = \sum_{z} p(z) \cdot p(\mathbf{x} \mid z) = \sum_{k=1}^{K} p(z=k) \cdot \mathcal{N}(\mathbf{x}; \mu_k, \Sigma_k)$</li>
<li>이때 얻을 수 있는 직관은 simple한 $p(x|z)$를 활용하여 복잡한 혼합 모델($p(x)$)을 구할 수 있다는 것이다.</li>
</ul>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture5/z-model-2.png" height="50%" width="50%"> </div>
<ol start="15">
<li>지금까진 z를 어떻게 예측을 할지에 대해서 알아보았다. 이 <code>Mixture of Gaussians</code>에서는 <code>Neural Network(NN)</code>를 사용하지 않아 z의 수가 많아지면 <strong>계산하기 어려운 단점이 있다.</strong> 그럼 우린 이 z를 가지고 <code>deep neural network</code>를 사용한 <code>VAE</code>에서 어떻게 활용이 되는지 알아볼 필요가 있다.</li>
</ol>
<br>
<ol start="16">
<li><code>Mixture of Gaussians</code>에서는 <code>NN</code>을 사용하지 않기 때문에 <code>임의의 k</code>에 따른 평균과 표준편차를 <strong>조회 테이블에서 각각 구할 수 있다.</strong> 하지만 <code>VAE</code>에선 유한한 수의 k개를 <strong>구하지 않아도 된다.</strong></li>
</ol>
<ul>
<li>무한한 수의 혼합 성분(각각 가우시안을 가지는)이 있다고 생각 해도 된다.</li>
<li>$\mathbf{z} \sim \mathcal{N}(1, I)$</li>
<li>$p(x|z) = \mathcal{N}(\mu_\theta(\mathbf{z}), \Sigma_\theta(\mathbf{z}))$</li>
</ul>
<br>
<ol start="17">
<li>또한 z를 샘플링 하고 z를 <code>neural network</code>에 넣어 가우시안 요소에 대한 평균, 표준편차를 구할 수 있다.</li>
</ol>
<ul>
<li><code>VAE</code>의 <code>생성 과정</code>은 <code>GMM</code>과 동일하다. 구성 요소($K$)를 정하고 z값을 샘플링한다. 그 다음 z에 맞는 평균, 표준편차를 구한 다음(<code>neural network</code>를 통하여) x를 샘플링 하는 과정으로 진행된다.</li>
</ul>
<hr>
<h2 id="maximum-marginal-likelihood">Maximum Marginal Likelihood</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture5/digit.png" height="50%" width="50%"> </div>
<ol start="18">
<li>만약 <strong>훈련</strong> 때 <code>위 그림</code> 처럼 어떤 pixel이 보이지 않을 경우를 생각해보자. 이때 $\mathbf{X}$ 를 관측된 부분이라고 하고 $\mathbf{Z}$ 를 관측되지 않은 초록색 영역이라고 하자.</li>
</ol>
<br>
<ol start="19">
<li><code>pixel cnn</code>처럼 <code>autoregressive model</code>은 안보이는 나머지 부분을 포함한 <code>전체 결합 확률 분포</code>를 다음과 같이 정의할 수 있다. $p(\mathbf{X},\mathbf{Z};\theta)$</li>
</ol>
<br>
<ol start="20">
<li>우리는 관측된 $\mathbf{X}$ 만을 사용하여 모든 가능한 $\mathbf{Z}$ 를 구해야한다. 그렇게 하기 위하여 우리는 <code>marginal likelihhod</code>를 구해야 한다.</li>
</ol>
<ul>
<li>모든 가능한 $\mathbf{Z}$ 를 고려해서 합산해야 한다.</li>
<li>$p(\mathbf{x}) = \sum_{z} p(\mathbf{x}, z) = \sum_{z} p(z) \cdot p(\mathbf{x} \mid z)$</li>
<li>하지만 여기 z가 고차원이라면 이 것을 계산하는데 엄청난 비용이 들 수 있다.</li>
</ul>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture5/vae.png" height="10%" width="10%"> </div>
<ol start="21">
<li>만약 <code>Variational Autoencoder</code>의 경우라도 위와 비슷하다. 훈련 시간에 z가 관찰되지 않고 특정 x를 관찰할 확률을 평가하려면 z가 취할 수 있는 모든 가능한 값을 살펴보고 얼마나 x를 생성할 가능성이 있는지 알아봐야한다.</li>
</ol>
<br>
<ol start="22">
<li>z는 연속확률분포 이므로, 모든 가능한 z의 값들을 적분해야한다.</li>
</ol>
<ul>
<li>$\mathbf{z} \sim \mathcal{N}(1, I)$</li>
<li>$p(x|z) = \mathcal{N}(\mu_\theta(\mathbf{z}), \Sigma_\theta(\mathbf{z}))$</li>
<li>$p(\mathbf{x}) = \int p(\mathbf{x}, z) , dz = \int p(z) \cdot p(\mathbf{x} \mid z) , dz$</li>
<li><code>VAE</code> 또한 z의 차원이 증가할수록 엄청난 비용이 든다.</li>
</ul>
<br>
<ol start="23">
<li>따라서 우리는 <strong>훈련</strong>을 할 때 특정 데이터 셋을 생성할 확률 ($p(x)$)를 <strong>최대화 하는 매개변수</strong>를 찾으려고 할 것이다. 이때 <code>autoregressive model</code>처럼 x변수만 있는 것이 아니라, x와 z가 있기 때문에 <code>Maximum Marginal Likelihood</code>로 계산해야한다.</li>
</ol>
<br>
<p>$$\log \prod_{x \in \mathcal{D}} p(x; \theta) = \sum_{x \in \mathcal{D}} \log p(x; \theta) = \sum_{x \in \mathcal{D}} \log \sum_{z} p(x, z; \theta)$$</p>
<ol start="24">
<li>하지만 계속 한계점을 언급했던 것 처럼 $\log \sum_{z} p(x, z; \theta)$ 는 z의 차원이 커질수록 <strong>계산이 불가능 하다.</strong></li>
</ol>
<ul>
<li>더불어, z가 연속형이어도 적분을 사용해야하기 때문에 계산하기 어렵고, gradient도 계산하기 어렵다.</li>
<li>그래서 <strong>아주 저렴한 approximation</strong> 방법이 필요하다.</li>
</ul>
<hr>
<h2 id="naive-monte-carlo">Naive Monte Carlo</h2>
<p>$$p_\theta(x) = \sum_{\text{All values of } z} p_\theta(x, z)
= |\mathcal{Z}| \sum_{z \in \mathcal{Z}} \frac{1}{|\mathcal{Z}|} p_\theta(x, z)
= |\mathcal{Z}| \mathbb{E_{z \sim \text{Uniform}}(\mathcal{Z})} \left[ p_\theta(x, z) \right]$$</p>
<p>$$\sum_{z} p_\theta(x, z) \approx |\mathcal{Z}| \cdot \frac{1}{k} \sum_{j=1}^{k} p_\theta(x, z^{(j)})$$</p>
<ol start="25">
<li>몇가지 가능한 대안 중 하나가 <code>Naive Monte Carlo</code>이다. 모든 z에 대한 계산을 하는 것이 아니라, <code>monte carlo</code>를 사용하여 몇개의 z만 가지고 근사값을 계산할 수 있다.</li>
</ol>
<br>
<ol start="26">
<li>위에서 본 <code>marginal likelihood</code>의 식에서 <code>평균의 형태</code>로 바꾸고, 그것을 <code>기대값으로 표현한 식(3)</code>으로 변경하면 <code>monte carlo 샘플링</code>으로 근사가 가능한 형태로 변형이 된다.</li>
</ol>
<ul>
<li><code>Monte carlo</code>는 기대값을 샘플들의 평균으로 근사할 수 있게 한다.</li>
</ul>
<br>
<ol start="27">
<li>하지만 단순하게 z에 대하여 uniform 샘플링을 하면 variance가 크고 모델이 불안정하다. 따라서 <strong>더욱 똑똑한 z 샘플링 방식</strong>이 필요하다.</li>
</ol>
<ul>
<li>또한 z, $p(x,z)$ 가 거의 0이라 의미있는 샘플을 못뽑는다.</li>
</ul>
<hr>
<h2 id="importance-sampling">Importance Sampling</h2>
<p>$$p_\theta(x) = \sum_{z \in \mathcal{Z}} p_\theta(x, z)
= \sum_{z \in \mathcal{Z}} \frac{q(z)}{q(z)} p_\theta(x, z)
= \mathbb{E_{z \sim q(z)}} \left[ \frac{p_\theta(x, z)}{q(z)} \right]$$</p>
<ol start="28">
<li>위 한계를 극복하기 위해 <code>Important Sampling</code> 방법을 도입한다. 위의 방법에서는 의미있는 z를 추출하지 못하므로 <strong>더욱 의미 있는 z를 더 자주 뽑는 분포</strong> $q(z)$를 두는 방법을 사용한다.</li>
</ol>
<ul>
<li>참고로, 기대값의 정의는 다음과 같다.</li>
<li>$\mathbb{E_{z \sim q(z)}} [f(z)] = \sum_{z \in \mathcal{Z}} q(z) \cdot f(z)$</li>
</ul>
<br>
<p>$$p_\theta(x) \approx \frac{1}{k} \sum_{j=1}^{k} \frac{p_\theta(x, z^{(j)})}{q(z^{(j)})} \quad \text{where } z^{(j)} \sim q(z)$$</p>
<ol start="29">
<li>기대값의 식이 나왔기 때문에 <code>monte carlo</code>를 사용할 수 있게 된다. 이때 z가 uniform 분포에 따른 것이 아니라 <strong>q의 분포에 따라 샘플링이 된다.</strong></li>
</ol>
<br>
<ol start="30">
<li>그렇다면 좋은 q의 분포가 무엇일까? 라는 의문이 생길 수 있다. $p_\theta(x,z)$ 가 z들을 자주 뽑는 분포가 좋은 q의 분포이다.</li>
</ol>
<br>
<p>$$\mathbb{E_{z^{(j)} \sim q(z)}} \left[ \frac{1}{k} \sum_{j=1}^{k} \frac{p_\theta(x, z^{(j)})}{q(z^{(j)})} \right] = p_\theta(x)$$</p>
<ol start="31">
<li>또한 이는 <code>unbiased estimator</code>이다. <code>Unbiased estimator</code>는 추정한 값의 평균이 진짜 참값을 정확히 맞춘다는 뜻이다. (정답에 수렴한다.)</li>
</ol>
<ul>
<li><code>Monte carlo</code> 실험을 아주 여러번 실행하면 결국 실제 값을 얻을 수 있게 된다.</li>
<li>해당 개념에 대한 정확한 내용은 <a href="https://untitledtblog.tistory.com/192" target="_blank" rel="noopener noreffer ">이 사이트</a>를 참고하면 좋을 것 같다.</li>
</ul>
<hr>
<h2 id="evidence-lower-boundelbo---1">Evidence Lower Bound(ELBO) - 1</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture5/term.png" height="85%" width="85%"> </div>
<ol start="32">
<li>훈련 시, 우리가 관심이 있는 것은 데이터 포인트의 <code>log-likelihood</code>를 최적화 하는 것이다. 그래서 $p_\theta(x)$ 에 log를 씌우려고 봤더니 <code>Jensen Inequality</code> 때문에 로그 안에 <strong>샘플을 하나만 넣으면 편향이 생긴다.</strong> 위 그림의 2번째 줄의 수식들은 <code>Monte carlo</code> 식을 기대값으로 표현한 것이다.</li>
</ol>
<ul>
<li><code>23번 수식</code>을 다시 보면, 각 <code>data point</code>들에 대한 <code>marginal-log</code>를 구하는 것을 확인할 수 있다.</li>
<li>로그의 특성(로그는 오목(concave)함수다.)상 <strong>샘플 평균에 로그를 씌우면 값이 크게 나오고, 로그 후 샘플 평균을 취하면 작게 나온다.</strong>
<img src="/images/cs236/lecture5/example-1.png" height="75%" width="75%"></li>
</ul>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture5/example-2.png" height="85%" width="85%"> </div>
<ol start="33">
<li>위 그림은 <code>Jensen Inequality</code>를 설명한 것이다. 선형결합을 한 것이 log 함수 값보다 훨씬 낮다는 것을 알 수 있다.</li>
</ol>
<br>
<ol start="34">
<li><code>이 성질</code>을 활용하여 다시 위 사진의 수식에서 $f(\mathbf{z}) = \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q(\mathbf{z})}$ 를 넣어보면 아래와 같은 식으로 정리를 할 수 있다.</li>
</ol>
<br>
<p>$$\log \left( \mathbb{E_{\mathbf{z} \sim q(\mathbf{z})}} \left[ \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q(\mathbf{z})} \right] \right)
\geq
\mathbb{E_{\mathbf{z} \sim q(\mathbf{z})}} \left[ \log \left( \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q(\mathbf{z})} \right) \right]$$</p>
<br>
<ol start="35">
<li>오른쪽 항을 이렇게 되면 우리가 원하는 객체에 대한 <strong>하한이</strong> 생긴다. 이를 <code>Evidence Lower Bound(ELBO)</code>라고 부른다.</li>
</ol>
<ul>
<li><strong>오른쪽을 최적화 하면, 왼쪽도 희망적으로 증가할 것을 의미한다.</strong></li>
<li>x가 evidence, x는 관찰된 부분이라는 뜻이다.</li>
</ul>
<br>
<ol start="36">
<li>왼쪽을 <strong>최적화를 하진 못한다.</strong> 왜냐하면 왼쪽 항은 기대값을 <strong>먼저 계산 후</strong>, 그 결과에 로그를 취하는 형태이다. 하지만 기대값은 결국 <code>28번 수식</code>과 같이 통으로 $p_\theta(x)$ 로 나타낼 수 있기 때문에 <strong>결국 계산이 불가능하다는 것을 알 수 있다.</strong></li>
</ol>
<br>
<ol start="37">
<li>하지만 <code>ELBO</code>는 로그를 계산 후, 그 결과에 기대값을 취하는 형태이다. 따라서 이건 샘플링 기반으로 근사가 가능하다.
<img src="/images/cs236/lecture5/sampling.png" height="85%" width="85%"></li>
</ol>
<ul>
<li><code>위 그림</code>처럼 <code>monte carlo</code>를 사용할 수 있다.</li>
<li>그 후 $z^{(j)} \sim q(z)$ 여러개 샘플링하여 각각에 대해 log 값을 계산하고, 그것을 평균내면 된다.</li>
</ul>
<br>
<ol start="38">
<li>따라서 <code>ELBO</code>를 활용하여 식을 전개하면 아래와 같다. 등호는 $q = p(z \mid x; \theta)$ 일때만 성립한다.
이에 대한 자세한 내용은 다음 강의에서 이어서 하겠다.</li>
</ol>
<p>$$
\begin{aligned}
\log p(x; \theta) &amp;\geq \sum_{z} q(z) \log \left( \frac{p_\theta(x, z)}{q(z)} \right) \\
&amp;= \sum_{z} q(z) \log p_\theta(x, z) - \sum_{z} q(z) \log q(z) \\
&amp;= \sum_{z} q(z) \log p_\theta(x, z) + H(q) \ \text{(H(q) is entropy)} \\
\end{aligned}
$$
$$
\begin{aligned}
&amp;\text{Equality holds if } q = p(z \mid x; \theta)\\
&amp;\log p(x; \theta) = \sum_{z} q(z) \log p(z, x; \theta) + H(q)
\end{aligned}
$$</p>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://deepgenerativemodels.github.io/" target="_blank" rel="noopener noreffer ">https://deepgenerativemodels.github.io/</a></li>
<li><a href="https://www.youtube.com/watch?v=tRArbBf-AbI&amp;list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&amp;index=3&amp;t=4s" target="_blank" rel="noopener noreffer ">https://www.youtube.com/watch?v=tRArbBf-AbI&list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&index=3&t=4s</a></li>
<li><a href="https://wikidocs.net/228835" target="_blank" rel="noopener noreffer ">https://wikidocs.net/228835</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-07-14</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/cs236/lecture5/" data-title="[CS236] 5. Latent Variable Models-1" data-hashtags="Latent Variable Models,VAE,CS236"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/cs236/lecture5/" data-hashtag="Latent Variable Models"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/cs236/lecture5/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/cs236/lecture5/" data-title="[CS236] 5. Latent Variable Models-1"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/cs236/lecture5/" data-title="[CS236] 5. Latent Variable Models-1"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/cs236/lecture5/" data-title="[CS236] 5. Latent Variable Models-1"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/cs236/lecture5/" data-title="[CS236] 5. Latent Variable Models-1" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/latent-variable-models/">Latent Variable Models</a>,&nbsp;<a href="/tags/vae/">VAE</a>,&nbsp;<a href="/tags/cs236/">CS236</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/cs236/lecture4/" class="prev" rel="prev" title="[CS236] 4. Maximum Likelihood Learning"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[CS236] 4. Maximum Likelihood Learning</a>
            <a href="/posts/cs236/lecture6/" class="next" rel="next" title="[CS236] 6. Latent Variable Models-2">[CS236] 6. Latent Variable Models-2<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
