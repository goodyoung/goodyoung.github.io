<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[CS236] 8. Normalizing Flows - 2 - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[CS236] 8. Normalizing Flows - 2" />
<meta property="og:description" content="개요 이번 포스트에서는 CS236 강의의 8강을 설명한다. 이전 포스트에서는 change of variable 공식을 사용하여 공식을 선형적인 예시부터, 비선형적인 예시까지 확장해보았다. 이번 포스트에서는 공식을 가지고 더 나아가보겠다. Normalizing Flow Models Flow Model은 위와 같이 결정적인 함수 식에 의하여 정해진다. 우리는 이에 대해서 배웠고 관련 공식도 배웠다. 이를 실제 neural network 모델에 사용하려면 어떻게 할까? $$\mathbf{z_m} = f_\theta^{m} \circ \cdots \circ f_\theta^{1}(\mathbf{z_0}) = f_\theta^{m}\big(f_\theta^{m-1}(\cdots(f_\theta^{1}(\mathbf{z_0})))\big) \triangleq f_\theta(\mathbf{z_0})$$
시작을 쉬운 분포 $z_0$ 으로 시작한다. 여러 간단한 invertible layer인 $f_\theta$ 를 여러 레이어로 쌓아서 $f_\theta(\mathbf{z}_0)$ 를 만든다. 그렇게 되면 매우 유연한 transform을 얻을 수 있다. 그리고 $x = z_m$ 이된다. 그럼 위 변환을 change of variable에 적용하게 되면 아래와 같은 수식을 얻을 수 있다. $$p_X(\mathbf{x};\theta) = p_Z\left( f_\theta^{-1}(\mathbf{x}) \right) \prod_{m=1}^{M}\left| \det\left( \frac{\partial (f_\theta^m)^{-1}(\mathbf{z_m})}{\partial \mathbf{z_m}} \right) \right|$$
각 개별 레이어의 야코비안 행렬식을 얻어 곱하면, 해당 행렬식을 얻을 수 있게 된다. 그리고 각 함수 $f$가 invertible 하기 때문에, $f^{-1}$ 을 계산할 수 있다. 여기서 각 개별 레이어마다 change of variable은 같지만 $\theta$ 는 다르다는 것을 유의해야한다. Learning and Inference 우리는 지금까지 Normalizing Flow 가 어떻게 생겼는지에 대해서 알아왔다. 그럼 어떻게 각 데이터 셋마다 $\theta$ 를 최적화 시킬까? 즉, 학습을 어떻게 할까? 우리는 VAE와는 달리 change of variable 공식 떄문에 $p_\theta$ 에 직접 접근할 수 있기 때문에, AutoRegreesive Model 처럼 특정 데이터 셋의 log-likelihood를 최대화 하는 $\theta$ 를 찾으면 된다. 그래서 log-likelihood의 식은 아래와 같게 표현이 될 수 있다. $$\max_{\theta} \log p_X(\mathcal{D}; \theta) = \sum_{x \in \mathcal{D}} \log p_Z \big( f_\theta^{-1}(x) \big) &#43; \log \left| \det \left( \frac{\partial f_\theta^{-1}(x)}{\partial x} \right) \right|$$
양쪽 항 다 미분이 가능하기 때문에, gradient인 $\nabla_\theta \log p_X(x;\theta)$ 는 구할 수 있어서 최적화 또한 문제없다. 만약 추론(inference)에서 sampling을 해야된다면 이는, VAE와 같게 $z \sim p_Z(z) \quad x = f_\theta(z)$ 으로 구할 수 있다. z도 latent variable이긴 하지만, VAE의 z와 같은 역할을 한다고 볼 수는 없다. Normalizing Flow에서 z는 x와 차원이 같기 때문이다. 이 과정을 하기 위해서는 $f_\theta$ 를 invertible하고 jacobian 행렬 계산이 용이하도록 parameterized 해야한다. (여러 모델들을 보면서 어떻게 parameterized가 되는지 살펴볼 것이다.) Triangular Jacobian 자, 그럼 지금까지 배운 flow model의 조건들에 대해서 살펴보겠다. p(z) 는 샘플링과, likelihood 계산이 효율적으로 가능한 분포를 선택해야한다. 또한 tractable한 Invertible transformation 을 해야한다. 자코비안 행렬식 계산이 빨라야한다. 기존의 자코비안 행렬식은 nXn 행렬이다. 이는 $O(n^3)$ 의 시간복잡도를 지니고 있다. 따라서 이를 해결하기 위하여 행렬식의 구조를 변형해야한다. 기존 자코비안 행렬식에 시간복잡도가 오래걸린다는 단점을 해결하기 위하여, 기존의 자코비안 행렬식을 먼저 봐보자. 아래의 식과 같다. $$ x = (x_1, \cdots, x_n) = f(z) = (f_1(z), \cdots, f_n(z)) $$
$$ J = \frac{\partial f}{\partial z} = \begin{pmatrix} \frac{\partial f_1}{\partial z_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial z_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_n}{\partial z_1} &amp; \cdots &amp; \frac{\partial f_n}{\partial z_n} \end{pmatrix} $$
위 식에서 우리는 해당 행렬을 상삼각, 하삼각 행렬을 만들면 행렬식의 계산이 $O(n)$ 으로 빨라지게 된다. 그렇게 하기 위해서 가정이 필요하다. 바로 $x_i = f_i(z)$ 가 $z_1, &hellip;, z_i$ 까지만 의존하게 된다면 위 행렬식은 하삼각 행렬이 된다. 하삼각 행렬의 determinant는 대각선 원소들의 곱이기 때문에 $O(n)$ 으로 계산이 된다. $x_2 = f_2(z_1, z_2) \quad \Rightarrow \quad \frac{\partial x_2}{\partial z_3} = 0$ 만약 상삼각 행렬을 만들고 싶다면 $z_i, &hellip;, z_n$ 까지 의존하게 하면 된다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/cs236/lecture8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-08-10T22:59:41+09:00" />
<meta property="article:modified_time" content="2025-08-10T22:59:41+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[CS236] 8. Normalizing Flows - 2"/>
<meta name="twitter:description" content="개요 이번 포스트에서는 CS236 강의의 8강을 설명한다. 이전 포스트에서는 change of variable 공식을 사용하여 공식을 선형적인 예시부터, 비선형적인 예시까지 확장해보았다. 이번 포스트에서는 공식을 가지고 더 나아가보겠다. Normalizing Flow Models Flow Model은 위와 같이 결정적인 함수 식에 의하여 정해진다. 우리는 이에 대해서 배웠고 관련 공식도 배웠다. 이를 실제 neural network 모델에 사용하려면 어떻게 할까? $$\mathbf{z_m} = f_\theta^{m} \circ \cdots \circ f_\theta^{1}(\mathbf{z_0}) = f_\theta^{m}\big(f_\theta^{m-1}(\cdots(f_\theta^{1}(\mathbf{z_0})))\big) \triangleq f_\theta(\mathbf{z_0})$$
시작을 쉬운 분포 $z_0$ 으로 시작한다. 여러 간단한 invertible layer인 $f_\theta$ 를 여러 레이어로 쌓아서 $f_\theta(\mathbf{z}_0)$ 를 만든다. 그렇게 되면 매우 유연한 transform을 얻을 수 있다. 그리고 $x = z_m$ 이된다. 그럼 위 변환을 change of variable에 적용하게 되면 아래와 같은 수식을 얻을 수 있다. $$p_X(\mathbf{x};\theta) = p_Z\left( f_\theta^{-1}(\mathbf{x}) \right) \prod_{m=1}^{M}\left| \det\left( \frac{\partial (f_\theta^m)^{-1}(\mathbf{z_m})}{\partial \mathbf{z_m}} \right) \right|$$
각 개별 레이어의 야코비안 행렬식을 얻어 곱하면, 해당 행렬식을 얻을 수 있게 된다. 그리고 각 함수 $f$가 invertible 하기 때문에, $f^{-1}$ 을 계산할 수 있다. 여기서 각 개별 레이어마다 change of variable은 같지만 $\theta$ 는 다르다는 것을 유의해야한다. Learning and Inference 우리는 지금까지 Normalizing Flow 가 어떻게 생겼는지에 대해서 알아왔다. 그럼 어떻게 각 데이터 셋마다 $\theta$ 를 최적화 시킬까? 즉, 학습을 어떻게 할까? 우리는 VAE와는 달리 change of variable 공식 떄문에 $p_\theta$ 에 직접 접근할 수 있기 때문에, AutoRegreesive Model 처럼 특정 데이터 셋의 log-likelihood를 최대화 하는 $\theta$ 를 찾으면 된다. 그래서 log-likelihood의 식은 아래와 같게 표현이 될 수 있다. $$\max_{\theta} \log p_X(\mathcal{D}; \theta) = \sum_{x \in \mathcal{D}} \log p_Z \big( f_\theta^{-1}(x) \big) &#43; \log \left| \det \left( \frac{\partial f_\theta^{-1}(x)}{\partial x} \right) \right|$$
양쪽 항 다 미분이 가능하기 때문에, gradient인 $\nabla_\theta \log p_X(x;\theta)$ 는 구할 수 있어서 최적화 또한 문제없다. 만약 추론(inference)에서 sampling을 해야된다면 이는, VAE와 같게 $z \sim p_Z(z) \quad x = f_\theta(z)$ 으로 구할 수 있다. z도 latent variable이긴 하지만, VAE의 z와 같은 역할을 한다고 볼 수는 없다. Normalizing Flow에서 z는 x와 차원이 같기 때문이다. 이 과정을 하기 위해서는 $f_\theta$ 를 invertible하고 jacobian 행렬 계산이 용이하도록 parameterized 해야한다. (여러 모델들을 보면서 어떻게 parameterized가 되는지 살펴볼 것이다.) Triangular Jacobian 자, 그럼 지금까지 배운 flow model의 조건들에 대해서 살펴보겠다. p(z) 는 샘플링과, likelihood 계산이 효율적으로 가능한 분포를 선택해야한다. 또한 tractable한 Invertible transformation 을 해야한다. 자코비안 행렬식 계산이 빨라야한다. 기존의 자코비안 행렬식은 nXn 행렬이다. 이는 $O(n^3)$ 의 시간복잡도를 지니고 있다. 따라서 이를 해결하기 위하여 행렬식의 구조를 변형해야한다. 기존 자코비안 행렬식에 시간복잡도가 오래걸린다는 단점을 해결하기 위하여, 기존의 자코비안 행렬식을 먼저 봐보자. 아래의 식과 같다. $$ x = (x_1, \cdots, x_n) = f(z) = (f_1(z), \cdots, f_n(z)) $$
$$ J = \frac{\partial f}{\partial z} = \begin{pmatrix} \frac{\partial f_1}{\partial z_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial z_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_n}{\partial z_1} &amp; \cdots &amp; \frac{\partial f_n}{\partial z_n} \end{pmatrix} $$
위 식에서 우리는 해당 행렬을 상삼각, 하삼각 행렬을 만들면 행렬식의 계산이 $O(n)$ 으로 빨라지게 된다. 그렇게 하기 위해서 가정이 필요하다. 바로 $x_i = f_i(z)$ 가 $z_1, &hellip;, z_i$ 까지만 의존하게 된다면 위 행렬식은 하삼각 행렬이 된다. 하삼각 행렬의 determinant는 대각선 원소들의 곱이기 때문에 $O(n)$ 으로 계산이 된다. $x_2 = f_2(z_1, z_2) \quad \Rightarrow \quad \frac{\partial x_2}{\partial z_3} = 0$ 만약 상삼각 행렬을 만들고 싶다면 $z_i, &hellip;, z_n$ 까지 의존하게 하면 된다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/cs236/lecture8/" /><link rel="prev" href="https://goodyoung.github.io/posts/cs236/lecture7/" /><link rel="next" href="https://goodyoung.github.io/posts/cs236/lecture9/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[CS236] 8. Normalizing Flows - 2",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/cs236\/lecture8\/"
        },"genre": "posts","keywords": "Normalizing Flows, CS236","wordcount":  2047 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/cs236\/lecture8\/","datePublished": "2025-08-10T22:59:41+09:00","dateModified": "2025-08-10T22:59:41+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[CS236] 8. Normalizing Flows - 2</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a>&nbsp;<a href="/categories/deep-generative-models/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Generative Models</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-08-10">2025-08-10</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;2047 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;10 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#normalizing-flow-models">Normalizing Flow Models</a></li>
    <li><a href="#learning-and-inference">Learning and Inference</a></li>
    <li><a href="#triangular-jacobian">Triangular Jacobian</a></li>
    <li><a href="#nonlinear-independent-components-estimationnice">Nonlinear Independent Components Estimation(NICE)</a>
      <ul>
        <li><a href="#additive-coupling-layers">Additive Coupling layers</a></li>
        <li><a href="#rescaling-layers">Rescaling Layers</a></li>
      </ul>
    </li>
    <li><a href="#non-volume-preserving-extension-of-nice-real-nvp">Non-volume preserving extension of NICE (Real-NVP)</a></li>
    <li><a href="#continuous-autoregressive-models-as-flow-models">Continuous Autoregressive models as flow models</a>
      <ul>
        <li><a href="#masked-autoregressive-flow-maf">Masked Autoregressive Flow (MAF)</a></li>
        <li><a href="#inverse-autoregressive-flow-iaf">Inverse Autoregressive Flow (IAF)</a></li>
        <li><a href="#parallel-wavenet">Parallel Wavenet</a></li>
      </ul>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- image format
<div style="text-align:center;">
<img src="/images/cs231n/lecture4/back-1.png" height="100%" width="80%"> </div>
 -->
<h2 id="개요">개요</h2>
<ol start="0">
<li>이번 포스트에서는 <code>CS236</code> 강의의 <code>8강</code>을 설명한다.</li>
</ol>
<br>
<ol>
<li>이전 포스트에서는 <code>change of variable 공식</code>을 사용하여 공식을 <code>선형적인 예시</code>부터, <code>비선형적인 예시</code>까지 확장해보았다.</li>
</ol>
<br>
<ol start="2">
<li>이번 포스트에서는 공식을 가지고 더 나아가보겠다.</li>
</ol>
<hr>
<h2 id="normalizing-flow-models">Normalizing Flow Models</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture8/flowmodel.png" height="50%" width="50%">
</div>
<ol start="3">
<li><code>Flow Model</code>은 위와 같이 결정적인 함수 식에 의하여 정해진다. 우리는 이에 대해서 배웠고 관련 공식도 배웠다. 이를 실제 <code>neural network</code> 모델에 사용하려면 어떻게 할까?</li>
</ol>
<br>
<p>$$\mathbf{z_m} = f_\theta^{m} \circ \cdots \circ f_\theta^{1}(\mathbf{z_0}) = f_\theta^{m}\big(f_\theta^{m-1}(\cdots(f_\theta^{1}(\mathbf{z_0})))\big) \triangleq f_\theta(\mathbf{z_0})$$</p>
<ul>
<li>시작을 쉬운 분포 $z_0$ 으로 시작한다.</li>
</ul>
<ol start="4">
<li>여러 간단한 <code>invertible layer</code>인 $f_\theta$ 를 여러 레이어로 쌓아서 $f_\theta(\mathbf{z}_0)$ 를 만든다. 그렇게 되면 매우 유연한 transform을 얻을 수 있다. 그리고 $x = z_m$ 이된다. 그럼 위 변환을 <code>change of variable</code>에 적용하게 되면 아래와 같은 수식을 얻을 수 있다.</li>
</ol>
<br>
<p>$$p_X(\mathbf{x};\theta) = p_Z\left( f_\theta^{-1}(\mathbf{x}) \right)
\prod_{m=1}^{M}\left| \det\left( \frac{\partial (f_\theta^m)^{-1}(\mathbf{z_m})}{\partial \mathbf{z_m}} \right) \right|$$</p>
<ol start="5">
<li>각 개별 레이어의 야코비안 행렬식을 얻어 곱하면, 해당 행렬식을 얻을 수 있게 된다. 그리고 각 함수 $f$가 invertible 하기 때문에, $f^{-1}$ 을 계산할 수 있다. 여기서 각 개별 레이어마다 change of variable은 같지만 $\theta$ 는 다르다는 것을 유의해야한다.</li>
</ol>
<hr>
<h2 id="learning-and-inference">Learning and Inference</h2>
<ol start="6">
<li>우리는 지금까지 <code>Normalizing Flow</code> 가 어떻게 생겼는지에 대해서 알아왔다. 그럼 어떻게 각 데이터 셋마다 $\theta$ 를 최적화 시킬까? 즉, 학습을 어떻게 할까?</li>
</ol>
<br>
<ol start="7">
<li>우리는 <code>VAE</code>와는 달리 <code>change of variable 공식</code> 떄문에 $p_\theta$ 에 직접 접근할 수 있기 때문에, <code>AutoRegreesive Model</code> 처럼 특정 데이터 셋의 <code>log-likelihood</code>를 최대화 하는 $\theta$ 를 찾으면 된다. 그래서 log-likelihood의 식은 아래와 같게 표현이 될 수 있다.</li>
</ol>
<br>
<p>$$\max_{\theta} \log p_X(\mathcal{D}; \theta) = \sum_{x \in \mathcal{D}} \log p_Z \big( f_\theta^{-1}(x) \big) + \log \left| \det \left( \frac{\partial f_\theta^{-1}(x)}{\partial x} \right) \right|$$</p>
<ol start="8">
<li>양쪽 항 다 미분이 가능하기 때문에, gradient인 $\nabla_\theta \log p_X(x;\theta)$ 는 구할 수 있어서 최적화 또한 문제없다.
<ul>
<li>만약 <code>추론(inference)</code>에서 sampling을 해야된다면 이는, <code>VAE</code>와 같게 $z \sim p_Z(z) \quad x = f_\theta(z)$ 으로 구할 수 있다.</li>
<li>z도 <code>latent variable</code>이긴 하지만, VAE의 z와 같은 역할을 한다고 볼 수는 없다. <code>Normalizing Flow</code>에서 <strong>z는 x와 차원이 같기 때문이다.</strong></li>
<li>이 과정을 하기 위해서는 $f_\theta$ 를 invertible하고 jacobian 행렬 계산이 용이하도록 parameterized 해야한다. (여러 모델들을 보면서 어떻게 parameterized가 되는지 살펴볼 것이다.)</li>
</ul>
</li>
</ol>
<hr>
<h2 id="triangular-jacobian">Triangular Jacobian</h2>
<ol start="9">
<li>자, 그럼 지금까지 배운 <code>flow model</code>의 조건들에 대해서 살펴보겠다.
<ul>
<li>p(z) 는 샘플링과, likelihood 계산이 효율적으로 가능한 분포를 선택해야한다.</li>
<li>또한 tractable한 <code>Invertible transformation</code> 을 해야한다.</li>
<li>자코비안 행렬식 계산이 빨라야한다.
<ul>
<li>기존의 자코비안 행렬식은 nXn 행렬이다. 이는 $O(n^3)$ 의 시간복잡도를 지니고 있다.</li>
<li>따라서 이를 해결하기 위하여 <strong>행렬식의 구조를 변형해야한다.</strong></li>
</ul>
</li>
</ul>
</li>
</ol>
<br>
<ol start="10">
<li>기존 자코비안 행렬식에 시간복잡도가 오래걸린다는 단점을 해결하기 위하여, 기존의 자코비안 행렬식을 먼저 봐보자. 아래의 식과 같다.</li>
</ol>
<br>
<p>$$
x = (x_1, \cdots, x_n) = f(z) = (f_1(z), \cdots, f_n(z))
$$</p>
<p>$$
J = \frac{\partial f}{\partial z} =
\begin{pmatrix}
\frac{\partial f_1}{\partial z_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial z_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_n}{\partial z_1} &amp; \cdots &amp; \frac{\partial f_n}{\partial z_n}
\end{pmatrix}
$$</p>
<ol start="11">
<li>위 식에서 우리는 해당 행렬을 <code>상삼각</code>, <code>하삼각 행렬</code>을 만들면 행렬식의 계산이 $O(n)$ 으로 빨라지게 된다. 그렇게 하기 위해서 <strong>가정이 필요하다.</strong> 바로 $x_i = f_i(z)$ 가 $z_1, &hellip;, z_i$ 까지만 의존하게 된다면 위 행렬식은 <code>하삼각 행렬</code>이 된다.
<ul>
<li><code>하삼각 행렬</code>의 determinant는 대각선 원소들의 곱이기 때문에 $O(n)$ 으로 <strong>계산이 된다.</strong></li>
<li>$x_2 = f_2(z_1, z_2) \quad \Rightarrow \quad \frac{\partial x_2}{\partial z_3} = 0$</li>
<li>만약 상삼각 행렬을 만들고 싶다면 $z_i, &hellip;, z_n$ 까지 의존하게 하면 된다.</li>
</ul>
</li>
</ol>
<p>$$
J = \frac{\partial f}{\partial z} =
\begin{pmatrix}
\frac{\partial f_1}{\partial z_1} &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_n}{\partial z_1} &amp; \cdots &amp; \frac{\partial f_n}{\partial z_n}
\end{pmatrix}
$$</p>
<hr>
<h2 id="nonlinear-independent-components-estimationnice">Nonlinear Independent Components Estimation(NICE)</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture8/nice.png" height="50%" width="50%">
</div>
<ol start="12">
<li>우리는 <code>Normalizing Flow Model</code>의 기본적인 개념에 대해서 알아봤고, 이제 이 개념이 실제 neural network에서 어떻게 쓰이는지 알아볼 것이다. 그 예시 중 첫 번째로 <code>Nonlinear Independent Components Estimation(NICE)</code>에 대해서 알아볼 것이다.</li>
</ol>
<br>
<ol start="13">
<li><code>NICE</code>에는 두 가지 기능이 있는데, 이 두 가지 기능에 대해서 살펴볼 것이다.</li>
</ol>
<br>
<h3 id="additive-coupling-layers">Additive Coupling layers</h3>
<ol start="14">
<li>첫 번째는 <code>Additive Coupling layers</code>이다. 이는 $f_\theta$ 에 <strong>비선형성을 부여하기 위한</strong> 기능이다. 이 기능은 z를 $\mathbf{z_{1:d}}, \mathbf{z_{d+1:n}}$ 로 2가지의 group으로 나눈다.
<ul>
<li>이때 $1 \leq d &lt; n$ 이고, d는 임의로 지정되는 인자이다.</li>
<li>각 neural network layer마다 d가 다르다.그러</li>
</ul>
</li>
</ol>
<br>
<ol start="15">
<li>그럼 이때 우리가 계속 배웠던 <code>Forward Mapping(z-&gt;x)</code> 과 <code>Inverse Mapping(x-&gt;z)</code>을 살펴보겠다.</li>
</ol>
<ul>
<li>
<p><strong>Forward Mapping</strong></p>
<ul>
<li>앞부분은 그대로 두어  $ \mathbf{x_{1:{d}}} = \mathbf{z_{1:{d}}} $  (Identity Transformation)</li>
<li>뒷부분은 앞부분을 이용해 변환<br>
$$ \mathbf{x_{d+1:{n}}} = \mathbf{z_{d+1:{n}}} + m_\theta(\mathbf{z_{1:{d}}}) $$<br>
여기서 $m_\theta(\cdot)$는 파라미터 $\theta$를 가진 신경망이며, 입력 차원은 $d$, 출력 차원은 $n-d$이다.
<ul>
<li>$m$ 을 <strong>하나의 layer로 이루어진 MLP라고 생각하면 쉽다.</strong></li>
<li>이로 인해 <strong>비선형성</strong>이 주어진다.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Inverse Mapping</strong></p>
<ul>
<li><code>Forward Mapping</code>에서 <strong>덧셈을</strong> 사용하였기 때문에, 그 역은 <strong>뺄셈만</strong> 사용하여 invertible이 쉽게 되는 것을 확인할 수 있다.</li>
<li>앞부분은 그대로 복원  $ \mathbf{z_{1:{d}}} = \mathbf{x_{1:{d}}} $  (Identity Transformation)</li>
<li>뒷부분은 앞부분을 이용해 복원<br>
$$ \mathbf{z_{d+1:{n}}} = \mathbf{x_{d+1:{n}}} - m_\theta(\mathbf{x_{1:{d}}}) $$</li>
</ul>
</li>
</ul>
<br>
<ol start="16">
<li>앞선 과정을 거치면 Jacobian 행렬식의 계산이 <strong>쉬워지게 된다.</strong> Jacobian 식은 다음과 같이 표현될 수 있다. 이유가 궁금하다면 하단의 toggle을 확인하면 좋을 것 같다.</li>
</ol>
<p>$$
J = \frac{\partial \mathbf{x}}{\partial \mathbf{z}} =
\begin{pmatrix}
I_d &amp; 0 \\
\frac{\partial \mathbf{x_{{d+1}:n}}}{\partial \mathbf{z_{1:{d}}}} &amp; I_{n-d}
\end{pmatrix}
$$</p>
  <details>
  <summary> 자코비안 행렬식 이유 </summary>
<ul>
<li>
<p><strong>(위-왼쪽 블록)</strong><br>
$$ \frac{\partial \mathbf{x_{1:{d}}}}{\partial \mathbf{z_{1:{d}}}} = I_d $$<br>
(앞부분을 <strong>그대로</strong> 복사하므로 항등 행렬)</p>
</li>
<li>
<p><strong>(위-오른쪽 블록)</strong><br>
$$ \frac{\partial \mathbf{x_{1:{d}}}}{\partial \mathbf{z_{d+1:{n}}}} = 0 $$
(앞부분 $\mathbf{x}$는 뒷부분 $\mathbf{z}$에 의존하지 않음)</p>
</li>
<li>
<p><strong>(아래-왼쪽 블록)</strong><br>
$$ \frac{\partial \mathbf{x_{d+1:{n}}}}{\partial \mathbf{z_{1:{d}}}} = \frac{\partial \mathbf{x_{{d+1}:n}}}{\partial \mathbf{z_{1:{d}}}} $$
(앞부분 $\mathbf{z}$가 $m_\theta$에 <strong>들어가므로</strong> 뒷부분 $\mathbf{x}$에 영향을 줌)</p>
</li>
<li>
<p><strong>(아래-오른쪽 블록)</strong><br>
$$ \frac{\partial \mathbf{x_{d+1:{n}}}}{\partial \mathbf{z_{d+1:{n}}}} = I_{n-d} $$<br>
(뒷부분 $\mathbf{z}$가 <strong>그대로</strong> 더해졌으므로 미분하면 항등 행렬)</p>
</li>
</ul>
  </details>
<br>
<ol start="17">
<li>이 행렬은 <strong>하삼각 행렬</strong>이므로 행렬식은 $\det(J) = \det(I_d) \cdot \det(I_{n-d}) = 1$ 이다.
<ul>
<li>따라서 이 변환은 <strong>Volume Preserving Transformation</strong> 이다.
<ul>
<li>초입방체를 축소하거나 확장하지 않고, 그저 확률 질량만 이동한다.</li>
</ul>
</li>
<li>또한 7번의 maximize log-likelihood 를 떠올려 보면 자코비안 항이 $\log 1 = 0 $ 으로 여러 <code>coupling layer</code>를 쌓아도 <strong>상관이 없다는 것을 확인할 수 있다.</strong></li>
</ul>
</li>
</ol>
<br>
<h3 id="rescaling-layers">Rescaling Layers</h3>
<ol start="18">
<li>여러 Addiitive Coupling Layer가 쌓여서 하나의 NICE를 구성할 것이다. 이때 마지막 layer에서 Rescaling 을 하여 정규화를 한다. 아래는 Rescaling Layer의 <code>Forward Mapping(z-&gt;x)</code> 과 <code>Inverse Mapping(x-&gt;z)</code>을 나타낸 것이다.</li>
</ol>
<ul>
<li><strong>Forward Mapping</strong>
<ul>
<li>각 차원 $i$에 대해 스케일 $s_i &gt; 0$을 곱해준다. $x_i = s_i z_i$
<ul>
<li>여기서 $s_i$는 $i$-번째 차원의 scaling factor.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Inverse Mapping</strong>
<ul>
<li>Forward Mapping의 역변환은 나눗셈으로 하면 된다. $z_i = \frac{x_i}{s_i}$</li>
</ul>
</li>
<li><strong>자코비안 행렬</strong>
<ul>
<li>자코비안 행렬은 대각선에만 $s_i$ 가 있고 나머지는 0인 대각 행렬이 된다. ($x_i = s_i z_i$ 이기 때문에)</li>
<li>$J = \mathrm{diag}(s)$, $\det(J) = \prod_{i=1}^n s_i$ 이다.</li>
</ul>
</li>
</ul>
<br>
<ol start="19">
<li><code>NICE</code> 모델은 간단하다고 생각할 수 있지만, 실험 결과는 생각보다 좋다. 하지만 z가 x와 같은 차원이다보니 연산량이 많은 모델이기도 하다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture8/nice-1.png" height="80%" width="80%">
</div>
<hr>
<h2 id="non-volume-preserving-extension-of-nice-real-nvp">Non-volume preserving extension of NICE (Real-NVP)</h2>
<ol start="20">
<li>위의 NICE 모델은 행렬식이 1이어서 <code>Volume Preserving Transformation</code>를 만족한다고 했다.
그렇다면 이동하는 동시에 확장하는 것은 안될까? <code>Real-NVP</code> 에서 그것을 설명한다.</li>
</ol>
<br>
<ol start="21">
<li>그럼 <code>Real-NVP</code>의 <code>Forward Mapping(z-&gt;x)</code> 도 <code>Inverse Mapping(x-&gt;z)</code>을 살펴보겠다.</li>
</ol>
<ul>
<li><strong>Forward Mapping</strong>
<ul>
<li>앞부분은 그대로 두어<br>
$ \mathbf{x_{1:{d}}} = \mathbf{z_{1:{d}}} $  (Identity Transformation)</li>
<li>뒷부분은 <strong>scale(α)</strong> 과 <strong>shift(μ)</strong> 를 적용하여 변환<br>
$$ \mathbf{x_{d+1:{n}}} = \mathbf{z_{d+1:{n}}} \odot \exp(\alpha_\theta(\mathbf{z_{1:{d}}})) + \mu_\theta(\mathbf{z_{1:{d}}}) $$<br>
여기서 $\mu_\theta(\cdot)$, $\alpha_\theta(\cdot)$는 모두 신경망(MLP)이며,<br>
입력 차원은 $d$, 출력 차원은 $n-d$이다.
<ul>
<li>$\mu$ : <strong>평행이동(translation)</strong> 역할</li>
<li>$\alpha$ : <strong>스케일(scale)</strong> 조정 역할 (<code>확장</code>을 할 수 있는 역할)</li>
</ul>
</li>
</ul>
</li>
<li><strong>Inverse Mapping</strong>
<ul>
<li>앞부분은 그대로 복원<br>
$ \mathbf{z_{1:{d}}} = \mathbf{x_{1:{d}}} $  (Identity Transformation)</li>
<li>뒷부분은 앞부분을 이용해 복원<br>
$$ \mathbf{z_{d+1:{n}}} = \left(\mathbf{x_{d+1:{n}}} - \mu_\theta(\mathbf{x_{1:{d}}})\right) \odot \exp(-\alpha_\theta(\mathbf{x_{1:{d}}})) $$</li>
</ul>
</li>
</ul>
<br>
<ol start="22">
<li>또한 <code>Real-NVP</code>의 자코비안 행렬식은 어떻게 계산이 될까? 아래에서 확인해보겠다.</li>
</ol>
<ul>
<li><strong>Jacobian of Forward Mapping</strong>
<ul>
<li>
<p>Forward mapping의 Jacobian은 <strong>블록 삼각행렬 형태</strong>가 된다.<br>
$$ J = \frac{\partial \mathbf{x}}{\partial \mathbf{z}} =
\begin{pmatrix}
I_d &amp; 0 \\
\frac{\partial \mathbf{x_{d+1:{n}}}}{\partial \mathbf{z_{1:{d}}}} &amp; \text{diag}!\big(\exp(\alpha_\theta(\mathbf{z_{1:{d}}}))\big)
\end{pmatrix} $$</p>
</li>
<li>
<p>determinant는 대각 블록의 곱으로 단순화된다.<br>
$$ \det(J) = \prod_{i=d+1}^{n} \exp(\alpha_\theta(\mathbf{z_{1:{d}}})i) = \exp\Bigg(\sum_{i=d+1}^{n} \alpha_\theta(\mathbf{z_{1:{d}}})_i\Bigg) $$</p>
</li>
</ul>
</li>
</ul>
<br>
<ol start="23">
<li>위 행렬식 $\det(J)$ 을 보면, NICE와 달리 1이 아닌 것을 확인할 수 있다. 따라서 <code>Real-NVP</code> 는 <code>Volume Preserving Transformation</code> 을 만족하지 않고 더욱 유연한 모델이라고 볼 수 있다. 그리고 <code>Real-NVP</code>에서 <code>latent variable z</code>에 대한 보간 실험도 했다. 아래 실험 결과를 보면 <code>latent variable z</code>가 <strong>압축적이지 않더라도</strong> 보간을 하면 <strong>의미 있는 결과</strong>를 얻는 것을 확인 할 수 있다.
아래는 <code>Real-NVP</code> 의 <strong>모델 실험 결과</strong>와 <strong>보간 실험이다</strong></li>
</ol>
<div style="text-align:center;">
<img src="/images/cs236/lecture8/nvp.png" height="80%" width="80%">
</div>
<div style="text-align:center;">
<img src="/images/cs236/lecture8/nvp-1.png" height="80%" width="80%">
</div>
<hr>
<h2 id="continuous-autoregressive-models-as-flow-models">Continuous Autoregressive models as flow models</h2>
<ol start="24">
<li><code>Normalizing Model</code>의 또 다른 관점은 <code>AutoRegreesive Model</code>을 <code>Normalizing Flow</code> 관점으로 볼 수도 있다는 것이다.</li>
</ol>
<br>
<ol start="25">
<li>따라서 <code>random variable</code>이 Gaussian에서 연속적일 때의 <code>AR</code>의 식을 아래와 같이 설정할 수 있고, 각 조건부 확률 분포 또한 표현하면 아래와 같다.</li>
</ol>
<br>
<p>$$p(\mathbf{x}) = \prod_{i=1}^n p(x_i \mid \mathbf{x_{&lt;i}})$$
$$p(x_i \mid \mathbf{x_{&lt;i}}) = \mathcal{N}\big(\mu_i(x_1, \cdots, x_{i-1}), ; \exp(\alpha_i(x_1, \cdots, x_{i-1}))^2\big)$$</p>
<ul>
<li>$\mu_i(\cdot)$ : 평균을 출력하는 네트워크 (혹은 함수)</li>
<li>$\alpha_i(\cdot)$ : 로그 분산을 출력하는 네트워크 (혹은 함수)</li>
</ul>
<br>
<ol start="26">
<li>
<p>이 AR 모델을 sampling을 한다면 아래와 같이 <strong>결정론적으로</strong> 계산할 수 있다. (이는 <code>AR</code>을 <code>Flow Model</code>의 특성을 띄게 하는 <strong>중요한 부분이다.</strong>)</p>
<ul>
<li>
<p>우선 $z_i \sim \mathcal{N}(0,1)$에서 표준정규분포 샘플을 얻습니다. 그 다음 $x_i$를 재구성할 수 있습니다.</p>
</li>
<li>
<p><code>첫 번째 Sampling</code>: $x_1 = \exp(\alpha_1) z_1 + \mu_1$</p>
</li>
<li>
<p><code>두 번째 Sampling</code>: $x_2 = \exp(\alpha_2(x_1)) z_2 + \mu_2(x_1)$</p>
</li>
<li>
<p><code>세 번째 Sampling</code>: $x_3 = \exp(\alpha_3(x_1, x_2)) z_3 + \mu_3(x_1, x_2)$</p>
</li>
</ul>
</li>
</ol>
<br>
<ol start="27">
<li>따라서, 위의 <code>AR</code> 모델이 <code>Flow Model</code>로 해석될 수 있다는 것을 알 수 있다. 왜냐하면 기본 분포(latent variable z)로 부터 invertible transformation을 통하여 데이터 (x)를 AR처럼 만들어 내기 때문이다.
<ul>
<li>따라서 이렇게 표현하면 <code>AR</code>은 <code>Flow Model</code>의 한 종류라고도 볼 수 있게 된다.</li>
</ul>
</li>
</ol>
<h3 id="masked-autoregressive-flow-maf">Masked Autoregressive Flow (MAF)</h3>
<div style="text-align:center;">
<img src="/images/cs236/lecture8/maf.png" height="70%" width="47.1%">
<img src="/images/cs236/lecture8/maf-1.png" height="80%" width="49.5%"></div>
<ol start="28">
<li>위에서 봤던 구조를 직접 사용한 모델인 <code>MAF</code>에 대한 설명이다.</li>
</ol>
<ul>
<li>
<p><strong>Forward Mapping</strong></p>
<ul>
<li>
<p>첫 번째 변수 변환: $ \mathbf{x_1} = \exp(\alpha_1) \cdot z_1 + \mu_1 $<br>
이때 $\mu_2(x_1), \alpha_2(x_1)$를 계산한다.</p>
</li>
<li>
<p>두 번째 변수 변환: $ \mathbf{x_2} = \exp(\alpha_2) \cdot z_2 + \mu_2 $<br>
이때 $\mu_3(x_1, x_2), \alpha_3(x_1, x_2)$를 계산한다.</p>
</li>
<li>
<p>이런 식으로 반복하여 $x_i$를 순차적으로 계산한다.</p>
</li>
<li>
<p>여기서,</p>
<ul>
<li>$\mu$ : <strong>평행이동(translation)</strong> 역할</li>
<li>$\alpha$ : <strong>스케일(scale)</strong> 조정 역할 (<code>확장/축소</code> 가능)</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Inverse Mapping</strong></p>
<ul>
<li><strong>학습할때</strong> 모든 $\mu_i, \alpha_i$를 먼저 계산 가능(e.g., MADE를 사용하여 병렬적으로 가능)
<ul>
<li>학습이기 때문에 모든 $x_i$를 알 수 있기 ㅇ때문이다. (for)</li>
</ul>
</li>
<li>첫 번째 변수 복원: $ \mathbf{z_1} = \frac{x_1 - \mu_1}{\exp(\alpha_1)} $</li>
<li>두 번째 변수 복원: $ \mathbf{z_2} = \frac{x_2 - \mu_2}{\exp(\alpha_2)} $</li>
<li>세 번째 변수 복원: $ \mathbf{z_3} = \frac{x_3 - \mu_3}{\exp(\alpha_3)} $</li>
<li>이런 식으로 $z_i$를 순차적으로 복원한다.</li>
</ul>
</li>
<li>
<p><strong>Jacobian</strong></p>
<ul>
<li><code>야코비안 행렬</code>은 다른 flow model과 마찬가지로 <code>lower diagonal</code>이다. 따라서 계산이 수월하다.</li>
</ul>
</li>
</ul>
<br>
<ol start="29">
<li><code>MAF</code>에서 $\alpha, \mu$의 계산은 MADE(네트워크에 마스크를 씌워서 특정 $i$ 까지만 의존하게 만드는 구조)와 같은 네트워크를 사용해서 $x_{1:i-1}$ 만 의존할 수 있도록 하고 <strong>병렬적으로 한번에 구할 수 있다.</strong> 하지만 결국 구조적 문제 때문에, <code>MAF</code>에서 <code>Sampling</code>을 할 땐 $z_1$ 만 구할 수 있으니까 <code>Autoregressive</code> 하게 $x_i$ 까지 계산을 해야되서 시간 복잡도가 $O(n)$ 이 걸리는 <strong>단점이 있다.</strong> 이를 해결하기 위하여 <code>MAF</code>가 아닌 <code>Inverse Autoregressive Flow (IAF)</code>가 나오게 되었다.</li>
</ol>
<h3 id="inverse-autoregressive-flow-iaf">Inverse Autoregressive Flow (IAF)</h3>
<div style="text-align:center;">
<img src="/images/cs236/lecture8/iaf-1.png" height="70%" width="47.1%"></div>
<ol start="30">
<li><code>IAF</code>에서는 <code>MAF</code>와 달리, 네트워크에 들어가는 <strong>입력을 z로 변환</strong>시켰는데, 이것이 가능한 이유는 <code>MAF</code>에서 보여준 network들은 <strong>invertible network</strong>이기 때문에 사실 입력, 출력이 무엇인지 상관이 없기 때문이다.</li>
</ol>
<ul>
<li>
<p><strong>Forward Mapping</strong></p>
<ul>
<li>첫 번째 변수 변환: $ \mathbf{x_1} = \exp(\alpha_1)\cdot z_1 + \mu_1 $</li>
<li>두 번째 변수 변환: $ \mathbf{x_2} = \exp(\alpha_2(z_1))\cdot z_2 + \mu_2(z_1) $</li>
<li>세 번째 변수 변환: $ \mathbf{x_3} = \exp(\alpha_3(z_1,z_2))\cdot z_3 + \mu_3(z_1,z_2) $</li>
<li>여기서,
<ul>
<li>$\mu$ : <strong>평행이동(translation)</strong> 역할</li>
<li>$\alpha$ : <strong>스케일(scale)</strong> 조정 역할 (<code>확장/축소</code> 가능)</li>
<li>모든 $z_i$는 base 분포에서 <strong>동시에 샘플</strong> 가능.</li>
<li>$\mu_i(\cdot), \alpha_i(\cdot)$는 **$z_{1:i-1}$**에만 의존하도록 마스킹된 네트워크가 <strong>한 번에 전부 출력</strong>할 수 있으므로,<br>
실제 구현에서는 $x_1,\dots,x_n$을 <strong>병렬적으로 한 번에 계산</strong>할 수 있다.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Inverse Mapping (x → z, 학습/likelihood 계산 시 사용)</strong></p>
<ul>
<li>이 방향에서는 $\mu_i, \alpha_i$가 **$z_{1:i-1}$**에 의존하므로, <strong>모두를 미리 병렬 계산할 수 없다</strong>.<br>
따라서 $z_i$를 <strong>순차적으로</strong> 복원해야 한다.</li>
<li>첫 번째 변수 복원: $\mathbf{z_1} = \frac{x_1 - \mu_1}{\exp(\alpha_1)} $</li>
<li>두 번째 변수 복원 : $\mathbf{z_2} = \frac{x_2 - \mu_2(z_1)}{\exp(\alpha_2(z_1))} $</li>
<li>세 번째 변수 복원 : $\mathbf{z_3} = \frac{x_3 - \mu_3(z_1,z_2)}{\exp(\alpha_3(z_1,z_2))} $</li>
</ul>
</li>
</ul>
<br>
<ol start="31">
<li>따라서 우리가 원하는 <code>Sampling</code>($x_i$가 나오기)을 할 때는 꽤 빠르다. 하지만 <code>IAF</code>도 역시 <strong>단점이</strong> 있다. 바로, <code>x-&gt;z</code>가 필요한 상황에서는 느리다는 것이다.
<ul>
<li><code>x-&gt;z</code>가 필요한 상황은 모델을 <strong>훈련시키고</strong> 싶어서 <code>maximum likelihood train</code>이 필요할 때이다.</li>
<li>이미지(x)에서 z로 갈 수 있어야 <strong>훈련이 가능</strong>하기 때문이다.
<strong>모든 데이터 포인트</strong>에서 이를 수행해야한다. 그럴 때 모든 데이터 포인트 (x)에 대하여 매핑되는 z를 구해야 하기 때문에, <strong>모두 계산을 해야한다.</strong></li>
<li>하지만 만약, forward 단계에서 z를 <code>cache</code>한다면 해결이 될수도 있다고 한다.</li>
</ul>
</li>
</ol>
<h3 id="parallel-wavenet">Parallel Wavenet</h3>
<ol start="32">
<li><code>MAF</code>와 <code>IAF</code>를 요약 비교해보자.</li>
</ol>
<ul>
<li><strong>MAF</strong>: Likelihood 계산은 빠르지만 샘플링이 느리다. → **훈련(MLE, 밀도추정)**에 적합</li>
<li><strong>IAF</strong>: 샘플링은 빠르지만 Likelihood 계산이 느리다. → <strong>실시간 생성</strong>에 적합</li>
</ul>
<br>
<ol start="33">
<li>그렇다면 <code>MAF</code>와 <code>IAF</code>의 장점을 <strong>모두 사용할 수 있는 방법은 없을까?</strong> 이 질문에 대한 방법은 있다. 바로 교사 모델은 <code>MAF</code>로 <code>훈련하여 효율적인 훈련</code>을 하고, 그 모델을 distillation하고, 학생 모델을 <code>IAF</code> 구조로 하여 학생 모델으로 <code>sampling을 진행하면</code> 매우 효율적인 프레임워크를 얻을 수 있다. 이것이 <code>Parallel Wavenet</code>이다.</li>
</ol>
<br>
<p>$$D_{KL}(s, t) = \mathbb{E_{x \sim s}}\left[ \log s(x) - \log t(x) \right]$$</p>
<ol start="34">
<li>위 식은 <code>Parallel Wavenet</code>의 목적식인데, <code>s-&gt;t</code>로 진행하면서 많은 이점을 얻을 수 있게 된다. 해당 목적 함수로 최적화를 진행할 때, 많은 sampling 과정을 거쳐야하는데 student 모델은 sampling이 teacher 보다 수월하기 때문에 효율적이다. 따라서 해당 목적식을 거치면 학생 모델의 분포가 <strong>교사 모델의 분포를 닮게 된다.</strong>
<ul>
<li>아래는 <code>Parallel Wavenet</code>의 전체 흐름이다.
<ul>
<li>훈련
<ul>
<li>Step1: Teacher 훈련</li>
<li>Step2: Student 훈련 (KL 목적식 사용) -&gt; (Teacher는 <strong>fixed</strong>)</li>
</ul>
</li>
<li>추론
<ul>
<li>Only Student (<strong>Sampling이 빨라서</strong>)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<br>
<ol start="35">
<li>그 다음 <code>Normalizing Flow</code>의 여러 모델들을 간단히 살펴 보았는데, <code>MintNet</code>, <code>Gaussianization Flows</code> 을 살펴보았다. 자세한건 논문을 살펴보면 될 것 같고, <code>Gaussianization Flows</code>에서 사용된 <code>Inverse CDF Trick</code>이 중요해보여서 아래 사이트에서 참고하여 공부하면 좋을 것 같다.</li>
</ol>
<ul>
<li><code>Inverse CDF Trick</code>이란 <code>Uniform 분포</code>에서 <code>특정한 분포</code>(예: Gaussian)로 변환이 되는 분포이다.
<ul>
<li>컴퓨터의 난수 생성기는 <strong>보통 Uniform</strong>이기 때문에 <code>특정 분포</code>를 띄고 싶을때 유용하게 사용된다.</li>
</ul>
</li>
<li><a href="https://domybestinlife.tistory.com/348" target="_blank" rel="noopener noreffer ">Inverse CDF Trick 참고 자료</a></li>
</ul>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://deepgenerativemodels.github.io/" target="_blank" rel="noopener noreffer ">https://deepgenerativemodels.github.io/</a></li>
<li><a href="https://www.youtube.com/watch?v=qgTvgBCOyn8&amp;list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&amp;index=9" target="_blank" rel="noopener noreffer ">https://www.youtube.com/watch?v=qgTvgBCOyn8&list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&index=9</a></li>
<li><a href="https://deepseow.tistory.com/45" target="_blank" rel="noopener noreffer ">https://deepseow.tistory.com/45</a></li>
<li><a href="https://domybestinlife.tistory.com/348" target="_blank" rel="noopener noreffer ">https://domybestinlife.tistory.com/348</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-08-10</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/cs236/lecture8/" data-title="[CS236] 8. Normalizing Flows - 2" data-hashtags="Normalizing Flows,CS236"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/cs236/lecture8/" data-hashtag="Normalizing Flows"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/cs236/lecture8/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/cs236/lecture8/" data-title="[CS236] 8. Normalizing Flows - 2"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/cs236/lecture8/" data-title="[CS236] 8. Normalizing Flows - 2"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/cs236/lecture8/" data-title="[CS236] 8. Normalizing Flows - 2"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/cs236/lecture8/" data-title="[CS236] 8. Normalizing Flows - 2" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/normalizing-flows/">Normalizing Flows</a>,&nbsp;<a href="/tags/cs236/">CS236</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/cs236/lecture7/" class="prev" rel="prev" title="[CS236] 7. Normalizing Flows - 1"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[CS236] 7. Normalizing Flows - 1</a>
            <a href="/posts/cs236/lecture9/" class="next" rel="next" title="[CS236] 9. GANs - 1">[CS236] 9. GANs - 1<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
