<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[CS236] 3. Autoregressive Models - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[CS236] 3. Autoregressive Models" />
<meta property="og:description" content="개요 CS236: Deep Generative Models (Stanford)는 스탠포드 대학교에서 진행하는 딥러닝 기반 생성 모델(Deep Generative Models) 에 대한 심화 강의이다. 이번 포스트에서는 CS236 강의의 3강 내용을 정리한다. 3강에서는 주어진 데이터셋 D의 분포를 학습하여, 이로부터 새로운 샘플을 생성하는 방법을 배운다. 그렇게 하기 위해서 2가지 절차가 진행되어야 한다: Model Family를 파라미터화 (3강에서 다룰 내용) 데이터셋 D에 대하여 모델 파라미터 $\theta$ 를 찾는 방법 (4강에서 다룰 내용) Autoregressive Models $$ \begin{aligned} p(x_1, x_2, \ldots, x_{784}) &amp;= p(x_1),p(x_2 \mid x_1),p(x_3 \mid x_1, x_2), \cdots, p(x_{784} \mid x_1, \ldots, x_{783}) \\ p(x_1, \cdots, x_{784}) &amp;= p_{\text{CPT}}(x_1; \alpha_1) \cdot p_{\text{logit}}(x_2 \mid x_1; \alpha_2) \cdot p_{\text{logit}}(x_3 \mid x_1, x_2; \alpha_3) \cdots p_{\text{logit}}(x_n \mid x_1, \cdots, x_{n-1}; \alpha_n) \\ \end{aligned} $$
Autoregressive Model은 각 픽셀을 이전 픽셀들에 대한 조건부 확률로 예측하는 모델이다. 이러한 조건부 확률을 계산을 하기 위해서 Chain rule factorization을 사용한다. 하지만 이때 모든 확률 조건들을 저장할 수 없다. 따라서 어떤 신경모델, 함수를 사용하여 조건문을 모델링 하려고 한다. 위의 두번째 수식 처럼 logistic regression 함수를 이용한 방법이 그 예시이다. $$ \begin{aligned} p_{\text{CPT}}(X_1 = 1; \alpha_1) = \alpha_1 &amp;, p(X_1 = 0) = 1 - \alpha_1, \\ p_{\text{logit}}(X_2 = 1 \mid x_1; \alpha_2) &amp;= \sigma(\alpha_2^0 &#43; \alpha_2^1 x_1), \\ p_{\text{logit}}(X_3 = 1 \mid x_1, x_2; \alpha_3) &amp;= \sigma(\alpha_3^0 &#43; \alpha_3^1 x_1 &#43; \alpha_3^2 x_2) \end{aligned} $$
MNIST 데이터셋으로 Autoregressive Model을 구성하는 수식을 살펴보게 되면 위의 수식과 같다. 첫번째 픽셀이 black or white인지 조건부 확률 테이블(CPT)에서 값을 받아 배정된다.(보통 black) 그 후 순서에 따라 각 값을 예측하게 된다. Fully Visible Sigmoid Belief Network (FVSBN) 초창기 autoregressive model인 FVSBN은 chain rule을 이용해 확률분포를 나타낸 이후, 많은 컴퓨팅 파워를 요구하는 조건부 확률을 매개변수화한 이후 logistic regression 알고리즘을 적용해 학습하는 생성모델 알고리즘이다. 만약 pixel이 4개라고 가정을 하면, 이전 픽셀에 따라서 값이 다른 것을 확인할 수 있다. 이 모델의 경우 파라미터는 $1&#43;2&#43;3&#43;\cdot\cdot\cdot&#43;n \approx n^2/2$ 인 것도 확인할 수 있다. FVSBN에서 sampling하는 방법이다. 랜덤한 난수를 먼저 생성하고, 그 난수($\bar{x_1}$)로부터 $\bar{x_2}$ 가 나오게 된다. 하지만 FVSBN의 sampling 결과가 좋진 않다. Neural Autoregressive Density Estimation (NADE) NADE는 많은 컴퓨팅 파워를 요구하는 FVSBN의 한계를 보완하기 위해서 logistic regression 대신 Neural Network를 사용한 모델이다. $$ \begin{aligned} h_i &amp;= \sigma(W_{\cdot,&lt;i} x_{&lt;i} &#43; c) \\ \hat{x_i} = p(x_i \mid x_1, \ldots, x_{i-1}) &amp;= \sigma(\alpha^\top h_i &#43; b_i) \end{aligned} $$
위 수식의 $h_i$는 neural network를 의미한다. NADE는 Weight Sharing(Weight tying)기법을 활용하여 계산량을 대폭 줄였다. Weight Sharing은 학습 과정에서 은닉층($h_i$)에서 사용하는 가중치 $w_i$를 동일하게 유지하는 방법이다. 층이 깊어질수록 기존 가중치 벡터 (W)에 열벡터 하나만 추가하는 것으로 생각하면 된다. 이런 결과로 계산 복잡도가 O(n)으로 줄어든 것을 확인할 수 있다. 위 사진이 NADE의 생성 결과인데, 왼쪽이 sample이고 오른쪽이 생성 결과이다. 이미지 구조(분포)를 꽤나 잘 파악하고 있는 것으로 확인할 수 있다. 생성의 작동 방식은 왼쪽 sample에서 픽셀 값을 가져오고 NADE가 계산을 통하여 확률 값이 나오게 된다. 따라서 각 픽셀에 대한 확률 값이기 때문에 오른쪽 샘플의 결과는 0~1사이의 확률값이기때문에 이미지가 부드러워 보인다. 지금까지는 이진 데이터만 다루었다. 하지만 color 이미지와 같은 범주형 변수일 때는 어떻게 작동 할까? $x_i \in {0, \ldots, K}$ 인 다항 변수의 모델링이 필요할 것이다. 다항 변수의 모델링을 하기 위해서 각 $x_i$에 softmax를 취하여 범주형 확률 분포를 만들어주면 기존의 조건부 확률을 계산할 수 있게 된다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/cs236/lecture3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-05-07T22:58:36+09:00" />
<meta property="article:modified_time" content="2025-05-07T22:58:36+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[CS236] 3. Autoregressive Models"/>
<meta name="twitter:description" content="개요 CS236: Deep Generative Models (Stanford)는 스탠포드 대학교에서 진행하는 딥러닝 기반 생성 모델(Deep Generative Models) 에 대한 심화 강의이다. 이번 포스트에서는 CS236 강의의 3강 내용을 정리한다. 3강에서는 주어진 데이터셋 D의 분포를 학습하여, 이로부터 새로운 샘플을 생성하는 방법을 배운다. 그렇게 하기 위해서 2가지 절차가 진행되어야 한다: Model Family를 파라미터화 (3강에서 다룰 내용) 데이터셋 D에 대하여 모델 파라미터 $\theta$ 를 찾는 방법 (4강에서 다룰 내용) Autoregressive Models $$ \begin{aligned} p(x_1, x_2, \ldots, x_{784}) &amp;= p(x_1),p(x_2 \mid x_1),p(x_3 \mid x_1, x_2), \cdots, p(x_{784} \mid x_1, \ldots, x_{783}) \\ p(x_1, \cdots, x_{784}) &amp;= p_{\text{CPT}}(x_1; \alpha_1) \cdot p_{\text{logit}}(x_2 \mid x_1; \alpha_2) \cdot p_{\text{logit}}(x_3 \mid x_1, x_2; \alpha_3) \cdots p_{\text{logit}}(x_n \mid x_1, \cdots, x_{n-1}; \alpha_n) \\ \end{aligned} $$
Autoregressive Model은 각 픽셀을 이전 픽셀들에 대한 조건부 확률로 예측하는 모델이다. 이러한 조건부 확률을 계산을 하기 위해서 Chain rule factorization을 사용한다. 하지만 이때 모든 확률 조건들을 저장할 수 없다. 따라서 어떤 신경모델, 함수를 사용하여 조건문을 모델링 하려고 한다. 위의 두번째 수식 처럼 logistic regression 함수를 이용한 방법이 그 예시이다. $$ \begin{aligned} p_{\text{CPT}}(X_1 = 1; \alpha_1) = \alpha_1 &amp;, p(X_1 = 0) = 1 - \alpha_1, \\ p_{\text{logit}}(X_2 = 1 \mid x_1; \alpha_2) &amp;= \sigma(\alpha_2^0 &#43; \alpha_2^1 x_1), \\ p_{\text{logit}}(X_3 = 1 \mid x_1, x_2; \alpha_3) &amp;= \sigma(\alpha_3^0 &#43; \alpha_3^1 x_1 &#43; \alpha_3^2 x_2) \end{aligned} $$
MNIST 데이터셋으로 Autoregressive Model을 구성하는 수식을 살펴보게 되면 위의 수식과 같다. 첫번째 픽셀이 black or white인지 조건부 확률 테이블(CPT)에서 값을 받아 배정된다.(보통 black) 그 후 순서에 따라 각 값을 예측하게 된다. Fully Visible Sigmoid Belief Network (FVSBN) 초창기 autoregressive model인 FVSBN은 chain rule을 이용해 확률분포를 나타낸 이후, 많은 컴퓨팅 파워를 요구하는 조건부 확률을 매개변수화한 이후 logistic regression 알고리즘을 적용해 학습하는 생성모델 알고리즘이다. 만약 pixel이 4개라고 가정을 하면, 이전 픽셀에 따라서 값이 다른 것을 확인할 수 있다. 이 모델의 경우 파라미터는 $1&#43;2&#43;3&#43;\cdot\cdot\cdot&#43;n \approx n^2/2$ 인 것도 확인할 수 있다. FVSBN에서 sampling하는 방법이다. 랜덤한 난수를 먼저 생성하고, 그 난수($\bar{x_1}$)로부터 $\bar{x_2}$ 가 나오게 된다. 하지만 FVSBN의 sampling 결과가 좋진 않다. Neural Autoregressive Density Estimation (NADE) NADE는 많은 컴퓨팅 파워를 요구하는 FVSBN의 한계를 보완하기 위해서 logistic regression 대신 Neural Network를 사용한 모델이다. $$ \begin{aligned} h_i &amp;= \sigma(W_{\cdot,&lt;i} x_{&lt;i} &#43; c) \\ \hat{x_i} = p(x_i \mid x_1, \ldots, x_{i-1}) &amp;= \sigma(\alpha^\top h_i &#43; b_i) \end{aligned} $$
위 수식의 $h_i$는 neural network를 의미한다. NADE는 Weight Sharing(Weight tying)기법을 활용하여 계산량을 대폭 줄였다. Weight Sharing은 학습 과정에서 은닉층($h_i$)에서 사용하는 가중치 $w_i$를 동일하게 유지하는 방법이다. 층이 깊어질수록 기존 가중치 벡터 (W)에 열벡터 하나만 추가하는 것으로 생각하면 된다. 이런 결과로 계산 복잡도가 O(n)으로 줄어든 것을 확인할 수 있다. 위 사진이 NADE의 생성 결과인데, 왼쪽이 sample이고 오른쪽이 생성 결과이다. 이미지 구조(분포)를 꽤나 잘 파악하고 있는 것으로 확인할 수 있다. 생성의 작동 방식은 왼쪽 sample에서 픽셀 값을 가져오고 NADE가 계산을 통하여 확률 값이 나오게 된다. 따라서 각 픽셀에 대한 확률 값이기 때문에 오른쪽 샘플의 결과는 0~1사이의 확률값이기때문에 이미지가 부드러워 보인다. 지금까지는 이진 데이터만 다루었다. 하지만 color 이미지와 같은 범주형 변수일 때는 어떻게 작동 할까? $x_i \in {0, \ldots, K}$ 인 다항 변수의 모델링이 필요할 것이다. 다항 변수의 모델링을 하기 위해서 각 $x_i$에 softmax를 취하여 범주형 확률 분포를 만들어주면 기존의 조건부 확률을 계산할 수 있게 된다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/cs236/lecture3/" /><link rel="prev" href="https://goodyoung.github.io/posts/cs236/lecture2/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[CS236] 3. Autoregressive Models",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/cs236\/lecture3\/"
        },"genre": "posts","keywords": "Bayesian network, Conditional Independence, CS236","wordcount":  1012 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/cs236\/lecture3\/","datePublished": "2025-05-07T22:58:36+09:00","dateModified": "2025-05-07T22:58:36+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[CS236] 3. Autoregressive Models</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a>&nbsp;<a href="/categories/deep-generative-models/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Generative Models</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-05-07">2025-05-07</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;1012 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;5 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#autoregressive-models">Autoregressive Models</a></li>
    <li><a href="#fully-visible-sigmoid-belief-network-fvsbn">Fully Visible Sigmoid Belief Network (FVSBN)</a></li>
    <li><a href="#neural-autoregressive-density-estimation-nade">Neural Autoregressive Density Estimation (NADE)</a></li>
    <li><a href="#autoregressive-autoencoders">Autoregressive Autoencoders</a></li>
    <li><a href="#recurrent-neural-nets-rnn">Recurrent Neural Nets (RNN)</a></li>
    <li><a href="#pixelrnn">PixelRNN</a></li>
    <li><a href="#pixelcnn">PixelCNN</a></li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- image format
<div style="text-align:center;">
<img src="/images/cs231n/lecture4/back-1.png" height="100%" width="80%"> </div>
 -->
<h2 id="개요">개요</h2>
<ol start="0">
<li><code>CS236: Deep Generative Models (Stanford)</code>는 스탠포드 대학교에서 진행하는 <strong>딥러닝 기반 생성 모델(Deep Generative Models)</strong> 에 대한 심화 강의이다.</li>
</ol>
<br>
<ol>
<li>이번 포스트에서는 <code>CS236</code> 강의의 3강 내용을 정리한다.</li>
</ol>
<br>
<ol start="2">
<li>3강에서는 주어진 <code>데이터셋 D</code>의 분포를 학습하여, 이로부터 새로운 샘플을 생성하는 방법을 배운다.</li>
</ol>
<ul>
<li>그렇게 하기 위해서 2가지 절차가 진행되어야 한다:
<ul>
<li><code>Model Family</code>를 파라미터화 (3강에서 다룰 내용)</li>
<li><code>데이터셋 D</code>에 대하여 모델 파라미터 $\theta$ 를 찾는 방법 (4강에서 다룰 내용)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="autoregressive-models">Autoregressive Models</h2>
<p>$$
\begin{aligned}
p(x_1, x_2, \ldots, x_{784}) &amp;= p(x_1),p(x_2 \mid x_1),p(x_3 \mid x_1, x_2), \cdots, p(x_{784} \mid x_1, \ldots, x_{783}) \\
p(x_1, \cdots, x_{784}) &amp;= p_{\text{CPT}}(x_1; \alpha_1) \cdot p_{\text{logit}}(x_2 \mid x_1; \alpha_2) \cdot p_{\text{logit}}(x_3 \mid x_1, x_2; \alpha_3) \cdots p_{\text{logit}}(x_n \mid x_1, \cdots, x_{n-1}; \alpha_n) \\
\end{aligned}
$$</p>
<ol start="3">
<li><code>Autoregressive Model</code>은 <code>각 픽셀</code>을 <code>이전 픽셀</code>들에 대한 조건부 확률로 예측하는 모델이다.</li>
</ol>
<br>
<ol start="4">
<li>이러한 조건부 확률을 계산을 하기 위해서 <code>Chain rule factorization</code>을 사용한다. 하지만 이때 모든 확률 조건들을 저장할 수 없다. 따라서 어떤 신경모델, 함수를 사용하여 조건문을 모델링 하려고 한다. <code>위의 두번째 수식</code> 처럼 <code>logistic regression</code> 함수를 이용한 방법이 그 예시이다.</li>
</ol>
<br>
<p>$$
\begin{aligned}
p_{\text{CPT}}(X_1 = 1; \alpha_1) = \alpha_1 &amp;, p(X_1 = 0) = 1 - \alpha_1, \\
p_{\text{logit}}(X_2 = 1 \mid x_1; \alpha_2) &amp;= \sigma(\alpha_2^0 + \alpha_2^1 x_1), \\
p_{\text{logit}}(X_3 = 1 \mid x_1, x_2; \alpha_3) &amp;= \sigma(\alpha_3^0 + \alpha_3^1 x_1 + \alpha_3^2 x_2)
\end{aligned}
$$</p>
<ol start="5">
<li><code>MNIST</code> 데이터셋으로 <code>Autoregressive Model</code>을 구성하는 수식을 살펴보게 되면 위의 수식과 같다. 첫번째 픽셀이 black or white인지 조건부 확률 테이블(CPT)에서 값을 받아 배정된다.(보통 black) 그 후 순서에 따라 각 값을 예측하게 된다.</li>
</ol>
<hr>
<h2 id="fully-visible-sigmoid-belief-network-fvsbn">Fully Visible Sigmoid Belief Network (FVSBN)</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/fvsbn.png" height="70%" width="70%"> </div>
<ol start="6">
<li>초창기 <code>autoregressive model</code>인 <code>FVSBN</code>은 <code>chain rule</code>을 이용해 확률분포를 나타낸 이후, 많은 컴퓨팅 파워를 요구하는 조건부 확률을 매개변수화한 이후 <code>logistic regression</code> 알고리즘을 적용해 학습하는 생성모델 알고리즘이다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/fvsbn-1.png" height="85%" width="80%"> </div>
<ol start="7">
<li>만약 pixel이 4개라고 가정을 하면, 이전 픽셀에 따라서 값이 다른 것을 확인할 수 있다.</li>
</ol>
<br>
<ol start="8">
<li>이 모델의 경우 파라미터는 $1+2+3+\cdot\cdot\cdot+n \approx n^2/2$ 인 것도 확인할 수 있다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/fvsbn-2.png" height="85%" width="80%"> </div>
<ol start="9">
<li><code>FVSBN</code>에서 sampling하는 방법이다. 랜덤한 난수를 먼저 생성하고, 그 난수($\bar{x_1}$)로부터 $\bar{x_2}$ 가 나오게 된다.</li>
</ol>
<ul>
<li>하지만 <code>FVSBN</code>의 sampling 결과가 좋진 않다.</li>
</ul>
<hr>
<h2 id="neural-autoregressive-density-estimation-nade">Neural Autoregressive Density Estimation (NADE)</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/nade.png" height="40%" width="40%"> </div>
<ol start="10">
<li><code>NADE</code>는 많은 컴퓨팅 파워를 요구하는 <code>FVSBN</code>의 한계를 보완하기 위해서 <code>logistic regression</code> 대신 <code>Neural Network</code>를 사용한 모델이다.</li>
</ol>
<br>
<p>$$
\begin{aligned}
h_i &amp;= \sigma(W_{\cdot,&lt;i} x_{&lt;i} + c) \\
\hat{x_i} = p(x_i \mid x_1, \ldots, x_{i-1}) &amp;= \sigma(\alpha^\top h_i + b_i)
\end{aligned}
$$</p>
<ol start="11">
<li><code>위 수식</code>의  $h_i$는 <code>neural network</code>를 의미한다. <code>NADE</code>는 <code>Weight Sharing(Weight tying)</code>기법을 활용하여 계산량을 대폭 줄였다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/nade-1.png" height="85%" width="85%"> </div>
<ol start="12">
<li><code>Weight Sharing</code>은 학습 과정에서 은닉층($h_i$)에서 사용하는 가중치 $w_i$를 동일하게 유지하는 방법이다.</li>
</ol>
<ul>
<li>층이 깊어질수록 기존 가중치 벡터 (W)에 <strong>열벡터 하나만 추가하는</strong> 것으로 생각하면 된다.</li>
</ul>
<br>
<ol start="13">
<li>이런 결과로 계산 복잡도가 <code>O(n)</code>으로 줄어든 것을 확인할 수 있다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/nade-1.png" height="85%" width="85%"> </div>
<ol start="14">
<li><code>위 사진</code>이 <code>NADE</code>의 생성 결과인데, <code>왼쪽이 sample</code>이고 <code>오른쪽이 생성 결과</code>이다. 이미지 구조(분포)를 꽤나 잘 파악하고 있는 것으로 확인할 수 있다.</li>
</ol>
<br>
<ol start="15">
<li><code>생성</code>의 작동 방식은 <code>왼쪽 sample</code>에서 <code>픽셀 값</code>을 가져오고 <code>NADE</code>가 계산을 통하여 확률 값이 나오게 된다. 따라서 각 픽셀에 대한 확률 값이기 때문에 오른쪽 샘플의 결과는 0~1사이의 확률값이기때문에 이미지가 부드러워 보인다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/nade-3.png" height="85%" width="85%"> </div>
<ol start="16">
<li>지금까지는 <code>이진 데이터</code>만 다루었다. 하지만 color 이미지와 같은 <code>범주형 변수일</code> 때는 어떻게 작동 할까? $x_i \in {0, \ldots, K}$ 인 다항 변수의 모델링이 필요할 것이다.</li>
</ol>
<br>
<ol start="17">
<li>다항 변수의 모델링을 하기 위해서 각 $x_i$에 <strong>softmax</strong>를 취하여 <strong>범주형 확률 분포</strong>를 만들어주면 기존의 조건부 확률을 계산할 수 있게 된다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/nade-5.png" height="85%" width="85%"> </div>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/nade-4.png" height="85%" width="85%"> </div>
<ol start="18">
<li>그렇다면 연속형 변수일 때는 어떻게 작동 하는지 알아 볼 것이다. $x_i \in \mathbb{R}$ 를 모델링 하기 위하여 <code>Gaussian Mixture</code>를 사용한다.</li>
</ol>
<br>
<ol start="19">
<li>따라서 각 $x_i$에 대하여 K개의 평균, 분산을 예측하여 <strong>연속 확률 분포</strong>를 얻을 수 있게 된다.</li>
</ol>
<ul>
<li>이 평균, 분산을 얻을 땐 신경망을 사용하게 된다.</li>
</ul>
<hr>
<h2 id="autoregressive-autoencoders">Autoregressive Autoencoders</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/ae.png" height="85%" width="85%"> </div>
<ol start="20">
<li><code>Autoregressive Model</code>은 <code>Autoencoder</code>와 유사하게 보일 수 있다.</li>
</ol>
<br>
<ol start="21">
<li>하지만 <code>Autoencoder</code>는 데이터의 분포를 정의하는 생성 모델이 아니기 때문에 <code>Autoregressive model</code>과 차이가 있다.</li>
</ol>
<br>
<ol start="22">
<li><code>Autoregressive model</code>은 시점 $i$ 이전 데이터만을 가지고 데이터 $x_i$를 추정하지만, 기존의 autoencoder는 전체 데이터의 특징을 학습을 한다. 이러한 <code>Autoencoder</code>를 생성 모델로 만들기 위한 방법이 있다. 바로 데이터에 대한 <strong>입력 순서를 강제로 정의</strong>하여 각 출력이 이전 값에만 의존하게 만드는 방법이다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/made.png" height="85%" width="85%"> </div>
<ol start="23">
<li><code>Autoencoder</code>에서 입력 순서를 강제로 정의하는 방법은 일부 weight을 0으로 설정하는 <code>Masking 방법</code>이 있다. <code>Masked Autoencoder for Distribution Estimation (MADE)</code>은 이러한 방식을 통해 오토인코더 구조를 가지고 확률 분포를 예측하는 <code>autoregressive model</code>이다.</li>
</ol>
<br>
<ol start="24">
<li>각 출력 유닛이 <strong>자신보다 앞선 입력에만 연결</strong>되도록 설정한다. 그렇게 되면 <code>단일 네트워크(single pass)</code>로 조건부 확률을 계산하면서 생성 모델의 역할을 수행할 수 있다.</li>
</ol>
<ul>
<li>다음은 <code>MADE</code>의 자세한 작동 방식이다:
<ol>
<li>각 은닉 노드에 정수 마스크 값 (1부터 n-1까지)을 랜덤한 숫자를 배정한다.</li>
<li>부모 노드의 숫자가 자식 노드의 숫자보다 작거나 같으면 ($&lt;= i$) 연결을 유지한다.</li>
<li>부모 노드의 숫자가 자식 노드의 숫자보다 크면 ($&gt; i$) 연결을 제거한다.</li>
</ol>
</li>
</ul>
<br>
<ol start="25">
<li>이 방식으로 전체 네트워크는 <code>DAG(Directed Acyclic Graph)</code> 구조를 가지게 되어 autoregressive 구조가 성립된다.</li>
</ol>
<hr>
<h2 id="recurrent-neural-nets-rnn">Recurrent Neural Nets (RNN)</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/rnn.png" height="85%" width="85%"> </div>
<ol start="26">
<li>지금까지의 <code>autoregressive model</code>은 시점 $i$ 이전의 데이터를 다 가지고 있으면서 예측에 활용 했어야 했다. 이렇게 되면 history가 길어질 수록 수식이 길어지는 단점이 있다. 이런 문제를 해결하고자 <strong>시간 순서에 따라 정보를 압축</strong>하여 <strong>유지하고 반복하여 업데이트</strong> 하는 <code>RNN</code> 구조가 나오게 되었다.</li>
</ol>
<br>
<ol start="27">
<li>
<p>$h_\text{t+1}$ 에서 이전 입력들을 요약 및 업데이트하고, 이것($h_\text{t+1}$)을 가지고 출력 $o_\text{t+1}$ 을 예측한다. 또 예측한 $o_\text{t+1}$ 를 다음 입력으로 사용한다.</p>
<img src="/images/cs236/lecture3/rnn-1.png" height="60%" width="60%">
<ul>
<li><code>rnn</code>의 행렬 계산을 이해하기 쉽게 정리 한 것이다.</li>
<li><a href="https://velog.io/@sujeongim/NLP-RNN-LSTM-GRU" target="_blank" rel="noopener noreffer ">출처</a></li>
</ul>
</li>
</ol>
<br>
<ol start="28">
<li>이런 <code>rnn</code>을 사용하면 입력 길이가 고정되지 않아도 된다. 또한 컴퓨팅의 비용이 상당히 줄어드는 장점이 있다. 하지만 <code>long-term dependency</code>의 문제가 있고, 여전히 <strong>입력 순서</strong>는 필요하다는 단점이 있다. 또한 훈련, 생성시 속도가 <strong>느리다는 단점</strong>이 있다.</li>
</ol>
<ul>
<li>이러한 단점을 해결하기위하여 attention based 모델이 등장했다.</li>
</ul>
<hr>
<h2 id="pixelrnn">PixelRNN</h2>
<ol start="29">
<li><code>Autoregressive</code>의 방법은 이미지 분야에도 적용이 되었다. 바로 <code>PixelRNN</code>이다. $p(x_t \mid x_{1:t-1})$ 를 예측하기 위하여  아래 수식과 같이 <strong>R,G,B의 세개의 값을 예측을 해야한다.</strong></li>
</ol>
<br>
<p>$$
p(x_t \mid x_{1:t-1}) = p(x_t^{\text{red}} \mid x_{1:t-1}) \cdot p(x_t^{\text{green}} \mid x_{1:t-1}, x_t^{\text{red}}) \cdot p(x_t^{\text{blue}} \mid x_{1:t-1}, x_t^{\text{red}}, x_t^{\text{green}})
$$</p>
<ul>
<li>해당 모델 결과를 보면, 모델이 이미지의 구조를 어느정도 파악하고 있음을 알 수 있다.</li>
</ul>
<hr>
<h2 id="pixelcnn">PixelCNN</h2>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/pixelcnn.png" height="85%" width="85%"> </div>
<ol start="30">
<li><code>CNN구조</code>에 <code>autoregresive</code>를 적용시킨 모델도 있다. 바로 <code>PixelCNN</code>이다. 이 모델은 CNN 구조를 사용하면서 이웃 픽셀들의 정보를 가지고 다음 pixel을 예측하는 방법이다.</li>
</ol>
<br>
<ol start="31">
<li>하지만 <code>autoregressive</code>하기 위하여 <code>MADE</code>방법 처럼 masked convolution을 사용하여 예측에 시도하는 방법이다.</li>
</ol>
<hr>
<h2 id="summary">Summary</h2>
<ol start="33">
<li><code>생성 모델</code>은 주어진 데이터셋 $D$로부터 데이터가 생성되는 확률 분포 $p_\text{data}$를 추정하여, 이 분포로부터 가능한 데이터 샘플 $x \sim p_\text{data}(x)$을 생성하는 방식이다</li>
</ol>
<br>
<ol start="34">
<li>그러나 실제로 $p_\text{data}$를 <strong>직접 구하는 것은 불가능하므로</strong>, 모델은 이와 유사한 분포를 나타내는 파라미터 $\theta$를 학습하여 근사 분포 $p_\theta(x)$ 를 모델링하는 방식(여기서 여러 생성 알고리즘이 등장)으로 작동한다.</li>
</ol>
<ul>
<li>이때 $p_\theta(x)$ 로부터 생성한 데이터 $\hat{x}$ 가 실제 데이터의 분포에서 생성한 데이터 x와 유사하게 설정할 수 있게 되는 것이다.</li>
</ul>
<br>
<ol start="35">
<li><code>Autoregressive model</code>은 $p_\theta(x)$ 를 <strong>연쇄 법칙</strong>을 통해 $i$ 시점 데이터 $x_i$ 의 가능도(확률) $p(x_i)$ 는 이전 시점까지의($i-1$) 데이터를 바탕으로 예측하는 조건부 확률 형태를 나타낸다.</li>
</ol>
<br>
<ol start="36">
<li>이런 <code>Autoregressive model</code>이 샘플링하고, 확률을 계산하는 방법은 아래와 같다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs236/lecture3/summary.png" height="85%" width="85%"> </div>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://deepgenerativemodels.github.io/" target="_blank" rel="noopener noreffer ">https://deepgenerativemodels.github.io/</a></li>
<li><a href="https://www.youtube.com/watch?v=tRArbBf-AbI&amp;list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&amp;index=3&amp;t=4s" target="_blank" rel="noopener noreffer ">https://www.youtube.com/watch?v=tRArbBf-AbI&list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&index=3&t=4s</a></li>
<li><a href="https://wikidocs.net/228835" target="_blank" rel="noopener noreffer ">https://wikidocs.net/228835</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-05-07</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/cs236/lecture3/" data-title="[CS236] 3. Autoregressive Models" data-hashtags="Bayesian network,Conditional Independence,CS236"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/cs236/lecture3/" data-hashtag="Bayesian network"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/cs236/lecture3/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/cs236/lecture3/" data-title="[CS236] 3. Autoregressive Models"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/cs236/lecture3/" data-title="[CS236] 3. Autoregressive Models"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/cs236/lecture3/" data-title="[CS236] 3. Autoregressive Models"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/cs236/lecture3/" data-title="[CS236] 3. Autoregressive Models" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/bayesian-network/">Bayesian network</a>,&nbsp;<a href="/tags/conditional-independence/">Conditional Independence</a>,&nbsp;<a href="/tags/cs236/">CS236</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/cs236/lecture2/" class="prev" rel="prev" title="[CS236] 2. Background"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[CS236] 2. Background</a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
