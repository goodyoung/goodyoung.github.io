<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>[CS231n] 09.CNN Architectures - Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="[CS231n] 09.CNN Architectures" />
<meta property="og:description" content="개요 CS231n의 9강에 대한 내용을 정리 할 것이다. 9강에 나오는 CNN Architecture중에 GoogLeNet과 ResNet에 대해서 살펴볼 예정이다. GoogLeNet 이때부터 이제 Network를 깊게 효율적을 만들기 시작했다. Network가 깊어질수록 학습하는데 걸리는 컴퓨팅 시간이 많이 걸린다. 따라서 Network를 깊고 효율적으로 만들기 위해서 GoogLeNet에서는 Inception module을 추가하였다. Inception Module이란 로컬 네트워크의 한 유형이다. 이는 네트워크의 네트워크 (Network In Network(NiN))의 개념에 바탕을 두고 있다.
Inception Module은 위 그림과 같이 병렬로 서로 다른 크기의 filter를 병렬로 돌려 차원을 줄여주는 것이다. 서로 다른 크기의 filter가 있으므로 여러 특징을 추출하는 효과를 가진다. 위 그림은 naive Inception module이다. Filter들의 stride와 padding을 통하여 입력과 출력 차원을 일치시키고 depth를 높였다. 이 module의 문제는 computational complexity이다. Pooling layer가 depth를 유지하기 때문에 every layer에서 전체적인 depth가 깊어진다. 이를 해결하려고 boottleneck layer가 나온다. 33, 55 filter에 들어가기 전 1*1 filter를 사용하여 depth를 줄인다. 따라서 기존과 확연히 다른 연산을 수행하는 것을 확인할 수 있게 된다. 1*1 를 사용하면 정보 손실이 발생할 수 있지만 이러한 예측을 수행하는 경우 이들의 조합을 계산하여 추가적으로 비선형성을 도입하므로 이를 보완할 수 있다. 또한 GoogLeNet에서는 auxiliary classification output이 있다. To inject additional gradient at lower layers Gradient가 잘 전달이 되지 않는 깊은 Network에서 중간 layer도 도움이 된다. 깊은 네트워크 때문에 gradient vanishing 현상을 극복하려고 inject를 한다. 이는 메인 네트워크의 최종 손실과 함께 결합되어 최종 학습 과정에 기여하게 된다. ResNet 다음은 2015년에 ILSVRC에서 우승을 한 ResNet이다. 이는 revolution of Depth인 만큼 많이 깊은 network이다. 엄청 깊게 CNN을 쌓으면 더 나은 결과가 나오나 싶지만 결과는 아니였다. Train시 overfitting이 예상이 되어 오류가 아주 적을 것이라 예상을 했지만 그것의 문제가 아니다. 이는 모델 자체는 학습이 이뤄지고 있어서 vanishing gradient문제가 아닌 degradation problem이다. 이는 optimization problem이다. 적어도 깊은 layer는 shallower의 성능은 기본적으로 있어야 하지만 그 조차도 아니다. 깊은 layer는 shallower model보다 최적화 하기 어렵다. 이걸 해결하려고 residual connection이 나왔다. 이는 아주 간단하게 입력을 출력에 더하는 아이디어 이런 단순한 아이디어 이다. 모델이 학습을 할 때 $H(x)$를 하는 것이 아니라 x를 뺀 나머지(잔차 $F(x)$)를 학습을 하게 된다. 이를 더 자세히 이해하자면 학습이 많이 될수록 x는 점점 출력값 $H(x)$에 근접하게 된다. 이 residual connection에서는 x가 identity mapping이 되기 때문에 $H(x) = F(x) &#43; x$에서 추가 학습량 F(x)는 점점 작아져서 최종적으로 0에 근접하는 최소값으로 수렴되어야 할 것이다. 이렇기 때문에 함수 전체를 학습하는 대신, 더 단순한 잔차만 학습하게 되므로 residual connection은 학습 과정이 더 빠르고 쉽고 깊은 네트워크에서 안정적이다. 이를 수식적인 이해를 한다면 아래와 같다. Residual connection이 있는 신경망에서 아래 같이 표현을 할 수 있다. $$y = F(x) &#43; x$$
기본 신경망에서는 손실 함수 $L$은 체인 룰을 통해 gradient update를 하게 된다. $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$
Residual connection이 있을 때 기울기 전파는 아래와 같다.
$$y = F(x) &#43; x$$
$$\frac{\partial y} {\partial x} = \frac{\partial (F(x) &#43; x)}{\partial x}$$
$\frac{\partial x}{\partial x} = 1$ 이므로 다음과 같이 표현할 수 있다. $$\frac{\partial y}{\partial x} = \frac{\partial F(x)}{\partial x} &#43; 1$$
결국, gradient전파는 다음과 같이 계산이 된다. $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot (\frac{\partial F(x)}{\partial x} &#43; 1)$$
Residual connection이 없는 경우, $(\frac{\partial F(x)}{\partial x})$가 매우 작아지면 기울기 소실 문제가 발생할 수 있다. 하지만, Residual connection이 있는 경우, $(\frac{\partial F(x)}{\partial x} &#43; 1)$의 형태로 계산되므로, $(\frac{\partial F(x)}{\partial x})$가 0에 가까워지더라도, 항상 1이 더해지기 때문에 기울기가 완전히 사라지지 않게 되어 vanishing gradient 현상도 해결이 될 수 있다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://goodyoung.github.io/posts/cs231n/lecture9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-30T22:58:52+09:00" />
<meta property="article:modified_time" content="2024-07-30T22:58:52+09:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="[CS231n] 09.CNN Architectures"/>
<meta name="twitter:description" content="개요 CS231n의 9강에 대한 내용을 정리 할 것이다. 9강에 나오는 CNN Architecture중에 GoogLeNet과 ResNet에 대해서 살펴볼 예정이다. GoogLeNet 이때부터 이제 Network를 깊게 효율적을 만들기 시작했다. Network가 깊어질수록 학습하는데 걸리는 컴퓨팅 시간이 많이 걸린다. 따라서 Network를 깊고 효율적으로 만들기 위해서 GoogLeNet에서는 Inception module을 추가하였다. Inception Module이란 로컬 네트워크의 한 유형이다. 이는 네트워크의 네트워크 (Network In Network(NiN))의 개념에 바탕을 두고 있다.
Inception Module은 위 그림과 같이 병렬로 서로 다른 크기의 filter를 병렬로 돌려 차원을 줄여주는 것이다. 서로 다른 크기의 filter가 있으므로 여러 특징을 추출하는 효과를 가진다. 위 그림은 naive Inception module이다. Filter들의 stride와 padding을 통하여 입력과 출력 차원을 일치시키고 depth를 높였다. 이 module의 문제는 computational complexity이다. Pooling layer가 depth를 유지하기 때문에 every layer에서 전체적인 depth가 깊어진다. 이를 해결하려고 boottleneck layer가 나온다. 33, 55 filter에 들어가기 전 1*1 filter를 사용하여 depth를 줄인다. 따라서 기존과 확연히 다른 연산을 수행하는 것을 확인할 수 있게 된다. 1*1 를 사용하면 정보 손실이 발생할 수 있지만 이러한 예측을 수행하는 경우 이들의 조합을 계산하여 추가적으로 비선형성을 도입하므로 이를 보완할 수 있다. 또한 GoogLeNet에서는 auxiliary classification output이 있다. To inject additional gradient at lower layers Gradient가 잘 전달이 되지 않는 깊은 Network에서 중간 layer도 도움이 된다. 깊은 네트워크 때문에 gradient vanishing 현상을 극복하려고 inject를 한다. 이는 메인 네트워크의 최종 손실과 함께 결합되어 최종 학습 과정에 기여하게 된다. ResNet 다음은 2015년에 ILSVRC에서 우승을 한 ResNet이다. 이는 revolution of Depth인 만큼 많이 깊은 network이다. 엄청 깊게 CNN을 쌓으면 더 나은 결과가 나오나 싶지만 결과는 아니였다. Train시 overfitting이 예상이 되어 오류가 아주 적을 것이라 예상을 했지만 그것의 문제가 아니다. 이는 모델 자체는 학습이 이뤄지고 있어서 vanishing gradient문제가 아닌 degradation problem이다. 이는 optimization problem이다. 적어도 깊은 layer는 shallower의 성능은 기본적으로 있어야 하지만 그 조차도 아니다. 깊은 layer는 shallower model보다 최적화 하기 어렵다. 이걸 해결하려고 residual connection이 나왔다. 이는 아주 간단하게 입력을 출력에 더하는 아이디어 이런 단순한 아이디어 이다. 모델이 학습을 할 때 $H(x)$를 하는 것이 아니라 x를 뺀 나머지(잔차 $F(x)$)를 학습을 하게 된다. 이를 더 자세히 이해하자면 학습이 많이 될수록 x는 점점 출력값 $H(x)$에 근접하게 된다. 이 residual connection에서는 x가 identity mapping이 되기 때문에 $H(x) = F(x) &#43; x$에서 추가 학습량 F(x)는 점점 작아져서 최종적으로 0에 근접하는 최소값으로 수렴되어야 할 것이다. 이렇기 때문에 함수 전체를 학습하는 대신, 더 단순한 잔차만 학습하게 되므로 residual connection은 학습 과정이 더 빠르고 쉽고 깊은 네트워크에서 안정적이다. 이를 수식적인 이해를 한다면 아래와 같다. Residual connection이 있는 신경망에서 아래 같이 표현을 할 수 있다. $$y = F(x) &#43; x$$
기본 신경망에서는 손실 함수 $L$은 체인 룰을 통해 gradient update를 하게 된다. $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$
Residual connection이 있을 때 기울기 전파는 아래와 같다.
$$y = F(x) &#43; x$$
$$\frac{\partial y} {\partial x} = \frac{\partial (F(x) &#43; x)}{\partial x}$$
$\frac{\partial x}{\partial x} = 1$ 이므로 다음과 같이 표현할 수 있다. $$\frac{\partial y}{\partial x} = \frac{\partial F(x)}{\partial x} &#43; 1$$
결국, gradient전파는 다음과 같이 계산이 된다. $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot (\frac{\partial F(x)}{\partial x} &#43; 1)$$
Residual connection이 없는 경우, $(\frac{\partial F(x)}{\partial x})$가 매우 작아지면 기울기 소실 문제가 발생할 수 있다. 하지만, Residual connection이 있는 경우, $(\frac{\partial F(x)}{\partial x} &#43; 1)$의 형태로 계산되므로, $(\frac{\partial F(x)}{\partial x})$가 0에 가까워지더라도, 항상 1이 더해지기 때문에 기울기가 완전히 사라지지 않게 되어 vanishing gradient 현상도 해결이 될 수 있다."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/posts/cs231n/lecture9/" /><link rel="prev" href="https://goodyoung.github.io/posts/paper/transformer/" /><link rel="next" href="https://goodyoung.github.io/posts/paper/vit/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[CS231n] 09.CNN Architectures",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/goodyoung.github.io\/posts\/cs231n\/lecture9\/"
        },"genre": "posts","keywords": "GoogLenet, ResNet, Neural Networks, CS231n","wordcount":  543 ,
        "url": "https:\/\/goodyoung.github.io\/posts\/cs231n\/lecture9\/","datePublished": "2024-07-30T22:58:52+09:00","dateModified": "2024-07-30T22:58:52+09:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[CS231n] 09.CNN Architectures</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-07-30">2024-07-30</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;543 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;3 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#googlenet">GoogLeNet</a></li>
    <li><a href="#resnet">ResNet</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- image format
<div style="text-align:center;">
<img src="/images/cs231n/lecture4/back-1.png" height="100%" width="80%"> </div>
 -->
<h2 id="개요">개요</h2>
<ol start="0">
<li><code>CS231n</code>의 9강에 대한 내용을 정리 할 것이다.</li>
</ol>
<br>
<ol>
<li>9강에 나오는 <code>CNN Architecture</code>중에 <code>GoogLeNet</code>과 <code>ResNet</code>에 대해서 살펴볼 예정이다.</li>
</ol>
<h2 id="googlenet">GoogLeNet</h2>
<ol start="2">
<li>이때부터 이제 <code>Network</code>를 깊게 효율적을 만들기 시작했다.</li>
</ol>
<br>
<ol start="3">
<li><code>Network</code>가 깊어질수록 학습하는데 걸리는 컴퓨팅 시간이 많이 걸린다. 따라서 <code>Network</code>를 깊고 효율적으로 만들기 위해서 <code>GoogLeNet</code>에서는 <code>Inception module</code>을 추가하였다.</li>
</ol>
<div style="text-align:center;">
<img src="/images/cs231n/lecture9/inception-1.png" height="100%" width="80%"> </div>
<ol start="4">
<li>
<p><code>Inception Module</code>이란 로컬 네트워크의 한 유형이다. 이는 네트워크의 네트워크 (Network In Network(NiN))의 개념에 바탕을 두고 있다.</p>
 <img src="/images/cs231n/lecture9/nin.png" height="100%" width="80%">
</li>
</ol>
<br>
<ol start="5">
<li><code>Inception Module</code>은 <code>위 그림</code>과 같이 병렬로 <strong>서로 다른 크기의 filter를 병렬로</strong> 돌려 차원을 줄여주는 것이다.</li>
</ol>
<br>
<ol start="6">
<li>서로 다른 크기의 filter가 있으므로 여러 특징을 추출하는 효과를 가진다.</li>
</ol>
<br>
<ol start="7">
<li>위 그림은 <code>naive Inception module</code>이다. Filter들의 <code>stride</code>와 <code>padding</code>을 통하여 입력과 출력 차원을 일치시키고 depth를 높였다.</li>
</ol>
<br>
<ol start="8">
<li>이 module의 문제는 <code>computational complexity</code>이다.
<ul>
<li>Pooling layer가 <strong>depth를 유지하기 때문에</strong> every layer에서 <strong>전체적인 depth가 깊어진다.</strong></li>
</ul>
</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs231n/lecture9/inception-2.png" height="100%" width="80%"> </div>
<ol start="9">
<li>이를 해결하려고 <code>boottleneck layer</code>가 나온다.
<ul>
<li>3<em>3, 5</em>5 filter에 들어가기 전 <strong>1*1 filter를 사용하여 depth를 줄인다.</strong></li>
</ul>
</li>
</ol>
<br>
<ol start="10">
<li>따라서 기존과 확연히 다른 연산을 수행하는 것을 확인할 수 있게 된다.</li>
</ol>
<br>
<ol start="11">
<li>1*1 를 사용하면 정보 손실이 발생할 수 있지만 이러한 예측을 수행하는 경우 이들의 조합을 계산하여 추가적으로 비선형성을 도입하므로 이를 보완할 수 있다.</li>
</ol>
<br>
<div style="text-align:center;">
<img src="/images/cs231n/lecture9/aux.png" height="100%" width="80%"> </div>
<ol start="12">
<li>또한 <code>GoogLeNet</code>에서는 <code>auxiliary classification output</code>이 있다.
<ul>
<li><strong>To inject additional gradient at lower layers</strong>
<ul>
<li><code>Gradient</code>가 잘 전달이 되지 않는 깊은 <code>Network</code>에서 중간 layer도 도움이 된다.</li>
<li>깊은 네트워크 때문에 <code>gradient vanishing</code> 현상을 극복하려고 <strong>inject를 한다.</strong></li>
</ul>
</li>
</ul>
</li>
</ol>
<br>
<ol start="13">
<li>이는 메인 네트워크의 <strong>최종 손실과 함께 결합되어 최종 학습 과정에 기여하게 된다.</strong></li>
</ol>
<hr>
<h2 id="resnet">ResNet</h2>
<ol start="14">
<li>다음은 2015년에 <code>ILSVRC</code>에서 우승을 한 <code>ResNet</code>이다. 이는 <code>revolution of Depth</code>인 만큼 <strong>많이 깊은 network이다.</strong></li>
</ol>
<div style="text-align:center;">
<img src="/images/cs231n/lecture9/result.png" height="100%" width="80%"> </div>
<ol start="15">
<li>엄청 깊게 <code>CNN</code>을 쌓으면 더 나은 결과가 나오나 싶지만 결과는 아니였다.</li>
</ol>
<br>
<ol start="16">
<li>Train시 <code>overfitting</code>이 예상이 되어 <strong>오류가 아주 적을 것이라 예상을 했지만</strong> 그것의 문제가 아니다.
<ul>
<li>이는 모델 자체는 학습이 이뤄지고 있어서 <code>vanishing gradient</code>문제가 아닌 <code>degradation problem</code>이다.</li>
</ul>
</li>
</ol>
<br>
<ol start="17">
<li>이는 <code>optimization problem</code>이다. 적어도 깊은 layer는 <strong>shallower의 성능은 기본적으로 있어야</strong> 하지만 그 조차도 아니다.
<ul>
<li>깊은 layer는 shallower model보다 최적화 하기 어렵다.</li>
</ul>
</li>
</ol>
<div style="text-align:center;">
<img src="/images/cs231n/lecture9/residual.png" height="100%" width="80%"> </div>
<ol start="18">
<li>이걸 해결하려고 <code>residual connection</code>이 나왔다.</li>
</ol>
<br>
<ol start="19">
<li>이는 아주 간단하게 <strong>입력을 출력에 더하는 아이디어</strong> 이런 단순한 아이디어 이다.</li>
</ol>
<br>
<ol start="20">
<li>모델이 학습을 할 때 $H(x)$를 하는 것이 아니라 x를 뺀 나머지(잔차 $F(x)$)를 학습을 하게 된다.</li>
</ol>
<br>
<ol start="21">
<li>이를 더 자세히 이해하자면 학습이 많이 될수록 <strong>x는 점점 출력값 $H(x)$에 근접하게 된다.</strong></li>
</ol>
<br>
<ol start="22">
<li>이 <code>residual connection</code>에서는 x가 <code>identity mapping</code>이 되기 때문에 $H(x) = F(x) + x$에서 추가 학습량 F(x)는 <strong>점점 작아져서</strong> 최종적으로 <strong>0에 근접하는 최소값으로 수렴되어야 할 것이다.</strong></li>
</ol>
<br>
<ol start="23">
<li>이렇기 때문에 함수 전체를 학습하는 대신, 더 단순한 잔차만 학습하게 되므로 <code>residual connection</code>은 <strong>학습 과정이 더 빠르고 쉽고 깊은 네트워크에서 안정적이다.</strong></li>
</ol>
<br>
<ol start="23">
<li>이를 수식적인 이해를 한다면 아래와 같다.</li>
</ol>
<br>
<ol start="24">
<li><code>Residual connection</code>이 있는 신경망에서 아래 같이 표현을 할 수 있다.</li>
</ol>
<p>$$y = F(x) + x$$</p>
<ol start="25">
<li>
<p>기본 신경망에서는 손실 함수 $L$은 체인 룰을 통해 gradient update를 하게 된다.
$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$</p>
</li>
<li>
<p><code>Residual connection</code>이 있을 때 기울기 전파는 아래와 같다.</p>
</li>
</ol>
<p>$$y = F(x) + x$$</p>
<br>
<p>$$\frac{\partial y} {\partial x} = \frac{\partial (F(x) + x)}{\partial x}$$</p>
<ol start="27">
<li>$\frac{\partial x}{\partial x} = 1$ 이므로 다음과 같이 표현할 수 있다.</li>
</ol>
<p>$$\frac{\partial y}{\partial x} = \frac{\partial F(x)}{\partial x} + 1$$</p>
<ol start="28">
<li>결국, <code>gradient</code>전파는 다음과 같이 계산이 된다.</li>
</ol>
<p>$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot (\frac{\partial F(x)}{\partial x} + 1)$$</p>
<br>
<ol start="29">
<li><code>Residual connection</code>이 없는 경우, $(\frac{\partial F(x)}{\partial x})$가 매우 작아지면 기울기 소실 문제가 발생할 수 있다.</li>
</ol>
<br>
<ol start="30">
<li>하지만, <code>Residual connection</code>이 있는 경우, $(\frac{\partial F(x)}{\partial x} + 1)$의 형태로 계산되므로, $(\frac{\partial F(x)}{\partial x})$가 <strong>0에 가까워지더라도</strong>, <strong>항상 1이 더해지기 때문에</strong> 기울기가 완전히 사라지지 않게 되어 <code>vanishing gradient</code> 현상도 해결이 될 수 있다.</li>
</ol>
<br>
<ol start="31">
<li>즉, <code>Residual connection</code>을 통해 기울기가 더 안정적으로 전파될 수 있으며, 이는 <strong>깊은 신경망에서도 학습이 잘 이루어지도록 도와주게 된다.</strong></li>
</ol>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://channelai.tistory.com/2" target="_blank" rel="noopener noreffer ">https://channelai.tistory.com/2</a></li>
<li><a href="https://velog.io/@lighthouse97/ResNet%EC%9D%98-%EC%9D%B4%ED%95%B4" target="_blank" rel="noopener noreffer ">https://velog.io/@lighthouse97/ResNet%EC%9D%98-%EC%9D%B4%ED%95%B4</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2024-07-30</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://goodyoung.github.io/posts/cs231n/lecture9/" data-title="[CS231n] 09.CNN Architectures" data-hashtags="GoogLenet,ResNet,Neural Networks,CS231n"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://goodyoung.github.io/posts/cs231n/lecture9/" data-hashtag="GoogLenet"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://goodyoung.github.io/posts/cs231n/lecture9/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://goodyoung.github.io/posts/cs231n/lecture9/" data-title="[CS231n] 09.CNN Architectures"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://goodyoung.github.io/posts/cs231n/lecture9/" data-title="[CS231n] 09.CNN Architectures"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://goodyoung.github.io/posts/cs231n/lecture9/" data-title="[CS231n] 09.CNN Architectures"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://goodyoung.github.io/posts/cs231n/lecture9/" data-title="[CS231n] 09.CNN Architectures" data-description=""><i class="fab fa-blogger fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/googlenet/">GoogLenet</a>,&nbsp;<a href="/tags/resnet/">ResNet</a>,&nbsp;<a href="/tags/neural-networks/">Neural Networks</a>,&nbsp;<a href="/tags/cs231n/">CS231n</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/paper/transformer/" class="prev" rel="prev" title="[Paper Review]Attention Is All You Need"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>[Paper Review]Attention Is All You Need</a>
            <a href="/posts/paper/vit/" class="next" rel="next" title="[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)">[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"Comment","lightTheme":"github-light","repo":"goodylung/blog-comment"}},"lightgallery":true};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
