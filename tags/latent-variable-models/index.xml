<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Latent Variable Models - Tag - Good Young</title>
        <link>https://goodyoung.github.io/tags/latent-variable-models/</link>
        <description>Latent Variable Models - Tag - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 26 Jul 2025 14:23:31 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/tags/latent-variable-models/" rel="self" type="application/rss+xml" /><item>
    <title>[CS236] 6. Latent Variable Models-2</title>
    <link>https://goodyoung.github.io/posts/cs236/lecture6/</link>
    <pubDate>Sat, 26 Jul 2025 14:23:31 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs236/lecture6/</guid>
    <description><![CDATA[개요 이번 포스트에서는 CS236 강의의 6강 내용과 7강의 앞부분을 설명한다. 7강에 VAE 내용이 포함이 되었기 때문이다. 지난 포스트에서 Latent Variable Model에서 z라는 latent variable이 여러개 있을 때 Mixture of Gaussian이 된다는 점, 그 수많은 분포에서 x의 값을 구하는 방법(marginal likelihood) 등에 대해서 배웠고 그 분포를 최적화 하는 과정에서 Evidence Lower Bound(ELBO) 개념이 나오게 되었다. ELBO 개념이 나오면서, 수식들을 전개했는데 이어서 설명하겠다. Evidence Lower Bound(ELBO) - 2 $$ \begin{aligned} \log p(x; \theta) &amp;\geq \sum_{z} q(z) \log \left( \frac{p_\theta(x, z)}{q(z)} \right) \\ &amp;= \sum_{z} q(z) \log p_\theta(x, z) - \sum_{z} q(z) \log q(z) \\ &amp;= \sum_{z} q(z) \log p_\theta(x, z) + H(q) \ \text{(H(q) is entropy)} \\ \end{aligned} $$ $$ \begin{aligned} &amp;\text{Equality holds if } q = p(z \mid x; \theta)\\ &amp;\log p(x; \theta) = \sum_{z} q(z) \log p(z, x; \theta) + H(q) \end{aligned} $$
지난 포스트에서 위 식이 나오게 된 이유를 간단하게 정리해보겠다. 우리는 latent variable z를 직접 관측할 수 없기 때문에, 이를 추론하기 위하여 보조의 분포 q를 도입했다. 이때 x는 관측할 수 있는 부분이고 z는 보이지 않는 부분이다. 이 상황에서 우리는 x만 관찰될 때 z 분포를 근사하고자 하며, 이 과정에서 $logp(x)$ 를 직접 계산하기 어렵기 때문에, 이 식을 최적화 가능한 ELBO 식이 등장하게 나오게 된 것이다. 이제 본론으로 넘어와서, 2번째 식을 살펴보자면 $\sum_{z} q(z) \log p_\theta(x, z)$ 이 항은 q모델을 사용하여 z부분을 추론할 때 x,z 부분이 모두 관찰될 떄의 평균 로그 확률이다. (모든 것이 관찰된다. 이는 본질적으로 생성 모델이다.) $\sum_{z} q(z) \log q(z)$ 이 항은 q함수이고, q의 entropy라고 볼 수 있다. (q가 얼마나 무작위적인지 알려주는 양이다.) $$D_{\mathrm{KL}}(q(z) \parallel p(z \mid x; \theta)) = - \sum_{z} q(z) \log p(z, x; \theta) + \log p(x; \theta) - H(q) \geq 0$$
이제 Equality 부분을 설명할 것이다. KL 식을 전개하면 위와 같은 식을 얻을 수 있는데 오른쪽 항에서 $\log p(z, x; \theta)$ 만 남기고 다 넘기면 ELBO의 식과 동일해지는 것을 확인할 수 있다. 그렇다면 어떻게 전개를 하면 위와 같은 식이 나왔을까? KL의 값은 항상 0보다 크거나 같다의 성질을 이용한다. 그 후, 기존 KL의 식에서 $p(z \mid x; \theta)$ 을 Bayes 정리를 활용해서 $p(z \mid x; \theta) = \frac{p(x, z; \theta)}{p(x; \theta)}$ 로 변형하여 식 전개하면 위와 같은 식이 나온다. 그렇기 때문에 만약 $ q = p(z \mid x; \theta)$ 라면 KL의 값이 0이 되기 때문에 부등식이 등식이 되는 것이다. 하지만 우리는 $p(z|x;\theta)$가 계산이 불가능하다는 것을 안다. GMM (lecture 5 12-17)에서 심층 가우시안 분포를 사용해 $p_\theta(x|z)$ 는 z의 분포가 정해져 있었기 때문에 구할 수 있었다. (lecture 5 16) 하지만 $p(z|x;\theta) = \frac{p(z)p_\theta(x|z)}{p_\theta(x)}$ 이기 때문에, $p(x)$ 를 구할 수 없다.(lecture 5 23) 따라서 $p(z|x;\theta)$ 도 계산이 불가능하다. 사실 따지고 보면 만약 posterior가 계산이 가능했으면 위 ELBO식도 필요가 없다. 결국, q분포가 $p(z \mid x; \theta)$에 최대한 가까운 q를 선택해야 하는 것을 알 수 있다. 이는 앞으로 나올 Variational Inference의 이론적 도구로써 작용이 될 것이다. 정리하자면 추정이 불가능한 분포를 근사하는 q를 두어 이 분포를 최적화하여 가장 강력한 lower bound를 찾는 것이 최종 목표인 것이다. 따라서 q의 역할을 하는 별도의 신경망을 두고, p와 q를 공동으로 최적화하여 ELBO를 최대화 시킨다. 조금 더 나아가, VAE의 인코더는 q가 되고 디코더는 p가 된다. Variational Inference Variational Auto Encoder(VAE) 모델에서는 decoder인 $p(x|z) = \mathcal{N}(\mu_\theta(z), \Sigma_\theta(z))$ 의 neural network가 있으면 이것을 뒤집은 encoder의 역할을 하는 $p(z|x;\theta)$ 를 계산하여 x가 주어지고 이 x를 생성할 가능성이 높은 z를 찾으려고 한다.]]></description>
</item>
<item>
    <title>[CS236] 5. Latent Variable Models</title>
    <link>https://goodyoung.github.io/posts/cs236/lecture5/</link>
    <pubDate>Mon, 14 Jul 2025 17:24:25 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs236/lecture5/</guid>
    <description><![CDATA[개요 이번 포스트에서는 CS236 강의의 5강 내용을 정리한다. Autoregressive 구조는 장/단점을 지닌다. 장점은 likelihood를 평가하여 maximum likelihood를 구할 수 있어 훈련이 비교적 쉽다. 반면, 단점은 순서대로 생성하여 생성 시간이 오래걸린다. 그리고 비지도 학습을 사용하여 데이터의 특징을 추출하는 것이 명확하지 않다. 이 점이 잠재 변수 모델을 사용해서 해결할 수 있는 일 중 하나이다. 이번 챕터에서는 이 latent variable(잠재 변수)가 있을 때 생성 모델이 추론과 학습을 수행하는 방법에 대해서 설명할 것이다. Latent Variable Model 위 그림과 같이 사람 이미지와 같은 이미지 데이터 안에는 단순히 픽셀 데이터가 아니라 그 안에 성별, 눈 색깔 등의 여러 변동성 큰 정보들이 포함되어 있다. 하지만 이 모든 정보들을 annotated 하여 이용하기 쉽지 않다. 그렇기 떄문에 latent variable에 원본 데이터 상에 숨겨진 특징들이 존재하게 된다. 만약 특정한 특징을 나타내는 latent variable(z)을 사용하여 생성하게 되면, 우린 더욱 더 유연한 생성 모델을 얻을 수 있을 것 이다. 그래서 이 latent variable(z)을 반영하여 모델의 확률 분포 $p(x,z)$를 구하는 Latent Variable Model에 대해서 알아볼 것이다. 왼쪽 그림은 Latent Variable Model을 간략하게 나타낸 구조이고 오른쪽 그림은 베이지안 네트워크일 때를 나타낸다. 우리는 $x$와 $z$의 결합 분포인 $p(x,z) = p(z)p(x|z)$ 를 구하게 될 것이고 이것이 베이지안 네트워크로 가면 $p(x,z) = p(z_1)p(x|z_1)+&hellip;+p(z_k)p(x|z_k)$ 가 될 것이다. 이렇게 잠재변수 z를 포함한 모델링을 사용하면 x만을 활용한 것보다 쉽다. 또한 만약 z의 특징을 각각 추출할 수 있다면, 그것을 이용해서 다른 종류의 task에도 이용이 가능하다. (ex)eyeColor = Blue만 식별 가능) 하지만 현실적으로 모든 z중에 특정 z만 따로 추출하는 것이 어렵다는 것이다.(이 말은 위에서 현실적으로 모든 z의 확률을 구할 수 없어서 베이지안 네트워크를 사용하기 어려울 수 있다는 말과 동일하다.) 따라서 우리는 이 문제를 해결하기 위하여 단순한 z(가우시안) 만을 가정하여 deep neural network(모든 각각의 관계의 정보를 따로따로 확인하지 않아도 모든 정보를 고려해줄 수 있다.) 를 사용하여 이 z를 예측하려고 한다. $z \sim \mathcal{N}(0,1)$ $p(x|z) = \mathcal{N}(\mu_\theta(z), \Sigma_\theta(z))$ 이 문제는 비지도학습의 representation learning이기 때문애 학습 시 z에 대해서 잘 학습(z의 특징을 잘 추출)되기를 희망하게 된다. (이 모델이 어떤 z를 추출할지 명확하지 않다.) 만약 학습이 잘된다면, z의 분포를 클러스터링 하여 우리가 원하는 클러스터에 해당하는 z의 값($p(z)$)을 구할 수 있고 이 z를 가지고 새로운 데이터 $p_\theta(x,z)$를 만들어낼 수 있게 된다. 다음 챕터에서 이에 대해 더 자세히 설명할 것이다. Mixture of Gaussians (GMM,VAE) 그렇다면 z를 어떻게 예측 하는 방법에 대해서 자세히 알아볼 필요가 있다. 데이터의 분포를 확인하면 위 그림과 같을 것이다. 그 후, $K$개의 가우시안 분포(혼합 성분)를 나눌 수 있다. 그럼 생성 과정에선 K개 중 하나를 선택하여 그 K에 해당하는 평균, 공분산을 따르는 가우시안 분포에서 x를 샘플링한다. (이는 VAE에서 생성 과정에서도 동일하다.) $\mathbf{z} \sim \text{Categorical}(1, \cdots, K)$ $\mathbf{x} \sim p(\mathbf{x} \mid \mathbf{z} = k) = \mathcal{N}(\mu_k, \Sigma_k)$ (likelihood) 이 방법은 생성 과정 뿐 아니라, x가 주어졌을 때 어떤 z에 속하는지 맞출 수 있는 추론 과정에도 (posterior $p(z|x)$) 사용할 수 있다. (clustering, unsupervised representation learning) 또한 Mixture of Gaussians의 전체 확률 ($p(x)$)도 구할 수 있는데 다음과 같은 식으로 가능하다. $p(\mathbf{x}) = \sum_{z} p(\mathbf{x}, z) = \sum_{z} p(z) \cdot p(\mathbf{x} \mid z) = \sum_{k=1}^{K} p(z=k) \cdot \mathcal{N}(\mathbf{x}; \mu_k, \Sigma_k)$ 이때 얻을 수 있는 직관은 simple한 $p(x|z)$를 활용하여 복잡한 혼합 모델($p(x)$)을 구할 수 있다는 것이다. 지금까진 z를 어떻게 예측을 할지에 대해서 알아보았다. 이 Mixture of Gaussians에서는 Neural Network(NN)를 사용하지 않아 z의 수가 많아지면 계산하기 어려운 단점이 있다.]]></description>
</item>
</channel>
</rss>
