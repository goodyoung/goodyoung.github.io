<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Reparametrization Trick - Tag - Good Young</title>
        <link>https://goodyoung.github.io/tags/reparametrization-trick/</link>
        <description>Reparametrization Trick - Tag - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 26 Aug 2024 18:21:20 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/tags/reparametrization-trick/" rel="self" type="application/rss+xml" /><item>
    <title>[Paper Review]Auto-Encoding Variational Bayes(VAE)</title>
    <link>https://goodyoung.github.io/posts/paper/vae/</link>
    <pubDate>Mon, 26 Aug 2024 18:21:20 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/vae/</guid>
    <description><![CDATA[개요 Generative분야에서 기초가 되는 논문인 VAE에 관한 리뷰를 할 것이다. Introduction 연속적인 latent variable(잠재변수)나 파라미터가 계산이 힘든 후방 확률 분포를 가지는 확률 모델을 어떻게 효과적으로 추론하고 훈련을 시키는 방법에 대해 설명이 나온다. 디코더만으로 학습 및 훈련을 진행할 수 없어서 인코더를 가져온 것이다. 해당 내용은 Method 부분에서 자세하게 다룰 것이다. 기존의 방법인 Variational Bayesian(VB) 방법은 계산하기 힘든 사후 확률을 적절하게 최적화 한다. 하지만 이 방법은 후방 확률에 대한 기대값의 분석적 해결책을 요구하며, 일반적인 경우에도 계산이 어렵다. 그래서 본 논문에서 기존 변동의 lower bound의 reparameterization가 어떻게 간단하고 다른 비편향적인 estimator의 lower bound를 만드는지 알려준다. Stochastic Gradient Variational Bayes(SGVB) estimator가 posterior inference(후방 추론)을 잘 하도록 한다. 이는 확률적 경사 하강법을 사용하여 곧바로 최적화도 된다. 독립적이고 동일한 분포를 띄는 이 latent variable에서 본 논문은 Auto-Encoding VB 알고리즘을 제안한다. SGVB estimator를 사용하여 추론과 learning을 neural network인 recognition model에 효율적으로 적용한다. 비싼 추론 없이 모델 param을 효과적으로 배우는 방법을 사용한다. 해당 estimator를 사용하면 효과적인 후방 추론을 수행하게 하여 최적화가 된다. 위처럼 neural network에서 recognition으로 사용하게 되면 variational auto-encoder(VAE)가 된다. Method 어떠한 dataset이 있을 때 실제 파라미터 $\theta$와 latent variable $z^\text{(i)}$ 는 우리가 알 수 없다. 따라서 본 논문에선 Intractability하고 large dataset에도 잘 최적화 할 수 있는 알고리즘을 개발을 한다. 그렇게 하기 위하여 세가지 문제를 설명한다. 파라미터 $\theta$ 에 대한 효율적인 근사를 하는 문제 관찰된 값 $x$와 $\theta$를 기반으로 latent variable $z$를 효율적인 사후 추론 하는 문제 $x$의 효율적인 근사 추론하는 문제 이미지 노이즈 제거(denoising), 이미지 복원, 초해상도 등 가능 이런 문제를 해결하기 위하여 recognition model ($q_{\phi}(z \mid x)$)을 도입하였다. 이는 $p_{\theta}(x \mid z)$를 가장 근사화 하는 네트워크 이다. 이 네트워크의 목표는 decoder에서 training data(x)의 likelihood를 최대화 하고 싶은 것이다. 따라서 해당 목표를 수학 식으로 표현하자면 아래와 같다. 이는 x가 나올 수 있는 확률 ($p_{\theta}(x)$)이 가장 커지는 distribution을 찾는 것으로 생각하면 된다. $$p_{\theta}(x) = \int p_{\theta}(z) , p_{\theta}(x \mid z) \ dz $$
이때 위 식은 아래의 식에서 나왔다. $$\frac{P(x, z)}{P_{\theta}(z)} = p(x \mid z)$$
따라서 식을 정리하면 아래와 같다. $$p_{\theta}(x) = \int P(x, z) \ dz $$
위 식을 해석하면 x와 z가 동시에 일어날 확률을 모든 z에 대해서 적분하면 그것이 x의 확률이 되는 것이다. 하지만 문제는 모든 z에 대해서 적분을 하기가 intractability하는 것이다. 이 문제를 해결하기 위하여 확률적 인코더인 $q_{\phi}(z \mid x)$ (encoder)가 나오게 되었다. $q_{\phi}(z \mid x) \approx p_{\theta}(z \mid x)$ 로 근사한 것이다. Evidence LowerBOund (ELBO) 다음은 data likelihood를 어떻게 최적화 하는지에 대해서 설명을 할 것이다. 먼저 우리의 목적인 $p_{\theta}(x^i)$의 식을 풀어보자면 아래와 같다. $$\log p_{\theta}(x^{(i)}) = E_{z \sim q_{\phi}(z \mid x^{(i)})} \left[ \log p_{\theta}(x^{(i)}) \right]$$
적분에서 기대값으로 변경하기 위하여 log를 씌우고 Decoder에서 $z$가 $q_{\phi}(z \mid x^{(i)})$ (encoder)의 분포를 따를 때를 의미한다. 위 식을 조금 더 분해하면 아래와 같다. 베이즈 정리를 사용하여 식을 변형하고 상수를 곱하였다. $$= E_{z} \left[ \log \frac{p_{\theta}(x^{(i)} \mid z) p_{\theta}(z)}{p_{\theta}(z \mid x^{(i)})} \right] (\therefore p(z \mid x^{(i)}) = \frac{p(x^{(i)} \mid z) p(z)}{p(x^{(i)})})$$ $$= E_{z} \left[ \log \frac{p_{\theta}(x^{(i)} \mid z) p_{\theta}(z)}{p_{\theta}(z \mid x^{(i)})} \frac{q_{\phi}(z \mid x^{(i)})}{q_{\phi}(z \mid x^{(i)})} \right] $$
위 식에서 log 변환을 하여 아래와 같은 식을 만들게 된다. $$ = E_{z} \left[ \log p_{\theta}(x^{(i)} \mid z) \right] - E_{z} \left[ \log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z)} \right] + E_{z} \left[ \log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z \mid x^{(i)})} \right] $$]]></description>
</item>
</channel>
</rss>
