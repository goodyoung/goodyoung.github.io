<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>KL-Divergence - Tag - Good Young</title>
        <link>https://goodyoung.github.io/tags/kl-divergence/</link>
        <description>KL-Divergence - Tag - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 30 Jun 2025 17:00:38 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/tags/kl-divergence/" rel="self" type="application/rss+xml" /><item>
    <title>[CS236] 4. Maximum Likelihood Learning</title>
    <link>https://goodyoung.github.io/posts/cs236/lecture4/</link>
    <pubDate>Mon, 30 Jun 2025 17:00:38 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs236/lecture4/</guid>
    <description><![CDATA[개요 이번 포스트에서는 CS236 강의의 4강 내용을 정리한다. 3강에선 데이터셋의 분포를 학습하여 Model Family를 파라미터화 하는 방법을 배웠다. 이번 4강에서는 데이터셋에 대하여 모델 파라미터 $\theta$ 를 찾는 방법을 배우게 된다. 3강에서 다뤘던 내용을 다시 한 번 살펴보자. 데이터가 실제 분포 $P_\text{data}$ 로부터 추출된 m개의 샘플 $\mathcal{D}$ 가 있다고 가정을 해보자. 그럼 생성 모델의 목표는 모델 $\mathcal{M}$ 에서 $P_\text{data}$ 와 가능한 가장 가까운 $P_\theta$ 를 학습하는 것이다. 이때의 모델 $\mathcal{M}$ 은 Bayes net이 될 수 있고, FVSBN이 될 수 있는 것 이다. 하지만 $P_\theta$ 가 완전히 실제 분포를 포착할 순 없다. 왜냐하면 제한된 data의 문제와 컴퓨팅 파워 문제가 있기 때문이다. 784개의 이진 픽셀로 이루어진 이미지를 생각해보면, 가능한 모든 이미지는 $\text{2}^\text{784} \approx \text{10}^\text{236}$ 가지이다. 대략 백만 개의 샘플로는 이 공간을 거의 다룰 수 없다. 그래서 우리는 $P_\text{data}$ 의 분포를 잘 근사하는 $P_\theta$ 를 선택해야한다. 그렇다면 어떤 것이 잘 근사하는 모델($P_\theta$)일까? 잘 근사하는 모델은 우리가 하려는 task에 따라 다르다. (Density Estimation에서는 전체 확률 분포를 잘 근사하는 것이 중요, Specific Prediction Task은 특정 예측을 만드는 분포(조건부 확률)가 중요, Structure or Knowledge Discovery 모델 그 자체의 구조가 중요) 생성의 관점에선 어떤 확률적 추론 쿼리에 답해야 하기 때문에 우린 전체 분포를 배워야한다. 따라서 학습을 Density Estimation 문제로 볼 수 있다. 여기서 말하는 확률적 추론 쿼리(any probabilistic inference query)란 확률 분포로부터 도출되는 질문들(조건부 확률, 마진, 샘플링 등)을 의미한다. 그래서 우리의 목표는 $P_\text{data}$ 에 가장 가까운 $P_\theta$ 를 만드는 것이 제일 중요하다. 그렇다면 가까운 정도를 어떻게 평가할까? 다음 챕터에서 설명 할 것이다. Kullback-Leibler divergence(KL-divergence) 어떤 두 분포간의 가까운 정도를 측정하기 위하여 Kullback-Leibler divergence(KL-divergence)지표를 사용하게 된다. 수식은 아래와 같다. $$D_{\text{KL}}(p || q) = \sum_x p(x) \log \frac{p(x)}{q(x)}$$
KL-Divergence는 몇가지 특징이 있다. $D_{\text{KL}}(p || q) \geq 0$ 이고, 같을 땐 $p = q$ 일 때 이다. $D_{\text{KL}}(p || q) \not = D_{\text{KL}}(q || p)$ 으로 비대칭적인 성질을 지닌다. 이 지표는 정보이론 관점에서 p와 q에 기반한 압축 방식이 얼마나 잘되는지 보여준다. (p가 진짜 분포이고 q가 학습한 분포) q의 분포로 p를 인코딩을 했을 때 생기는 비트 낭비량을 확인할 수 있다. 우리의 목표는 $P_\theta$ 가 $P_\text{data}$ 와 가깝도록 만드는 것이기 때문에 $D_{\text{KL}}(P_\text{data} || P_\theta)$ 로 표현할 수 있다. 따라서 KL-Divergence의 값을 확인하여 data를 잘 압축할 수 있는 모델을 선정해야한다. KL이 작을수록, 압축 손실도 작아짐 → 더 나은 모델 $$ D_\text{KL}(P_\text{data} || P_\theta) = \mathbb{E_{x \sim P_\text{data}}}[\log P_\text{data}(x)] - \mathbb{E_{x \sim P_\text{data}}}[\log P_\theta(x)] $$
위 KL을 다음과 같이 분해가 가능하고, 앞 항은 $P_\theta$ 에 영향받지 않는 상수이기 때문에 두번째 항에 집중을 할 것이다. 그렇게 되면 수식은 아래와 같아진다. $$ argmin_\theta \ D_(P_\text{data} || P_\theta) = argmin_\theta -\mathbb{E_{x \sim P_{\text{data}}}} [\log P_\theta(x)] = argmax_\theta \ \mathbb{E_{x \sim P_{\text{data}}}} [\log P_\theta(x)] $$
그럼 이제 KL을 최소화 하기 위하여 두번째 항을 최대화 하는것이 목표이다. 이 수식을 통하여 알 수 있는 KL의 특징이 있다. 바로 두 모델 간 KL을 비교했을 때 누가 가까운지는 알지만 얼마나 가까운지 (정확한 거리)는 모른다는 것이다. $D_(P_\text{data} || P_{\theta_{1}}) - D_(P_\text{data} || P_{\theta_{2}})$ 가 계산이 되면 상수(첫번째 항)가 사라져서 거리를 모르게 된다. 이제 이 수식을 풀려고 보니, 정리된 수식의 모든 $P_\text{data}$를 우리는 일반적으로 (expected log-likelihood) 구할 수 없다. 왜냐하면 현실에선 이 기대값을 계산할 수 없으므로 주어진 데이터 샘플의 평균으로 근사 해야한다. (empirical log-likelihood) 따라서 데이터 샘플의 평균으로 근사하면 아래와 같은 식이 나오고, 그것을 최대화 하는 $\theta$를 찾는 방향으로 학습을 진행하면 된다.]]></description>
</item>
</channel>
</rss>
