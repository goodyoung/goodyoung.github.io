<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Transformer - Tag - Good Young</title>
        <link>https://goodyoung.github.io/tags/transformer/</link>
        <description>Transformer - Tag - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 31 Jul 2024 19:28:04 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/tags/transformer/" rel="self" type="application/rss+xml" /><item>
    <title>[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)</title>
    <link>https://goodyoung.github.io/posts/paper/vit/</link>
    <pubDate>Wed, 31 Jul 2024 19:28:04 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/vit/</guid>
    <description><![CDATA[개요 Vision분야에서 Transformer를 사용한 ViT에 관한 논문에 대한 리뷰를 할 것이다. Introduction Self-Attention기반 archtecture인 Transformer는 NLP에서 중요한 역할을 하고 있다. 이때의 주요 접근 방법은 아주 큰 text data로 학습 시킨 사전 훈련 모델을 fine tuning을 하는 것이다. Transformer의 계산 효율성과 확장성으로 인해 전레없는 크기의 모델을 훈련하는 것이 가능해졌다. Model과 dataset이 커져도 성능의 포화는 보이지 않는다. 하지만 Vision에서는 CNN 구조가 지배적이었다. NLP의 성공 후, attention을 사용하려는 여러 시도들이 있었지만 특수한 attention pattern의 사용 때문에 효과적으로 사용되진 않았다. CNN + self-attention의 구조 CNN을 완전히 대체하는 구조 따라서 이런 상황속에서 최소한의 수정으로 image를 직접적으로 Transformer에 넣는 실험을 했다. Image를 패치로 나누고 이 패치를 linear 한 embedding의 연속으로 나눈 후 이것을 Transformer에 바로 넣는 방식이다. 그렇게 되면 Image patches는 NLP의 관점으론 token(word)가 되는 것이다. 이 ViT를 ImageNet과 같은 mid-size dataset으로 학습을 했다. 이땐 ResNet보다 몇 퍼센트 낮은 정확도를 보였다. 이는 예상된 결과이다. 왜냐하면 기존 CNN은 inductive biases를 가지고 있어 적은 양의 데이터로도 translation equivariance, locality를 잘하여 일반화 성능이 좋다.
inductive biases: 모델이 학습된 데이터 외의 데이터에 대해 얼마나 잘 일반화할 수 있는지에 사용하는 가정
머신러닝의 최종 목표는 generalization, 즉 학습 데이터로 학습시킨 모델이 본 적 없는 데이터에 대해서도 예측(prediction, approximation)을 잘 해내는 것이다. 본 적 없는 상황을 예측하기 위해서는 학습된 가정 이외에 추가적인 가정이 필요한데, 이것이 바로 inductive bias이다. translation equivariance: 입력 데이터가 일정한 변환을 받을 때, 그 변환이 모델의 출력에도 동일하게 반영되는 성질
CNN의 합성곱 층에서는 이미지의 특정 패턴이 위치를 옮겨도 그 패턴을 감지하는 필터의 반응이 동일하게 이동한다. 즉, 입력 이미지의 패턴이 이동하면, 해당 패턴을 인식하는 뉴런의 활성화 위치도 동일하게 이동한다. 어떠한 사물이 들어 있는 이미지를 제공해줄 때 사물의 위치가 바뀌면 CNN과 같은 연산의 activation 위치 또한 바뀌게 된다. Locality: 이미지 내의 픽셀들이 인접한 다른 픽셀들과 더 밀접한 관계를 가지는 특성
그에 반해, Transformer는 inductive biases가 부족하여 데이터가 충분하지 않을 때 일반화가 되지 않는다. 그러나 더 큰 데이터 셋으로 훈련을 했을 경우 모델의 성능이 크게 향상이 된다. 따라서 대규모 훈련이 inductive bias를 능가하는 것을 확인을 하였다. Method 위 사진은 ViT의 전체적인 구성에 대한 그림이다. 본 논문에서는 최대한 가능한 원래의 Transformer구조를 따를려고 했다. 그렇게 되면 확장 가능한 NLP Transformer 아키텍처와 그 효율적인 구현을 거의 그대로 사용할 수 있다는 장점이 있기 때문이다. Vision Transformer (ViT) ViT의 전체적인 구조를 설명하고 있다. 크게 이미지 입력 처리, 패치 임베딩, [class] 토큰 추가, position 임베딩, Transformer Encoder 구조 순으로 설명할 것이다. 먼저 이미지 입력 처리에서 입력 이미지는 $ H * W * C $ (H,W 이미지 높이와 너비, C: 채널 수) 크기를 가지지만 1D 시퀀스의 입력을 받는 Transformer의 구조에 맞게 변형이 필요하다. 따라서 $ H * W * C $ 에서 $ N * (P^2 * C) $ ($N = \frac{H*W}{P^2}$, $P^2$은 각 패치의 크기, C: 채널 수)로 변환을 해야한다. 또한 이 각 패치는 입력에 맞게 linear projection되어 고정된 크기 $D$의 벡터로 매핑이 되어 이 과정을 통해 입력 데이터를 학습 가능하게 변환한다. Linear projcetion: 입력 벡터에 선형 변환을 적용하여 다른 차원의 벡터로 변환하는 과정. $y = W * x + b$ 일 때 $y$는 출력 백터로 고정된 크기의 $D$차원의 벡터이고, W는 가중치 행렬로 학습 가능한 행렬이다. 이미지 패치가 $16 * 16 * 3$일 때 이 패치를 펼쳐서 $16 * 16 * 3 = 768$크기의 벡터로 나타낼 수 있다 이때 Linear projection을 통해 $D = 512$ 크기의 벡터로 변환하려면, $512 * 768$크기의 학습 가능한 가중치 행렬 $W$를 사용해 Linear projection을 사용하여 다른 차원의 벡터로 변환하는 것이다.]]></description>
</item>
<item>
    <title>[Paper Review]Attention Is All You Need</title>
    <link>https://goodyoung.github.io/posts/paper/transformer/</link>
    <pubDate>Wed, 24 Jul 2024 11:32:59 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/transformer/</guid>
    <description><![CDATA[개요 요즘 모든 분야에서 자주 사용되고 있는 아키텍쳐인 Transformer에 대한 논문 리뷰를 해보려고 한다. Abstract 기존의 sequence transduction model(sequence 간 변형이 이루어 지는)은 complex recurrent나 cnn기반으로 구성되어 있다. 그리고 좋은 성능을 내는 것이 attention 기반에 encoder &amp; decoder 기반으로 연결되어 있는 네트워크이다. 본 논문에서는 오로지 attention mechanism만 사용하는 Transformer를 제안한다. 즉, 순환이나 cnn의 연산을 사용하지 않는 네트워크이다. 이는 훈련시간 절감, 행렬곱을 이용하여 병렬적으로 수행하면서 번역 작업에 우수한 품질을 보여준다. English-German 번역 작업에 28.4 BLEU를 달성하였고 다른 task에서도 잘 작동한다. Introduction RNN, long-term memory, GRU이 language modeling, machine translation 문제에서 SOTA를 달성하고 있다. 많은 연구가 encoder-decoder의와 순환 language model의 성과를 달성하려고 하고 있다. 순환 모델의 연산 특성은 training sample 내에서 병렬화를 방해하며, 시퀀스 길이가 길어질수록 메모리 제약으로 인해 예제 간 배칭이 제한되므로 병렬화가 중요해집니다. 기존 RNN의 단점 따라서 기존의 RNN의 단점에 대해서 간략히 살펴 보겠다. 위 그림처럼 sequence에 포함되어 있는 순서 정보를 정렬하고 이것을 차례대로 hidden state의 값을 반복을 통하여 갱신하기 때문에 병렬적으로 수행하기에 어렵다는 사실을 알 수 있게 된다. 또한 input 단어가 많아지면 encoder의 마지막 부분의 출력인 context vector를 만들어야 하기 때문에 병목의 문제도 있다. 다시 논문 내용으로 넘어가면 Factorization trick과 conditional computation의 연구로 인해 이러한 문제점은 상당히 해결되고 있지만 여전히 sequential computation은 제약이다. Attention은 input과 output의 거리에 관여하지 않고 model의 종속성을 허락해준다. 이러한 attention mechanism은 주로 순환 신경망과 함께 사용이 되어 왔다. Attention RNN처럼 하나의 context vector가 소스 문장의 모든 정보를 가지고 있어야하는 단점을 해결하기 위해 나온 기법이 Attention이다. Attention은 매번 출력 단어를 만들어 낼 때마다 소스 문장의 출력 정보 중에서 어떤 정보가 가장 중요한지 가중치를 부여해서 출력 단어를 보다 효과적으로 생성할 수 있게 한다. 간단히 매번 소스 문장에서의 출력 전부를 decoder의 입력으로 받는 방식이다. 결론적으로 본 논문에서 제안하는 Transformer는 순환을 피하고 대신에 attention mechanism에 온전히 의존하여 input과 output 사이의 ``global dependencies를 이끈다. 이러한 구조는 병렬 처리에 특화되었다. Background 기존 연구들은 CNN을 사용하여 sequenrtial computation을 줄이는데 초점을 맞추었지만 이 방법은 다른 position 사이의 종속성을 배우기 쉽지 않다. 하지만 Multi-Head Attention의 방법을 사용하여 상수 시간으로 줄였다. 또한 기존 순환 end-to-end memory network는 attention mechanism을 사용했다. 하지만 Transformer에서는 RNN이나 CNN을 사용하지 않고 input output의 representation을 계산하는 전체의 self-attention을 사용한다. 여기서 나온 Self-Attention이란 input으로 들어오는 각각의 단어가 서로에게 얼만큼의 영향을 미치는지 알려준다. 문맥에 대한 정보를 잘 학습하도록 만드는 것이다. Model Architecture 기존 모델들은 encoder-decoder구조를 따르는데 이전 단계에서 생성한 symbol을 활용해서 decoder가 다음번에 나올 output을 만든다. Transformer 또한 encoder-decoder의 구조를 띄는데 모델을 순환적으로 사용하지 않고 attention mechanism만 활용하여 sequence에 대한 정보를 한 번에 입력으로 준다는 것이 특징이다. 위의 그림은 transformer의 전체적인 구조인데 이를 앞으로 하나씩 살펴볼 예정이다. Encoder and Decoder Stacks Encoder부분은 여러번 stack이 가능한 구조이고 하나의 layer는 크게 두가지의 구조를 지닌다. multi-head self attention feed-forward network 또한 이 두가지의 구조 모두 residual connection을 활용하여 Identity mapping을 거치게 한다. Decoder부분은 여러번 stack이 가능한 구조이고 보통 encoder랑 같은 layer의 개수랑 맞춘다. Decoder third sub-layer에 encoder의 output값을 활용하여 multi-head attention을 수행하도록 한다. residual connection을 활용하여 보다 더 쉽게 global optima를 찾도록 한다. 이전에 등장한 단어들만 참고할 수 있도록 mask를 씌워서 multi-head attention을 사용할 수 있도록 한다. Attention Attention Function은 쿼리와 key-value 쌍을 output으로 mapping을 한다. Query, keys, values, output들은 다 vector이다. Query(Q): 물어보는 주체 (어떤 단어가 가장 중요했는지를 key에서 계산하여 결과를 낸다.]]></description>
</item>
</channel>
</rss>
