<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>computer vision - Tag - Good Young</title>
        <link>https://goodyoung.github.io/tags/computer-vision/</link>
        <description>computer vision - Tag - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 29 Aug 2024 18:37:03 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/tags/computer-vision/" rel="self" type="application/rss+xml" /><item>
    <title>[Paper Review]Multimodal Machine Learning: A Survey and Taxonomy</title>
    <link>https://goodyoung.github.io/posts/paper/multimodal-survey/</link>
    <pubDate>Thu, 29 Aug 2024 18:37:03 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/multimodal-survey/</guid>
    <description><![CDATA[개요 Multi-Modal Learning에 관련한 서베이 논문을 리뷰해보려고 한다. Introduction 세상을 둘러싼 환경은 여러 modality를 포함하고 있다. 사람들은 이러한 modality를 sensory modality(vision or touch)와 연결을 짓는다.
본 논문은 natural language, visual, vocal signal에 관해서 중점을 두어 설명을 한다.
Multi modal을 AI에 사용하기 위해서는 multimodal message에 대해 여러 정보(multiple modalities)를 연결시킬줄 알아야 한다.
또한 여러 데이터 간 데이터의 이질성으로 인하여 Multimodal machine learning에서는 여러 해결해야할 문제가 있는데 본 논문에서는 5개를 제시한다.
첫 번째는 Representation이다. 이는 multimodal data를 어떻게 잘 요약하고 표현을 할 지에 대한 문제이다.
두 번째는 Translation이다. 이는 하나의 modality에서 다른 하나의 modality로 어떻게 mapping(translate)을 할 지에 대한 문제이다.
이미지에 대한 올바른 방식의 해석이 있어도 단 하나의 완벽한 해석은 존재하지 않는다. 세 번째는 Alignment이다. 이는 여러개의 modality로부터 요소 사이의 관계들을 정렬하여 식별하는 것이다. 서로 다른 modality 간의 유사성을 측정하고, 가능한 장거리 의존성 및 모호성을 처리해야 한다. 네 번째는 Fusion이다. 여러 modality의 추론 결과를 합치는 것이다. 다른 modality로부터 오는 정보들은 다양한 예측 결과를 가져올 수 있다. 다섯 번째는 Co-learning이다. Modality간에 knowledge를 전달하는 것이다. 이것은 한 modality의 data가 부족할 때 유용하다. 위 표는 multi-modal을 적용하는 application에서 위에 설명한 5가지의 challenge의 포함 여부를 나타낸 것이다.
위 표를 확인하며 multi-modal로 활용할 수 있는 task가 무엇인지도 함께 확인하면 될 것 같다.
Representation 이젠 5가지의 challenge에 대해서 설명할 것이다. 먼저 첫 번째로 설명하는 Multimodal Representation에 대한 설명이다.
여러 modality를 representing하는 것은 다양한 어려움이 존재한다.
이질성인 데이터에서 어떻게 섞을 것인지 다른 종류의 noise를 어떻게 처리할 것인지 missing data를 어떻게 처리할 것인지 좋은 representation하는 방법은 model의 성능을 중요하다. (최근 speech recognition, visual object detection 등의 성능 향상 사례가 있다.)
또한 좋은 representation을 위한 몇 가지 속성으로 부드러움(smoothness), 시간적 및 공간적 일관성(temporal and spatial coherence), 희소성(sparsity), 자연스러운 클러스터링(natural clustering) 등이 있다.
Multi modal representation을 위한 여러 속성들이 있다. 그것은 표현 공간에서의 유사성이다. 이는 해당 개념들의 유사성을 반영해야 하며, 일부 modality가 없어도 쉽게 표현을 얻을 수 있어야 하고, 관찰된 modality를 바탕으로 누락된 모달리티를 채울 수 있어야 합니다.
이전까지 단일 modality에 대한 연구는 광범위 하게 연구되어 왔다. 이미지에 관련한 data는 SIFT기법 에서 CNN기법으로 연구되어 왔고 audio domain은 음향적 특징들이 deep neural network에서 rnn으로 연구되어 왔다.
이런 와중 multi-modal에선 단일 modality에 대한 연구들을 단순히 concat하는 방법만 사용하고 있다. 이런 방법론들이 변화되고 있다.
따라서, 본 논문에선 joint와 coordinated라는 두 가지의 representation 방법을 소개한다.
joint는 각각의 modality를 같은 representation space에 결합하는 방식이다. 이는 아래와 같은 수식으로 표현할 수 있다. ($x_1, x_n$등은 각각 modality이다.)
$$ x_m = f(x_1,&hellip;,x_n)$$
coordinated는 각각의 modality를 각각 분리해서 처리하지만 similarity 규정을 사용해 coordinated space에 가져온다. 이는 아래와 같은 수식으로 표현할 수 있다. $$f(x_1) ~ g(x_2)$$
Joint Representation Joint Representation은 독립적인 modality 특징들을 concatenation을 한다고 생각하면 된다.
앞으로는 data의 representation 방법 중 가장 유명한 방법인 Neural network에서 Joint Representation을 하는 방법에 대해서 설명할 것이다.
Neuraul network을 사용해 multi modal representation을 구축하기 위해 각 modality는 여러 개의 개별 신경 계층으로 시작하고, 이후 이 modality들을 joint space에 투영하는 hidden layer가 따른다.
이렇게 joint된 representation 들은 hidden layer를 거치거나 예측에 직접적으로 사용을 한다.
이런 neural network에서 훈련을 할 때, 많은 label data가 필요하게 된다. 따라서 unsupervised data에서 autoencoder를 사용해 이러한 표현을 pre-training하는 것이 일반적이다.
하나의 예시로 denoising하는 여러개 autoencoder를 stack한 후 다른 autoencoder layer를 사용하여 fuse하게 된다.]]></description>
</item>
<item>
    <title>[Paper Review]Training data-efficient image transformers &amp; distillation through attention(DeiT)</title>
    <link>https://goodyoung.github.io/posts/paper/deit/</link>
    <pubDate>Sun, 18 Aug 2024 21:02:30 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/deit/</guid>
    <description><![CDATA[개요 ViT의 단점을 보완하고자 나온 논문인 DeiT에 대한 리뷰를 할 것이다. Abstract Convolution Neural Networks은 image classification에서 대규모 훈련 세트를 같이 사용한 주요 설계 방식이었다. 점점 NLP에서도 attention 기반 모델들의 사용이 많아지고 있었다. 따라서 Vision분야에서도 attention 기반과 CNN을 합치는 하이브리드 아키텍쳐가 있었다. 그 결과 ViT가 나오게 되었는데 이는 3억 개의 사설 라벨 이미지 데이터셋으로 학습을 했다. ViT의 논문에서는 Do not generalize well when trained on insufficient amounts of data으로 설명을 하였고 이를 위해선 방대한 컴퓨팅 자원이 필요했다. 따라서 본 논문에서는 ViT보다 훨씬 적은 데이터로 학습된 Data-efficient image Transformer(DeiT) 모델을 제시한다. 적은 데이터로 학습을 하기 위해서 Knowledge Distillation의 방법을 사용하며 그것을 이루기 위하여 token-based 방식을 취한다. 본 논문에 나온 연구를 요약하자면 다음과 같다. 이번 DeiT는 CNN이 없는 neural network가 외부 데이터 없이 ImageNet의 훈련만으로 SOTA들과 경쟁을 할 수 있게 된다. Distillation token의 기반으로 지식 증류를 하여 기존 증류 방식보다 현저히 더 우수한 성능을 보여준다. 전이 학습을 할 때 경쟁력을 유지하는 성능을 보여 일반화가 잘 되어있다는 것을 알 수 있게 된다. Vision transformer: overview 다음 내용은 본 논문에서 기반을 둔 아키텍쳐인 ViT에 관한 간략한 요약에 관한 내용들이다. 이 내용들은 ViT 논문 리뷰에서 정리를 해두었으니 자세한 내용은 해당 리뷰를 가서 확인 하면 될 것 같다. ViT의 Multi-head Self-Attention, Transformer Block 구성, Class token에 대한 설명이 나와 있다. 또한 더 낮은 해상도로 훈련하고 더 높은 해상도에서 네트워크를 fine-tuning하는 것이 바람직하다고 나온다. 이 방법은 전체 훈련 속도를 높이고 데이터 증강 방식에서 정확도를 향상시킨다. 해상도를 높이고 훈련을 다시 할 때 패치 크기는 동일하게 유지하므로 입력 패치의 개수는 변하지 않지만 위치 임베딩은 조정이 필요하다. 따라서 해상도를 변경할 때 위치 인코딩을 interpolate하는 방법을 사용한다. Distillation through attention 이번 내용은 Knowledge Distillation을 통해 ViT를 학습시키는 방법에 대해서 설명을 한다. 방법에 대해서 설명하기 앞서 Knowledge Distillation의 기본 개념인 Soft distillation과 Hard-label distillation에 대한 설명을 한다. $$L_{\text{global}} = (1 - \lambda) L_{\text{CE}}\left( \psi(Z_s), y \right) +\lambda \tau^2 \text{KL}\left( \psi\left(\frac{Z_s}{\tau}\right), \psi\left(\frac{Z_t}{\tau}\right) \right)$$
Sort distillation은 위 식과 같이 Cross entropy loss를 사용하고 교사 모델 loss($Z_t$)와 학생 모델 loss($Z_s$)를 온도 $\tau$를 사용하여 증류를 한다. $$L_{\text{global}}^{\text{hardDistill}} = \frac{1}{2} L_{\text{CE}}\left( \psi(Z_s), y \right) + \frac{1}{2} L_{\text{CE}}\left( \psi(Z_s), y_t \right)$$
또한 Hard distillation은 실제 레이블로 변형한 형태로 argmax 함수로 표현을 하는데 이는 특정 이미지에 대해 교사와 관련된 하드 라벨은 특정 데이터 augmentation 방법에 따라 변경이 될 수 있다고 한다. 또한 Hard distillation에 Label Smoothing 기법을 사용하여 훈련 데이터의 레이블에 약간의 불확실성을 추가를 하여 Soft distillation으로 변형할 수도 있다고 한다. Label Smoothing을 적용하면 원래 레이블이 $[1,0,0]$이라면 $[ 1 - \epsilon, \frac{\epsilon}{K - 1}, \frac{\epsilon}{K - 1}]$ 으로 Soft distillation이 가능하게 변경될 수 있게 된다. 이를 사용하면 과적합 방지,일반화 성능 향상, 손실 함수 안정화의 장점을 얻게 된다. 본 논문에서는 $\epsilon = 0.1$로 설정하여 훈련을 진행하였다고 한다. Knowledge Distillation 추가적으로 Knowledge Distillation에 대해서 알아보자면 딥러닝에서 지식 증류는 큰 모델(Teacher Network)로부터 증류한 지식을 작은 모델(Student Network)로 transfer하는 일련의 과정이라고 할 수 있다. 이는 방대한 양의 데이터로 학습한 모델을 model deployment의 관점으로 봤을 때 더 가벼운 모델을 만들기 위하여 복잡한 모델의 일반화 능력을 가벼운 모델에게 transfer하는 것을 말한다. Transfer하는 방법은 위 그림의 오른쪽 윗 부분이 있는데 Teacher와 Student의 output을 loss fn으로 계산하고 있다. 또한 오른쪽 아래 부분은 Student의 hard prediction을 Ground truth와 loss fn을 계산하는 구조가 보인다.]]></description>
</item>
<item>
    <title>[Paper Review]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(ViT)</title>
    <link>https://goodyoung.github.io/posts/paper/vit/</link>
    <pubDate>Wed, 31 Jul 2024 19:28:04 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/vit/</guid>
    <description><![CDATA[개요 Vision분야에서 Transformer를 사용한 ViT에 관한 논문에 대한 리뷰를 할 것이다. Introduction Self-Attention기반 archtecture인 Transformer는 NLP에서 중요한 역할을 하고 있다. 이때의 주요 접근 방법은 아주 큰 text data로 학습 시킨 사전 훈련 모델을 fine tuning을 하는 것이다. Transformer의 계산 효율성과 확장성으로 인해 전레없는 크기의 모델을 훈련하는 것이 가능해졌다. Model과 dataset이 커져도 성능의 포화는 보이지 않는다. 하지만 Vision에서는 CNN 구조가 지배적이었다. NLP의 성공 후, attention을 사용하려는 여러 시도들이 있었지만 특수한 attention pattern의 사용 때문에 효과적으로 사용되진 않았다. CNN + self-attention의 구조 CNN을 완전히 대체하는 구조 따라서 이런 상황속에서 최소한의 수정으로 image를 직접적으로 Transformer에 넣는 실험을 했다. Image를 패치로 나누고 이 패치를 linear 한 embedding의 연속으로 나눈 후 이것을 Transformer에 바로 넣는 방식이다. 그렇게 되면 Image patches는 NLP의 관점으론 token(word)가 되는 것이다. 이 ViT를 ImageNet과 같은 mid-size dataset으로 학습을 했다. 이땐 ResNet보다 몇 퍼센트 낮은 정확도를 보였다. 이는 예상된 결과이다. 왜냐하면 기존 CNN은 inductive biases를 가지고 있어 적은 양의 데이터로도 translation equivariance, locality를 잘하여 일반화 성능이 좋다.
inductive biases: 모델이 학습된 데이터 외의 데이터에 대해 얼마나 잘 일반화할 수 있는지에 사용하는 가정
머신러닝의 최종 목표는 generalization, 즉 학습 데이터로 학습시킨 모델이 본 적 없는 데이터에 대해서도 예측(prediction, approximation)을 잘 해내는 것이다. 본 적 없는 상황을 예측하기 위해서는 학습된 가정 이외에 추가적인 가정이 필요한데, 이것이 바로 inductive bias이다. translation equivariance: 입력 데이터가 일정한 변환을 받을 때, 그 변환이 모델의 출력에도 동일하게 반영되는 성질
CNN의 합성곱 층에서는 이미지의 특정 패턴이 위치를 옮겨도 그 패턴을 감지하는 필터의 반응이 동일하게 이동한다. 즉, 입력 이미지의 패턴이 이동하면, 해당 패턴을 인식하는 뉴런의 활성화 위치도 동일하게 이동한다. 어떠한 사물이 들어 있는 이미지를 제공해줄 때 사물의 위치가 바뀌면 CNN과 같은 연산의 activation 위치 또한 바뀌게 된다. Locality: 이미지 내의 픽셀들이 인접한 다른 픽셀들과 더 밀접한 관계를 가지는 특성
그에 반해, Transformer는 inductive biases가 부족하여 데이터가 충분하지 않을 때 일반화가 되지 않는다. 그러나 더 큰 데이터 셋으로 훈련을 했을 경우 모델의 성능이 크게 향상이 된다. 따라서 대규모 훈련이 inductive bias를 능가하는 것을 확인을 하였다. Method 위 사진은 ViT의 전체적인 구성에 대한 그림이다. 본 논문에서는 최대한 가능한 원래의 Transformer구조를 따를려고 했다. 그렇게 되면 확장 가능한 NLP Transformer 아키텍처와 그 효율적인 구현을 거의 그대로 사용할 수 있다는 장점이 있기 때문이다. Vision Transformer (ViT) ViT의 전체적인 구조를 설명하고 있다. 크게 이미지 입력 처리, 패치 임베딩, [class] 토큰 추가, position 임베딩, Transformer Encoder 구조 순으로 설명할 것이다. 먼저 이미지 입력 처리에서 입력 이미지는 $ H * W * C $ (H,W 이미지 높이와 너비, C: 채널 수) 크기를 가지지만 1D 시퀀스의 입력을 받는 Transformer의 구조에 맞게 변형이 필요하다. 따라서 $ H * W * C $ 에서 $ N * (P^2 * C) $ ($N = \frac{H*W}{P^2}$, $P^2$은 각 패치의 크기, C: 채널 수)로 변환을 해야한다. 또한 이 각 패치는 입력에 맞게 linear projection되어 고정된 크기 $D$의 벡터로 매핑이 되어 이 과정을 통해 입력 데이터를 학습 가능하게 변환한다. Linear projcetion: 입력 벡터에 선형 변환을 적용하여 다른 차원의 벡터로 변환하는 과정. $y = W * x + b$ 일 때 $y$는 출력 백터로 고정된 크기의 $D$차원의 벡터이고, W는 가중치 행렬로 학습 가능한 행렬이다. 이미지 패치가 $16 * 16 * 3$일 때 이 패치를 펼쳐서 $16 * 16 * 3 = 768$크기의 벡터로 나타낼 수 있다 이때 Linear projection을 통해 $D = 512$ 크기의 벡터로 변환하려면, $512 * 768$크기의 학습 가능한 가중치 행렬 $W$를 사용해 Linear projection을 사용하여 다른 차원의 벡터로 변환하는 것이다.]]></description>
</item>
<item>
    <title>[Paper Review]Attention Is All You Need</title>
    <link>https://goodyoung.github.io/posts/paper/transformer/</link>
    <pubDate>Wed, 24 Jul 2024 11:32:59 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/transformer/</guid>
    <description><![CDATA[개요 요즘 모든 분야에서 자주 사용되고 있는 아키텍쳐인 Transformer에 대한 논문 리뷰를 해보려고 한다. Abstract 기존의 sequence transduction model(sequence 간 변형이 이루어 지는)은 complex recurrent나 cnn기반으로 구성되어 있다. 그리고 좋은 성능을 내는 것이 attention 기반에 encoder &amp; decoder 기반으로 연결되어 있는 네트워크이다. 본 논문에서는 오로지 attention mechanism만 사용하는 Transformer를 제안한다. 즉, 순환이나 cnn의 연산을 사용하지 않는 네트워크이다. 이는 훈련시간 절감, 행렬곱을 이용하여 병렬적으로 수행하면서 번역 작업에 우수한 품질을 보여준다. English-German 번역 작업에 28.4 BLEU를 달성하였고 다른 task에서도 잘 작동한다. Introduction RNN, long-term memory, GRU이 language modeling, machine translation 문제에서 SOTA를 달성하고 있다. 많은 연구가 encoder-decoder의와 순환 language model의 성과를 달성하려고 하고 있다. 순환 모델의 연산 특성은 training sample 내에서 병렬화를 방해하며, 시퀀스 길이가 길어질수록 메모리 제약으로 인해 예제 간 배칭이 제한되므로 병렬화가 중요해집니다. 기존 RNN의 단점 따라서 기존의 RNN의 단점에 대해서 간략히 살펴 보겠다. 위 그림처럼 sequence에 포함되어 있는 순서 정보를 정렬하고 이것을 차례대로 hidden state의 값을 반복을 통하여 갱신하기 때문에 병렬적으로 수행하기에 어렵다는 사실을 알 수 있게 된다. 또한 input 단어가 많아지면 encoder의 마지막 부분의 출력인 context vector를 만들어야 하기 때문에 병목의 문제도 있다. 다시 논문 내용으로 넘어가면 Factorization trick과 conditional computation의 연구로 인해 이러한 문제점은 상당히 해결되고 있지만 여전히 sequential computation은 제약이다. Attention은 input과 output의 거리에 관여하지 않고 model의 종속성을 허락해준다. 이러한 attention mechanism은 주로 순환 신경망과 함께 사용이 되어 왔다. Attention RNN처럼 하나의 context vector가 소스 문장의 모든 정보를 가지고 있어야하는 단점을 해결하기 위해 나온 기법이 Attention이다. Attention은 매번 출력 단어를 만들어 낼 때마다 소스 문장의 출력 정보 중에서 어떤 정보가 가장 중요한지 가중치를 부여해서 출력 단어를 보다 효과적으로 생성할 수 있게 한다. 간단히 매번 소스 문장에서의 출력 전부를 decoder의 입력으로 받는 방식이다. 결론적으로 본 논문에서 제안하는 Transformer는 순환을 피하고 대신에 attention mechanism에 온전히 의존하여 input과 output 사이의 ``global dependencies를 이끈다. 이러한 구조는 병렬 처리에 특화되었다. Background 기존 연구들은 CNN을 사용하여 sequenrtial computation을 줄이는데 초점을 맞추었지만 이 방법은 다른 position 사이의 종속성을 배우기 쉽지 않다. 하지만 Multi-Head Attention의 방법을 사용하여 상수 시간으로 줄였다. 또한 기존 순환 end-to-end memory network는 attention mechanism을 사용했다. 하지만 Transformer에서는 RNN이나 CNN을 사용하지 않고 input output의 representation을 계산하는 전체의 self-attention을 사용한다. 여기서 나온 Self-Attention이란 input으로 들어오는 각각의 단어가 서로에게 얼만큼의 영향을 미치는지 알려준다. 문맥에 대한 정보를 잘 학습하도록 만드는 것이다. Model Architecture 기존 모델들은 encoder-decoder구조를 따르는데 이전 단계에서 생성한 symbol을 활용해서 decoder가 다음번에 나올 output을 만든다. Transformer 또한 encoder-decoder의 구조를 띄는데 모델을 순환적으로 사용하지 않고 attention mechanism만 활용하여 sequence에 대한 정보를 한 번에 입력으로 준다는 것이 특징이다. 위의 그림은 transformer의 전체적인 구조인데 이를 앞으로 하나씩 살펴볼 예정이다. Encoder and Decoder Stacks Encoder부분은 여러번 stack이 가능한 구조이고 하나의 layer는 크게 두가지의 구조를 지닌다. multi-head self attention feed-forward network 또한 이 두가지의 구조 모두 residual connection을 활용하여 Identity mapping을 거치게 한다. Decoder부분은 여러번 stack이 가능한 구조이고 보통 encoder랑 같은 layer의 개수랑 맞춘다. Decoder third sub-layer에 encoder의 output값을 활용하여 multi-head attention을 수행하도록 한다. residual connection을 활용하여 보다 더 쉽게 global optima를 찾도록 한다. 이전에 등장한 단어들만 참고할 수 있도록 mask를 씌워서 multi-head attention을 사용할 수 있도록 한다. Attention Attention Function은 쿼리와 key-value 쌍을 output으로 mapping을 한다. Query, keys, values, output들은 다 vector이다. Query(Q): 물어보는 주체 (어떤 단어가 가장 중요했는지를 key에서 계산하여 결과를 낸다.]]></description>
</item>
<item>
    <title>[Paper Review]Fast R-CNN</title>
    <link>https://goodyoung.github.io/posts/paper/fast-rcnn/</link>
    <pubDate>Wed, 17 Jul 2024 18:19:31 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/fast-rcnn/</guid>
    <description><![CDATA[개요 Object Detection분야에서 널리 사용되는 딥러닝 모델인 Fast R-CNN에 대한 논문 리뷰를 해보려고 한다. 이번 Fast R-CNN은 R-CNN의 단점을 고안하고자 나온 모델이다. Introduction 본 논문이 나온 시기의 detection은 classification보다 더 복잡한 모델로 해결을 했는데 multi stage pipeline을 가진 모델들은 slow하고 inelegant하다. 이런 complexity는 object의 localization 때문에 일어난다. 이는 두가지 문제점을 지니고 있는데 수많은 후보 object들이 제안된다 이 후보들이 정확한 localization을 하기 위해 다시 refine해야 된다. 따라서 본 논문에서는 이전 R-CNN과는 달리 single-stage(분류하고 공간 정보를 강화하는) 훈련 기법을 제안한다 VGG16을 사용했으며 R-CNN보다 9배 빠르고, SppNet보다 3배 빠르다. 그리고 논문에서는 이전 모델인 R-CNN의 몇가지 단점을 설명한다. 훈련 시 multi-stage pipeline이다.
object proposals을 cnn을 통하여 특징 추출을 하고 그것들의 feature를 svm이 분류를 해주고 마지막으로 bounding box regressor를 통해 3단계를 거친다. 훈련 시 공간과 시간적으로 낭비가 된다.
svm, bounding-box regressor를 할 때, 오버헤드가 심하다 마지막으로 이미지를 test를 할 때 이미지당 47초가 걸린다.
R-CNN은 계산량 공유 없이 각각의 object마다 계산을 해서 오래걸린다. SppNet은 이러한 점을 극복했다. SppNet은 전체 이미지의 feature map을 계산 한 후 거기서 각각의 object proposal을 분류 한다. proposal을 위해 feature에서 고정된 크기로 추출한다. 그리고 다중 출력 크기로 추출한 다음 그것을 spatial pytamid pooling에서 합친다. 이를 통해 테스트 시간(10~100배)과 훈련 시간(3배)을 크게 줄일 수 있습니다. 근데 이러한 SppNet도 단점이 있다. 여러 단계를 거친 pipeline이라는 것이다. 하지만 R-CNN과 달리, SppNet에서 제안된 미세 조정 알고리즘은 공간 피라미드 풀링 이전의 합성곱 층들을 업데이트할 수 없다. 이러한 제한 사항(고정된 합성곱 층들)은 매우 깊은 네트워크의 정확도를 제한한다. 따라서 이런 단점들을 보안하고자 Fast R-CNN을 고안했다. Fast R-CNN은 몇가지 장점이 있다. 다른 것들 보다 높은 mAP(점수) multi task loss를 사용한 single-stage 훈련 기법 모든 network layer가 update된다. 특징 추출에 disk storage가 필요하지 않다. 사진은 Fast R-CNN의 architecture의 overview이다. Fast R-CNN은 input으로 전체 이미지를 넣고 그와 함께 object proposal (selective search로 구해둠)의 set을 같이 넣는다. 그럼 network는 여러 conv를 거쳐 conv feature map을 생성한다. 그럼 각각의 RoI pooling layer은 추출한다. 고정된 크기의 feature vector가 생성이 된다. (ROI들은 각각 다른 크기를 지녔기 때문) 그 후 Fully Connected (FC)층으로 가며 이것은 또 2가지 분기로 나뉜다. 하나는 (K +1 class의)softmax 확률 추정치를 구한다 다른 하나는 각 K 개의 객체 클래스에 대해 4개의 실수 값을 출력합니다. 4개의 값 집합 각각은 K 개 클래스 중 하나에 대한 세밀한 바운딩 박스 위치를 인코딩합니다. RoI pooling layer RoI pooling layer는 max pooling을 사용하여 region of interest(RoI)를 고정된 크기의 spatial small feature map으로 변환한다. Selective search를 통해 resion proposal을 얻게 된다. 이때 spatial small feature map의 $H, W$의 값은 특정 RoI와는 독립적인 하이퍼 파라미터이다. RoI는 합성곱 특징 맵(conv feature map) 내의 사각형 창을 의미합니다. 각각의 RoI는 **(r,c,h,w)**의 특징을 지니고 있는데 **(r,c)**는 top-left를 의미하고 height와 width는 (h,w)를 의미한다. RoI max pooling은 $h × w$ 크기의 RoI 창을 $h/H × w/W$ 크기의 grid를 만든다. 그 후 grid에 max pooling하여 해당 $H × W$ 크기의 출력 grid 셀에 넣는 방식으로 동작합니다. Pooling은 표준 max pooling에서처럼 각 feature map channel에 독립적으로 적용됩니다. 이는 Sppnet에서 하나의 pyramid level만 사용한 것과 동일하다. 결론적으로 원래 이미지를 CNN에 통과시킨 후 나온 feature map에 이전에 생성한 RoI를 projection시키고, 이 RoI를 FC layer input 크기에 맞게 고정된 크기로 변형할 수가 있다. 이를 통해 RCNN 처럼 2000번의 CNN연산 필요 없이, 단 한번의 연산으로 속도를 대폭 높일 수 있게 된다.]]></description>
</item>
<item>
    <title>[Paper Review]Rich feature hierarchies for accurate object detection and semantic segmentation</title>
    <link>https://goodyoung.github.io/posts/paper/r-cnn/</link>
    <pubDate>Wed, 10 Jul 2024 16:01:50 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/r-cnn/</guid>
    <description><![CDATA[개요 Object Detection분야에서 널리 사용되는 딥러닝 모델인 RCNN에 대한 논문 리뷰를 해보려고 한다. RCNN은 이미지 내에서 객체를 정확하게 탐지하고 분류하는 문제를 해결하기 위해 개발되었다. Abstract 최근 몇 년간 Object Detection 분야가 정체 상태에 있었다. 그동안 가장 성능이 좋은 방법론은 low-level image feature와 high-level context를 섞는 것이다. 본 논문에서는 간단하고 확장 가능한 object-algorithm을 제시한다. 이는 VOC2012에서 SOTA 결과 대비 30%나 향상된 mAP를 보여준다. 본 논문에서는 두가지 방법을 사용한다 high-capacity cnn을 하위의 region proposal에 적용시킨다. 이를 통해 RCNN이라고 불린다. data가 부족할 때, 사전 학습 된 모델을 도메인 특화 미세 조정을 하면 성능이 크게 향상된다. 따라서 region proposal을 CNN과 결합했기 때문에 R-CNN(Regions with CNN features)라고 한다. Introduction 지난 10년간 visual recognition task에서의 진전은 SIFT와 HOG의 사용에 의존되어 왔다. 둘 다 컴퓨터 비전에서 널리 사용되는 두 가지 이미지 특징 추출 기법이다. SIFT: 다양한 스케일과 회전에 대해 불변인 키포인트와 불변인 특징을 만드는 기법이다. HOG: 이미지의 지역적인 형태나 외곽선을 표현하는 방법으로, 물체 탐지, 특히 사람 탐지에 사용된다. 2010~2012년가지 소폭적인 개선만 이루어 졌다. AlexNet의 개발로 인하여 CNN이 크게 향상이 되었다. 그 후 Classification이 Object Detection의 결과에 어느정도의 영향을 미치는지에 대해 관건이었다. 본 논문은 HOG와 같은 기법들과 비교하여 CNN이 Object Detection 성능을 향상시켰음을 보여준다. 이러한 결과를 얻기 위하여 두가지 문제에 집중하였다. Deep network에서의 localizing object 작은 양의 detection data로 높은 capacity model 훈련하기 Classification과는 달리 Object Detection에서는 localization이 문제이다.이를 해결하기 위한 방법은 2가지 방법이 있다. Regression problem으로 설정 실용적으로 좋지 않다. (30%의 결과가 나온다) Sliding-window detector를 구축 본 논문의 CNN은 좀 더 깊은(다섯 개의 CNN layer) layer를 구축하였는데 이는 높은 spatial resolution을 유지하기 어렵다. 따라서 이는 객체의 정확한 위치를 찾는데 어려움이 있다는 것이고 이 역시 아직 남아있는 과제임을 나타낸다. 본 논문은 첫 번째 문제를 Recognition using region을 통해 해결하려고 한다. 위 그림 처럼 Region-proposal을 통해 2000개의 카테고리를 만들고 이를 CNN을 사용해 고정적인 길이의 특징 벡터를 추출 한다. 이때 입력되는 이미지의 사이즈도 고정되어야 하기 때문에 아핀 변환 등으로 이미지를 추출한 후 입력으로 사용한다. 그 후 이를 선형 SVM으로 분류한다. 이는 영역의 크기에 상관없이 동일한 크기로 변환이 된다. 본 논문에서는 이를 Region-proposal과 CNN을 같이 사용하므로 R-CNN이라고 한다. 본 논문은 두 번째의 문제를 사전 학습 미세조정에 따른 비지도 사전 훈련을 허용하여 해결했다. ILSVRC인 임의의 큰 데이터로 지도 학습을 한 모델에 PASCAL의 작은 데이터를 domain 특화 미세 조정을 하는 패러다임을 제시한다. 이를 통해 결과가 33%가 올랐다. Object Detection with R-CNN 크게 3가지 모듈을 포함하고 있다. Category-independent region proposal Large Convolutional neural network Class specific linear SVM 본 논문에서는 Region-proposal을 생성하기 위해 Selective Search방법을 이용한다.
Selective Search란 색상, 질감, 영역크기 등을 이용해 non-objective segmentation을 수행한다. 이 작업을 통해 좌측 제일 하단 그림과 같이 많은 small segmented areas들을 얻을 수 있다. Bottom-up 방식으로 small segemented areas들을 합쳐서 더 큰 segemented areas들을 만든다. 두 번째의 작업을 반복하여 최종적으로 2000개의 region proposal을 생성한다. 또한 본 논문에서는 AlexNet의 모델을 사용하여 $227 * 227$의 고정적 크기인 이미지를 받게 한다.
따라서 임의의 다양한 크기를 가진 영역들을 고정된 크기로 바꾸는 작업인 warping의 과정을 거친다. 2000장의 region-proposal이 selective-search에 의해 나오면 ground-truth와 IoU를 비교하여 0.5 보다 큰 경우를 positive로 구분하고 그 외를 negative로 구분한다. 또한 positive랑 negative가 겹치는 객체를 정확히 탐지하기 위하여 IoU overlap threshold를 사용하여 IoU 임계치를 주어 객체 탐지 성능을 높인다.]]></description>
</item>
<item>
    <title>[Paper Review]DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(DeepLab V2)</title>
    <link>https://goodyoung.github.io/posts/paper/deeplab-v2/</link>
    <pubDate>Sat, 06 Jul 2024 17:22:47 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/deeplab-v2/</guid>
    <description><![CDATA[개요 DeepLab V1에 이어서 DeepLab V2에 대한 논문 리뷰를 해보려고 한다. V1과 엄청나게 큰 차이는 없지만 방법론의 변화가 있었다. Abstract 본 논문에서 해당 모델(DeepLab-V2)에서 사용하고 있는 세가지의 주된 방법을 설명하고 있다. 첫 번째로 Atrous convolutional이다. 이는 파라미터의 수 증가 없이 더 많은 context들을 포함하여 FOV(Field of View)를 증가 시킨다. 두 번째로 Atrous spatial pyramid pooling (ASPP)이다. 이는 multi scale로 image context를 다양한 context들을 효과적으로 포착이 가능하다. 따라서 ASPP를 사용하게 되면 더욱 robust하게 객체를 분할 할 수 있다고 설명한다. 세 번째로 Conditional Random Field (CRF)이다. 이는 max-pooling, downsampling의 결합이 배치되어 invariance가 있던 것들을 CRF를 사용하면서 질, 양적으로 localization 성능을 향상시킨다고 나온다. Introduction hand-crafted feature보다 더 좋은 성능을 나타내기 때문에 Deep Convolutional Neural Networks (DCNN)이 classification, object detection에서 많이 사용된다. 이러한 성공을 이끈 것은 image 변형에 대한 추상적인 data representation을 학습하게 하는 built-in invariance(불변성)이다. 하지만 이런 invariance는 classification task에는 좋지만 spatial information이 undesired한 segmentation에는 좋지 않다. 따라서 본 논문에서는 이런 단점을 극복하기 위해 아래 세가지 사항을 고려 했다. reduce feature resolution existence of object at multiple scale reduce localization accuracy 첫 번째 challenge는 max-pooling과 downsampling이 반복되어서 나타낸다. 이는 spatial resolution을 줄이기 때문에 안좋다. 이를 해결하기 위해서 우린 맨 마지막 단의 몇개의 maxpooling layer를 제거하고 높은 sampling 비율(더 높은 해상도)로 계산하기 위하여 대신에 upsample 하는 filter를 추가한다. 이때 filter는 hole algorithm을 사용한 atrous convolution을 추가한다. 실제 atrous convolution과 hole algorithm을 사용하여 계산량을 증가시키지 않고 FOV를 증가시켰다. 두 번째 challenge는 기존의 방법은 동일한 이미지를 rescale 버전을 DCNN에 입력한 후 얻은 feature map을 aggregate한다. 이 방법은 performance는 증가하지만 computing overhead가 발생한다. 따라서 본 논문에서는 spatial pyramid pooling (SPP)을 사용하여 convolution 전에 주어진 feature layer를 resampling 하는 효율적인 구조를 제시한다. 이것은 원본 이미지를 여러(Multiple) 필터로 보는 것과 같아서 효율적인 FOV, useful한 multi scale의 관점에서 다양한 image context를 포착할 수 있다. 우린 병렬로 atrous convolutional layer를 사용한다. 이를 &lsquo;atrous spatial pyramid pooling&rsquo; (ASPP)라고 불린다. 세 번째 challenge는 object-centric classifier는 공간적 변형에 대한 불변성이 필요하다. 이를 해결하기 위해선 마지막 segmentation결과를 계산하기 위한 hyper-column의 특징을 뽑는 skip layer를 사용하여 해결한다. 미세한 edge detail을 포착하기 위하여 fully connected pairwise CRF를 사용한다. 또한, CRF랑 DCNN이랑 연결했을 때 SOTA를 달성했다. 마지막으로 따라서 본 논문에서 DCNN 모델로 VGG-16, ResNet-101을 사용했고 Fully convolutional 한 층을 사용하였다. Methods 앞에서 말한 세가지 방법에 대해 좀 더 자세히 설명하도록 하겠다. Atrous Convolution for Dense Feature Extraction and Field-of-View Enlargement x[i]: input w[k]: filter (length: K) r: rate,stride 1차원에서 atrous convolution의 기존 식은 위 그림과 같다. 하지만 atrous convolution을 사용 하기 위해 위 그림과 같이 r의 값을 조절하여 high resolution input feature map에도 사용할 수 있다. 또한 위 그림 처럼 2-D일때의 feature map의 특징을 보면 더욱 뚜렷한 것을 확인할 수 있다. 기존 방식대로 커널을 사용하게 된다면 1/4 의 이미지의 위치에 있는 데이터만 얻을 수 있게 된다. 하지만 atrous convolution을 사용하게 된다면 모든 이미지에 대한 정보를 얻을 수 있기에 spatial resolution이 증가한다. atorous convolutional layer를 모든 층에 사용하기엔 비용적 오버헤드가 발생하여 하이브리드 전략을 취한다. 따라서 본 논문에서는 마지막 풀링, convolution layer에 atrous convolution layer를 추가하여 (stride = 2) 4배로 늘리고, 이중 선형 보간법을 사용해 8배로 늘려 원래 이미지 해상도에서 특징 맵을 복원한다. 이런 atrous convolution은 어떤 레이어에서든 FOV(field of view)를 임의로 확대할 수 있게 된다.]]></description>
</item>
<item>
    <title>[Paper Review]Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs(DeepLab V1)</title>
    <link>https://goodyoung.github.io/posts/paper/deeplab-v1/</link>
    <pubDate>Thu, 30 May 2024 21:38:53 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/deeplab-v1/</guid>
    <description><![CDATA[개요 Image Segmentation 문제를 해결하기 위해 제안된 대표적인 딥러닝 모델인 DeepLab V1에 대한 논문 리뷰를 해보려고 한다. Abstract 기존 Deep Convolutional Neural Networks이하(DCNN)의 마지막 층에서 충분한 localized가 이루어지지 않았기 때문에 고수준 작업(이미지 분류)에서 좋은 성능을 발휘한다. 그러나 이런 불변성 특성 때문에 마지막 층의 응답은 특정 객체의 위치를 세밀하게 나타내지 못하여, 각 픽셀의 정확한 분류가 필요한 객체 분할(segementation)에는 단점이 될 수 있다. 불변성이란 특정 객체의 위치나 회전 여부에 상관없이 객체의 가장 두드러지는 특징을 추출할 수 있다는 것이다.
따라서 이러한 단점을 극복하기 위해 CRF랑 DCNN 마지막 층과 결합 하고, GPU에서 초당 8 프레임의 계산이 가능하게 한 hole 알고리즘을 앞으로 설명할 것 이다. Introduction 지난 2년 동안 DCNN은 이미지 분류, 객체 탐지 등 고수준 문제에서의 컴퓨터 비전 시스템의 성능을 향상시켰다. End-to-end 방식으로 학습된 DCNN이 SIFT, HOG과 같은 의존 시스템 보다 현저히 더 나은 결과를 제공한다. 이러한 이유는 불변성 덕분이었지만 지역적 정확도를 원하는 pose estimation, semantic segmentation등의 작업(low-level task)에서는 방해가 된다. signal down-sampling과 spatial insensitivity(invariance)이라는 두가지의 어려움이 있었다. signal down-sampling은 반복되는 downsampling 작업과 max-pool작업 때문에 낮아지는 해상도가 문제였다. 본 논문에서는 해당 문제를 atrous (with holes)알고리즘을 통해 해결을 하려고 한다. invariance은 DCNN의 특징 때문에 공간적 정확성을 본질적으로 제한시키는 문제가 있다. 본 논문에서는 해당 문제를 fully-connected Conditional Random Field(CRF)로 해걸하려고 한다. CRC은 Krahenbühl &amp; Koltun이 제안한 fully connected pairwise를 사용한다. 이는 부스팅 기반의 픽셀 분류기의 성능을 크게 향상시킨다. 아래는 DeepLab system의 세가지 큰 장점들 이다. speed atrous algorithm accuracy PASCAL semantic segmentation challenge에서 SOTA달성 simplicity DCNN &amp; CRFs으로 구성되어 있다. Related Work Segmentation에 대한 여러 work들이 있었지만 DeepLab은 FCN과 비슷하게 픽셀 표현에 직접 작동한다. FCN, 다른 모델들과 DeepLab의 차이점은 CRF와 DCNN-based unary term의 결합이라고 볼 수 있다. 본 논문 이후 더 향상된 방법을 사용한 여러 DeepLab의 버전(V2,V3)들이 나와 있다. CONVOLUTIONAL NEURAL NETWORKS FOR DENSE IMAGE LABELING [그림 1] DeepLabv1의 DCNN 구조 VGG16으로 모델을 구성하였으며 더 효율적인 특징 추출기를 사용하였고 그것을 기반으로 finetuned와 re-purposed를 하였다. EFFICIENT DENSE SLIDING WINDOW FEATURE EXTRACTION WITH THE HOLE ALGORITHM 본 논문에선 효과적인 CNN 특징 추출을 위한 방법에 대해서 설명을 한다. 먼저 그것을 구현하기 앞서 VGG16을 FCN과 같이 FC층을 Fully Convolutional층으로 변환을 하고 최종 이미지가 원본의 해상도를 얻게 변환을 한다. 이때 32 pixel 간격으로 매우 드물게 계산이 되기 때문에 이를 8 pixel로 변경이 필요하였다. 따라서 마지막 2개의 pooling층을 스킵하였다. 32 pixel로 한다면 localization이 떨어지기 때문에 8 pixel로 변경이 필요했다. 또한 VGG16의 네트워크는 5개의 pooling층이 존재하는 네트워크이다. 따라서 마지막 2개를 스킵한다면 원본 해상도의 1/8만큼의 해상도가 나오게 되는 것이다. 원본 이미지의 해상도를 복구 하기 위하여 즉, upsample을 하기 위하여 filter에 zero를 추가하여 sparse한 filter를 얻는 방식의 hole algorithm을 사용하였다. 따라서 마지막 3개의 convolutional은 2배가 되었고 first fully connected layer는 4배가 되었다. 이때 필터를 그대로 유지하면서 각각 입력 스트라이드 2 또는 4 픽셀을 사용하여 적용되는 특징 맵을 드문드문 샘플링함으로써 이 방법을 더 효율적으로 구현할 수 있게 된다. [그림 2] Atrous Convoltion 이때 나타나는 hole algorithm (atrous convolution)이란 필터 중간 중간에 0을 채워 넣어서 학습해야할 파라미터 수는 유지하면서 보다 넓은 영역(Receptive Field)을 참조하게끔 하는 방법이다. Receptive Field(수용 영역): 특징 추출에 사용되는 한 개의 뉴런이 받아들일 수 있는 입력 데이터의 영역
따라서 atrous convolution을 사용하면 Receptive Field가 넓어지며 localization이 완화되는 효과가 있다. 따라서 이런 atrous convolution을 사용하게 된다면 두가지의 이점이 발생한다.]]></description>
</item>
<item>
    <title>[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)</title>
    <link>https://goodyoung.github.io/posts/paper/unet/</link>
    <pubDate>Fri, 17 May 2024 12:59:43 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/unet/</guid>
    <description><![CDATA[개요 Image Segmentation 문제를 해결하기 위해 제안된 대표적인 딥러닝 모델인 UNet에 대한 논문 리뷰를 해보려고 한다. Abstract 딥러닝 네트워크에선 성공적인 훈련을 위해선 수많은 annotated training samples들이 필요한데 이것들을 효율적으로 사용할 수 있는 강력한 data augmentation 방법에 대해서 소개한다. 그 다음으론 Unet의 구조에 대해서 간략한 소개가 나온다. Unet의 구조는 context들을 포착할 수 있는 contracting path와 localization을 정확하게 해주는 expanding path가 서로 대칭적으로 이루어져 있다는 것을 알려준다. 그리고 또한 Unet은 적은 양의 image로 학습을 할 수 있고 ISBI challenge에서 그 전 모델보다 더 좋은 성능을 보여주었다. 이런 Unet에 대해서 좀 더 자세히 Introduction의 파트에서 설명하게 될 것이다. Introduction 본 논문에서는 지난 2년 간 deep convolutional net이 성공을 이룬 이유는 Krizhevsky의 연구(Alex Net) 때문이라고 설명했다. 보통 convolutional network의 사용은 classification task(하나의 이미지를 single class label) 였지만 biomedical image task는 localization(class label이 픽셀 단위로 분류) 이 필요했다. 또한 biomedical tasks에서는 수많은 양의 dataset들을 구하기 어려운 한계도 있었다. [그림 1] sliding-window 따라서 위 그림 처럼 이전에 sliding-window 방식으로 픽셀의 주변 patch가 한 단위가 되어 pixel predict를 하여 localization을 수행하는 방법이 있었지만 두가지의 단점이 있었다. 모든 patch 별로 연산을 하므로 시간이 오래 걸린다. 수많은 중복된 패치들이 많다. patch 크기가 크면 max-pooling layer가 많아져서 localization accuracy가 떨어지고 반대로 patch 크기가 작아지면 littel context를 하여서 localization accuracy와 use of context 간의 trade off가 있다. 따라서 이러한 단점들을 해결하기 위해 Fully convolutional network의 구조를 변형하여 더욱 elegant한 구조로 모델을 만들었다. 적은 데이터셋으로도 work가 가능하고 더욱 더 정밀한 segmentation이 가능해짐 [그림 2] unet main idea Unet의 main idea는 contracting layer(downsampling) 에서 pooling operator가 upsampling operator로 교체되는 연속적인 layer들을 보완하는 것이다. contracting path로 부터 high resolution feature들이 upsampled output과 합쳐진다. 그런 다음 연속적인 합성곱 층들이 더 precise한 output을 조합한다. output의 해상도를 증가시킨다. Upsampling part에서 더 많은 feature channel들을 가지고 있게 된다. 이는 context 정보가 high resolution으로 잘 전달이 되게 한 것이다. 따라서 contracting path와 expansive path가 U-shaped architecture가 된다. 또한 Unet은 fully connected layer를 사용하지 않고 오직 각각의 convolution의 필요한 부분만 사용을 하였다. 이것은 overlap-tile strategy으로 임의의 큰 이미지의 seamless segmentation을 가능하게 한다. 다른 방식은 GPU memory의 한계 때문에 거대한 이미지를 처리할 수 없었다. Sliding Window 방식과 유사하지만 1개의 픽셀만 classification을 하는 것이 아닌 패치 영역을 classification을 한다. 또한 전체 영역을 다 예측하는 것이 아닌 572의 입력을 받아 388 크기의 아웃풋을 생성한다. fully connected layer를 사용하지 않고 오직 convolution만 사용하여 임의의 이미지를 입력받아도 문제가 없다. 큰 이미지를 한정된 GPU에서 학습하게 된다. Border region을 예측하기 위해서 missing context는 input image를 mirroring 하여 extrapolated(나타나게 만들 수 없는 부분을 예측)로 예측이 된다. 위의 오른쪽 그림을 보면 노란색 박스 영역을 예측 하려면 파란색 박스 영역의 이미지(노란 부분보다 더 큰 크기)가 필요한데 이렇게 누락되는 부분은 mirroring의 전략을 가져간다. 실제로 오른쪽 그림의 왼쪽을 자세히 보면 테두리의 이미지가 원본 이미지랑 대칭임을 볼 수 있게 된다. 또한 그 다음 그림은 단계로 넘어가는 과정인데 이전에 예측에 사용했던 patch와 겹치기 때문에 overlap-tile이라고 불린다. 또 만약 572의 입력이 들어올 때 388로 예측을 하기 때문에 외각 부분은 예측할 수 없어서 이 때문에 mirroring 전략을 취하는 것이다. [그림 3] elastic deformation Unet은 little training data에서도 가능했다. 모델 학습 시 data augmentation을 image의 invariance(이미지 불변성)를 학습하는 elastic deformation방법을 적용하였다. deformation: 변형
image를 다양하게 변형하여 augmentation을 하는 방법이다.]]></description>
</item>
<item>
    <title>[Paper Review]Fully Convolutional Networks for Semantic Segmentation(FCN) &amp; Implement</title>
    <link>https://goodyoung.github.io/posts/paper/fcn/</link>
    <pubDate>Sun, 14 Apr 2024 16:25:24 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/fcn/</guid>
    <description><![CDATA[개요 Semantic Segmentation 문제를 해결하기 위해 제안된 딥러닝 모델인 Fully Convolutional Networks(이하 FCN) 논문 리뷰를 해보려고 한다. Abstract FCN은 convolutional network 자체로 end-to-end 학습, pixels-to-pixels을 훈련을 하고 semantic segmentation에서 state-of-the-art(이하 SOTA)를 달성했다.
이때 나오는 end-to-end 학습이란 독립적인 것이 아니라 하나의 모델에서 작업이 다 끝나고, 모델의 모든 filter들이 학습이 가능한 학습을 하는 학습 방법이라고 생각하면 된다. [그림 1] end-to-end 학습 end-to-end 학습이 아닌 모델인 pattern recognition모델은 사람들이 직접 filter를 설계하여 classifier만 학습을 하는 방법이다. 본 논문의 핵심 아이디어는 임의의 크기인 input과 input과 같은 크기의 output을 생성하는 fully convolutional network를 구축하는 것이다. 또한 기존의 classification networks(AlexNet, VGG net, GoogLeNet)을 통해 transfer learning을 통한 fine-tuning을 진행하였다. 마지막으로 모델의 deep layer에서 얻은 의미(Semantic) 정보와 shallow layer에서 얻은 외관(Appearance)정보를 섞는 skip architecture를 정의하였다. [그림 2] CNN layers Semantic information deep, coarse(굵다) layer
CNN에서 제일 deep한 위치에 있는 layer들이 객체의 외관은 파악하기 힘든 반면에 feature들이 활성화가 된 부분을 보면 의미가 있는 정보를 나타낸다. Appearance information shallow, fine layer
CNN의 첫 번째 layer에 있는 filter들은 보통 edge feature들을 추출하기 때문에 윤곽과 관련된 feature들을 추출한다. Introduction Convolutional networks의 등장으로 인해 classification, local task(object detection, key-point등)와 같은 분야에서 엄청난 발전을 이룰 수 있었다. 이러한 Convolutional networks의 다음 단계는 segmentation을 위한 모든 pixel에 대한 예측이다. 또한 기존의 방법들 (patchwise training, pre-post processing)등의 수고로움을 해결할 수 있다고 나와있다. 이때 설명하는 patchwise training이란 FCN이 나오기 전 segmentation 학습 방법론 이다. [그림 3] Patchwise training Patchwise learning 방식 특정 크기의 patch를 설정 및 CNN input 이때의 input -&gt; CNN에 의해 classification이 된다. 이때 특정 class로 분류가 되었다면, 해당 patch 중앙에 위치한 pixel을 분류된 class로 분류한다. 이 과정을 슬라이딩 윈도우 방식으로 모든 픽셀을 반복한다. 따라서 Patchwise learning의 방법에는 여러 단점이 생기게 된다. 많은 계산량 patch끼리 겹칠 때의 중복 계산의 우려, patch의 크기를 크게할 때 분류가 애매한 상황 patch 크기를 줄여주면 low resolution(해상도가 낮음)의 문제 마지막으로 FCN이 등장하기 전 semantic한 정보와 location한 정보를 어떻게 잘 조합할 지 몰랐다. 이를 해결하기 위해서 skip architecture가 나오게 된다. Fully convolutional networks 이제까진 기존 CNN에 대한 설명과 FCN이 나오기전의 모델들의 단점들을 살펴보며 FCN 모델의 우수성을 추상적으로 설명했다. 이젠 FCN의 알고리즘을 구체적으로 설명 해보려고 한다. Adapting classifiers for dense prediction [그림 4] FC -&gt; Convolutional layers 전형적인 분류 net(LeNet, AlexNet &hellip;)은 FC층 때문에 고정적인 input size와 non-spatial output을 가진다. 입력 이미지의 크기에 비례하여 FC층에 들어가기 위해 flatten되는 neuron의 수가 비례하기 때문에 input size가 고정적이다. 이때 flatten 되기 때문에 이미지의 공간적 정보의 손실이 있게 된다. 따라서 이러한 linear한 FC층을 [그림 1]처럼 Fully Convolutional한 구조로 변경을 하였다. 이로써 모든 layer에 Conv를 적용하여 ground truth를 각 layer의 출력으로부터 얻을 수 있어 forward와 backward가 계산 효율성에서 장점을 얻는다. 또한 단점이었던 공간 정보의 손실이 없어지는 것도 해결하였다. convolutionalization된 층의 resulting map에 해당되는 특정 위치가 특정 patch 상의 CNN 결과랑 같게 된다. 지금까진 FCN구조의 encoder 부분을 설명한 것과 같다. 다음으론 output map(coarse output)으로 부터 dense prediction을 하는 방법을 알아 볼 것 이다. Shift-and-stitch is filter rarefaction dense prediction을 하기 위해 output map을 upscaling을 해야 한다. upscaling을 하기 위한 방법으로 본문에선 shift-and-stitch방법을 고려했다고 나타난다. [그림 5] shift and stitch 그림을 토대로 max pooling을 하고 위치 정보를 저장하여 원래의 이미지 크기로 upscaling이 가능하다. 하지만 계산 비용이 큰 단점이 있게 된다.]]></description>
</item>
</channel>
</rss>
