<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LDM - Tag - Good Young</title>
        <link>https://goodyoung.github.io/tags/ldm/</link>
        <description>LDM - Tag - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 17 Feb 2025 17:57:23 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/tags/ldm/" rel="self" type="application/rss+xml" /><item>
    <title>[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)</title>
    <link>https://goodyoung.github.io/posts/paper/ldm/</link>
    <pubDate>Mon, 17 Feb 2025 17:57:23 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/paper/ldm/</guid>
    <description><![CDATA[개요 DDPM 이후, 고해상도 이미지 생성을 위해 효율적인 latent space에서 확산 과정을 수행하는 LDM에 대해 논문 리뷰를 할 것이다. Introduction 이미지 합성(Image synthesis)은 최근 가장 빠르게 발전이 되어왔지만, 많은 컴퓨터 계산 비용이 크다. 특히 고해상도 복원 문제는 AR(Autoregressive) 기반 모델들이 자주 사용하지만 모델 수십억 개의 파라미터를 요구한다. GAN은 학습 방식의 한계 때문에 multi-modal 분포에서는 모델링 하는데 한계가 있다. 이러한 가운데, Diffusion model이 이미지 합성의 여러 분야에서 뛰어난 성과(SOTA)를 보여줬다. Diffusion 모델은 다른 모델들과 다르게 모델 붕괴(model collapse), 학습 불안정성, 많은 파라미터에서 강점을 지닌다. 이러한 Diffusion 모델도 문제점이 있는데, Diffusion 모델은 mode-covering 성질을 갖고 있다. 그래서 데이터의 모든 세부 패턴을 학습하려는 경향이 있어서 많은 연산 자원을 필요로 하게 된다. Reweighted Variational Objective 방법이 연산을 줄이려고 하지만 여전히 계산량이 많다. A100 기준으로 50,000개의 샘플을 생성하는데 5일이 걸린다. 이러한 문제는 두 가지 영향을 보여준다. 첫 번째로는 훈련 시 거대한 컴퓨팅 자원을 필요로 하므로 일반 연구자나 소규모 연구팀에게 접근성이 낮다. 두 번째로는 추론 시 높은 비용과 시간을 소모하여 학습 뿐 아니라 샘플링 시에도 매우 비효율적이다.' 따라서 이 두 가지의 문제를 해결하기 위한 것이 핵심이다. 본 논문은 pixel space에서 이미 학습된 diffusion 모델을 분석하는 것 부터 시작한다. 기존의 DM은 픽셀 단위에서 학습을 진행 하였다. 이미지 자체에 대해서 훈련하는 방식이였다. 위 그림과 같이 모든 likelihood-based 모델들의 학습 과정은 두 단계로 나뉠 수 있다. 첫 번째로 높은 주파수 영역의 세부사항을 제거하며 압축을 수행하는 지각적 압축 단계이고, 두 번째로는 실제 생성 모델이 데이터의 의미적이고 개념적인 구성을 학습하는 단계이다. 본 논문은 위 두 단계와 동일하지만, 계산적으로는 더욱 효율적인 공간을 사용하는 모델을 제안한다. 본 논문에서 모델은 두 단계를 제안한다. 첫 번째로 autoencoder를 학습하여 pixel space와 지각적으로 동일하지만, 더욱 효율적인 저차원의 잠재 공간을 만든다. 두 번째로는 추가적인 공간 압축에 의존할 필요 없이 잠재 공간에서 diffusion 모델을 학습시켜 공간적 차원성(spatial dimensionality)에 대해 더 나은 확장성을 갖고 있다. 이미지 자체에 노이즈를 추가하여 학습을 했던 방식과 달리, 잠재 공간에서 학습을 하자는 것이다. 이러한 복잡성 감소 때문에 단 한 번의 네트워크 실행으로도 효율적인 이미지 생성을 할 수 있고, 본 논문은 이 모델을 Latent Diffsion Models(LDMs) 라고 부른다. 이 방법의 장점은 방대한 autoencoder를 한 번만 학습을 하게 된다면, 이를 통해 나온 latent space를 여러 DM 모델의 훈련에 사용할 수 있게 된다. 이는 곧 여러 task에서도 재사용할 수 있게 된다는 것이다. 마지막으로 본 논문의 주요 contribution에 대해 정리한다. Transformer만으로 이루어진 접근법들과 달리 더 높은 차원의 데이터에도 효율적으로 적용할 수 있다.
여러 task(inpainting 등)에서 계산 비용을 크게 감소 시키면서 경쟁력 있는 성능을 달성하였다.
기존 연구는 재구성(reconstruction)과 생성(generative)능력 사이의 차이를 조절하는 것이 중요했지만, 본 논문의 모델은 그것이 필요 없다.
두 가지를 분리해서 해결했기 때문이다. Autoencoder는 오직 **재구성(압축과 복원)**만 담당한다. Diffusion 모델은 오직 **이미지 생성(새로운 이미지 합성)**만 담당한다. 초해상도와 같은 고밀도 작업에서도 적용이 가능하다.
cross attention을 기반으로 하는 매커니즘을 개발하여 multi-modal data에도 사용할 수 있다.
Method Introduction에서도 설명이 되어있듯이, 기존 DM은 pixel space에서 매우 비용이 큰 연산을 수행해야한다는 단점이 있다. 이를 해결하기 위하여 압축과 생성 단계를 분리 하였고, 압축 단계는 계산 비용이 작은 autoencoder를 사용한다. 이를 통해 계산 효율성이 증가하고, 반복적으로 사용할 수 있는 latent space를 제공하여 범용적인 압축 모델이 된다. 또한 본 논문은 UNet의 Inductive bias를 활용하여 공간적 구조를 잘 표현하기 때문에, 과도하게 압축하지 않고도 효과적으로 이미지를 잘 생성해낼 수 있게 된다.]]></description>
</item>
</channel>
</rss>
