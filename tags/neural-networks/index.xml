<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Neural Networks - Tag - Good Young</title>
        <link>https://goodyoung.github.io/tags/neural-networks/</link>
        <description>Neural Networks - Tag - Good Young</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 30 Jul 2024 22:58:52 &#43;0900</lastBuildDate><atom:link href="https://goodyoung.github.io/tags/neural-networks/" rel="self" type="application/rss+xml" /><item>
    <title>[CS231n] 09.CNN Architectures</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture9/</link>
    <pubDate>Tue, 30 Jul 2024 22:58:52 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture9/</guid>
    <description><![CDATA[개요 CS231n의 9강에 대한 내용을 정리 할 것이다. 9강에 나오는 CNN Architecture중에 GoogLeNet과 ResNet에 대해서 살펴볼 예정이다. GoogLeNet 이때부터 이제 Network를 깊게 효율적을 만들기 시작했다. Network가 깊어질수록 학습하는데 걸리는 컴퓨팅 시간이 많이 걸린다. 따라서 Network를 깊고 효율적으로 만들기 위해서 GoogLeNet에서는 Inception module을 추가하였다. Inception Module이란 로컬 네트워크의 한 유형이다. 이는 네트워크의 네트워크 (Network In Network(NiN))의 개념에 바탕을 두고 있다.
Inception Module은 위 그림과 같이 병렬로 서로 다른 크기의 filter를 병렬로 돌려 차원을 줄여주는 것이다. 서로 다른 크기의 filter가 있으므로 여러 특징을 추출하는 효과를 가진다. 위 그림은 naive Inception module이다. Filter들의 stride와 padding을 통하여 입력과 출력 차원을 일치시키고 depth를 높였다. 이 module의 문제는 computational complexity이다. Pooling layer가 depth를 유지하기 때문에 every layer에서 전체적인 depth가 깊어진다. 이를 해결하려고 boottleneck layer가 나온다. 33, 55 filter에 들어가기 전 1*1 filter를 사용하여 depth를 줄인다. 따라서 기존과 확연히 다른 연산을 수행하는 것을 확인할 수 있게 된다. 1*1 를 사용하면 정보 손실이 발생할 수 있지만 이러한 예측을 수행하는 경우 이들의 조합을 계산하여 추가적으로 비선형성을 도입하므로 이를 보완할 수 있다. 또한 GoogLeNet에서는 auxiliary classification output이 있다. To inject additional gradient at lower layers Gradient가 잘 전달이 되지 않는 깊은 Network에서 중간 layer도 도움이 된다. 깊은 네트워크 때문에 gradient vanishing 현상을 극복하려고 inject를 한다. 이는 메인 네트워크의 최종 손실과 함께 결합되어 최종 학습 과정에 기여하게 된다. ResNet 다음은 2015년에 ILSVRC에서 우승을 한 ResNet이다. 이는 revolution of Depth인 만큼 많이 깊은 network이다. 엄청 깊게 CNN을 쌓으면 더 나은 결과가 나오나 싶지만 결과는 아니였다. Train시 overfitting이 예상이 되어 오류가 아주 적을 것이라 예상을 했지만 그것의 문제가 아니다. 이는 모델 자체는 학습이 이뤄지고 있어서 vanishing gradient문제가 아닌 degradation problem이다. 이는 optimization problem이다. 적어도 깊은 layer는 shallower의 성능은 기본적으로 있어야 하지만 그 조차도 아니다. 깊은 layer는 shallower model보다 최적화 하기 어렵다. 이걸 해결하려고 residual connection이 나왔다. 이는 아주 간단하게 입력을 출력에 더하는 아이디어 이런 단순한 아이디어 이다. 모델이 학습을 할 때 $H(x)$를 하는 것이 아니라 x를 뺀 나머지(잔차 $F(x)$)를 학습을 하게 된다. 이를 더 자세히 이해하자면 학습이 많이 될수록 x는 점점 출력값 $H(x)$에 근접하게 된다. 이 residual connection에서는 x가 identity mapping이 되기 때문에 $H(x) = F(x) + x$에서 추가 학습량 F(x)는 점점 작아져서 최종적으로 0에 근접하는 최소값으로 수렴되어야 할 것이다. 이렇기 때문에 함수 전체를 학습하는 대신, 더 단순한 잔차만 학습하게 되므로 residual connection은 학습 과정이 더 빠르고 쉽고 깊은 네트워크에서 안정적이다. 이를 수식적인 이해를 한다면 아래와 같다. Residual connection이 있는 신경망에서 아래 같이 표현을 할 수 있다. $$y = F(x) + x$$
기본 신경망에서는 손실 함수 $L$은 체인 룰을 통해 gradient update를 하게 된다. $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$
Residual connection이 있을 때 기울기 전파는 아래와 같다.
$$y = F(x) + x$$
$$\frac{\partial y} {\partial x} = \frac{\partial (F(x) + x)}{\partial x}$$
$\frac{\partial x}{\partial x} = 1$ 이므로 다음과 같이 표현할 수 있다. $$\frac{\partial y}{\partial x} = \frac{\partial F(x)}{\partial x} + 1$$
결국, gradient전파는 다음과 같이 계산이 된다. $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot (\frac{\partial F(x)}{\partial x} + 1)$$
Residual connection이 없는 경우, $(\frac{\partial F(x)}{\partial x})$가 매우 작아지면 기울기 소실 문제가 발생할 수 있다. 하지만, Residual connection이 있는 경우, $(\frac{\partial F(x)}{\partial x} + 1)$의 형태로 계산되므로, $(\frac{\partial F(x)}{\partial x})$가 0에 가까워지더라도, 항상 1이 더해지기 때문에 기울기가 완전히 사라지지 않게 되어 vanishing gradient 현상도 해결이 될 수 있다.]]></description>
</item>
<item>
    <title>[CS231n] 07.Training Neural Networks II</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture7/</link>
    <pubDate>Mon, 22 Jul 2024 10:29:23 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture7/</guid>
    <description><![CDATA[개요 CS231n의 7강에 대한 내용을 정리 할 것이다. 저번 강에서는 Training Neural Networks에 대해 배웠고 이번 강에서는 그것에 추가적인 내용을 배울 것이다. Fancier Optimization Stochastic Gradient Descent(SGD) (SGD 그림)
이전까지 Stochastic Gradient Descent(SGD)에 대해서 배웠다. 이는 매우 간단한 Optimization의 기법인데, SGD에는 여러 문제가 있다. 이 문제들에 대해서 살펴볼 예정이다.
먼저 Taco shell problem이다. 위 그림이고 W1,W2가 있을 때, 손실 함수가 매우 느리게 작동한다. SGD와 같은 목적함수는 방향이 최소값을 향한 방향으로 정렬되어 있지 않고 지그재그의 형식으로 움직이게 된다.
Loss가 수직 방향으로만 sensitive해서 덜 민감한 수평 방향으로 진행 속도가 매우 느려지고 수직 방향으로는 빠르게 진행되어 지그재그의 형태로 학습을 하게 된다. 이것은 고차원에서 더 흔해진다. (graph 그림)
다른 문제는 Local minima과 Saddle point (안장점)이 있다.
x축이 하나의 파라미터를 뜻하고 y축이 loss를 뜻할 때 위 그림에서 SGD는 local minima일 때 멈춘다.
왜냐하면 SGD는 기울기를 계산하고 반대 기울기 방향으로 진행하기 때문에 빨간색 위치에서 gradient가 0이기 때문에 멈추게 된다.
이는 아래 그래프 처럼 Saddle point에서도 똑같이 기울기가 0이기 때문에 멈추게 된다.
Saddle point: 어떤 방향에서는 손실이 증가, 어떤 방향에서는 손실이 감소하는 그 중간 지점
High dimension에서는 saddle point가 local minima보다 더 흔하다. Saddle point의 근처 지점에서도 문제가 된다. Saddle point의 근처에 있을 때 마다 우리는 매우 느린 진행을 하게 될 것이다. 다른 문제는 SGD의 S이다. 확률적으로 모든 dataset의 loss를 계산하기엔 계산 비용이 많이 들어서 우리는 mini Batch의 방법을 사용하여 값을 추정한다.
하지만 이는 W의 값을 쪼개서 사용하므로 noisy data일 수 있다는 것이다.
기본적인 전체 배치를 사용하더라고 이런 모든 문제들은 해결되지 않는다.
SGD + Momentum 앞서 말한 문제들을 해결하기 위한 기본적인 idea가 있다. 그것은 바로 Momentum의 개념이다.
Momentum이란 시간이 지남에 따라 속도를 유지하고 기울기 추정치를 속도에 추가한다는 것이다. 기울기 방향이 아닌 속도 방향으로 gradient update가 일어난다.
마찰 상수인 rho도 존재. 마찰에 의해 속도를 감소시킨다음 gradient에 추가한다.
vx = 0 while True: dx = compute_gradient(x) # gradient 계산 vx = rho * vx + dx # 마찰 상수로 속도 감소 그것을 gradient에 추가 x += learning_rate * vx # 내려가기 (그래프)
이는 위 문제들을 다 해결하게 된다. 앞선 그래프에서 Local minima이든 Saddle point가 속도가 빨라지는 것과 같은 물리적인 해석을 할 수 있게 된다.
이제 속도가 있으면 기울기가 없더라도 해당 점은 여전히 속도를 가진다. 이러면 극복할 수 있게 된다.
따라서 위 그림처럼 지그재그들이 서로 상쇄되고 민감한 방향 (수직 방향)으로 걷는 양을 효과적으로 줄이고 덜 민감한 차원을 가로지르는 하강을 가속할 것이다.
그래서 시간이 지남에 따라 속도가 증가가 되고 노이즈가 기울기 추정에서 평균화가 된다.
그래서 SGD와는 다르게 부드러운 경로를 취하게 된다. (Momentum 그래프) 18. 위 그림은 Momentum의 종류를 나타낸 것이다.
기존 momentum은 현재 지점(빨간색)에서 기울기를 추정한 후 속도 벡터와 섞어서 사용하는 반면 Nesterov momentum은 기존 속도방향으로 나아간 후 그 시점에서 기울기를 추정한다. 그 후 원래 지점으로 돌아가서 이 두개를 섞는 방법이다.
Nesterov momentum은 기존보다 정보를 더 혼합하는 것으로 생각할 수 있게 된다. 이는 Convex optimization에서는 잘 작동하지만, Neural Network와 같은 non-convex의 문제에서는 보장된 방식은 아니다.
(Nesterov momentum 식 사진 )
우리는 항상 동일한 지점에서 loss를 평가하고 싶다. 하지만 Nesterov momentum은 그것이 아니기 때문에 따라서 이 식을 조금 더 변형하면 아래의 식과 같이 항상 동일한 지점에서 손실과 기울기를 평가할 수 있게 된다.
하단의 박스를 보면 현재 시점에서 $v_{t+1}$를 더하고 현재의 $v_{t+1}$와 이전 속도 $v_{t}$의 차이를 더해주면 항상 동일한 지점 $\tilde{x_t}$ 에서 평가하게 된다.]]></description>
</item>
<item>
    <title>[CS231n] 06.Training Neural Networks I</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture6/</link>
    <pubDate>Thu, 18 Jul 2024 15:36:30 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture6/</guid>
    <description><![CDATA[개요 CS231n의 6강에 대한 내용을 정리 할 것이다. 저번 강에서는 Convolutional Neural Networks에 대해 배웠고 이번 강에서는 이 Neural Networks를 어떻게 하면 잘 학습시킬 수 있을지에 대해서 배울 것이다. Activation Function Deep Network에 비선형성을 부여하는 Activation Function의 종류들에 대해서 알아볼 것이다. Sigmoid $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
먼저 첫 번째로 Sigmoid이다. 함수의 모양은 위 그림과 같고 (0,1) 사이의 값을 가지게 된다.Sigmoid가 가지게 되는 몇가지 문제들이 있다. 첫 번째 문제는 경사도를 죽일 수 있는 문제이다.
이는 입력으로 들어오는 x가 -10, 10이라면 global gradient(이전 노드의 기울기)는 0이 되고 이것이 역전파를 통해 다운스트림($global grdient * local gradient$) 되면 global gradient가 0이므로 아래 노드들은 0으로 수렴하게 되어 기울기가 소실이 된다. 이러한 기울기 소실로 인하여 W의 값을 더이상 update 하지 못하는 문제를 vanishing gradient라고 한다. 두 번째 문제는 output이 zero-center가 아닌 문제이다. 즉, Signmoid의 output이 항상 양수이기 때문에 0을 중심으로 있지 않는 문제가 발생하게 된다. ouput이 항상 양수 이라면 입력값 또한 양수인 것인데, 그렇게 된다면 back propagation에서 $\frac{dl}{df} * \frac{df}{wi}$ 로 W의 기울기를 chain-rule로 구하게 된다. 근데 우리가 이전 4강에서 배운 chain-rule의 pattern 중 mul gate(곱)일 때 grdient가 서로 전환이 된다고 배웠다. 따라서 $\frac{df}{wi}$ 이것이 바로 $x_i$인 것을 알 수 있게 된다. 이때 $x_i$가 이전 layer의 sigmoid 연산을 거친 output이기 때문에 무조건 양수이다. 입력값은 항상 양수라면 back propagation을 할 때 $\frac{dl}{df}$(global gradient이다.)의 부호를 따라가게 되어서 gradient의 부호는 항상 양수이거나 음수가 된다. 부가적으로 gradient의 부호는 다운 스트림으로 내려온 gradient의 부호와 같아진다고 생각하면 쉽다. 이렇게 부호가 모두 양수 또는 음수가 되어버리면 발생하는 문제는 gradient update를 할 때 나타난다. w1,w2가 있다면 gradient update시 항상 (+)(+) 이거나 (-)(-)가 된다. 따라서 위 그림과 같이 파란색 방향(최적의 gradient update)과는 다르게 비효율적으로 지그재그 방향으로 내려가게 되는 상황이 벌어지게 된다. 이를 해결할 수 있는 방법은 input값에 mean을 모두 빼버리면 zero-mean data가 된다. 그렇게 되면 X가 양수/음수를 모두 가지고 있는 상황을 만들 수 있게 된다. 마지막 세 번째 문제는 Sigmoid에서의 exp의 연산의 계산 비용이 많이 들어간다는 것이다. 따라서 위 세 가지의 문제로 잘 사용하지 않게 된다. tanh 다음은 tanh함수이다. 이는 범위를 (-1,1)까지 하여 Sigmoid의 단점인 non-zero-center문제를 해결하게 되었다. 하지만 여전히 그래프를 봐도 확인 할 수 있지만 경사도를 죽일 수 있는 문제는 해결되지 못하였다. 따라서 Network가 깊어지는 모델이 많아진 요즘 활성화 함수로 잘 사용하지 않게 된다. ReLU $$f(x) = max(0,x)$$
다음은 ReLU함수 이다. 이는 위 두 함수과 다르게 양수에서 경사도를 죽일 수 있는 문제를 해결 하였다. 또한 이 함수는 최대값만 계산하기 때문에 연산이 효율적이고 실제로 앞선 함수들과 비교하여 6배가 빠르다고 한다. 하지만 ReLU함수도 몇가지 문제점이 있다. Non-zero-center문제를 가지고 있고 또한 음수일 경우에는 경사도를 죽일 수 있는 문제가 발생할 수 있다. 따라서 W를 initialization하는 과정에서 잘못되었을 때 dead ReLU가 발생할 수 있다. 따라서 이런 dead ReLU를 피하기 위하여 초기화를 할 때 positive한 bias를 얻게 초기화를 하는 방법도 있지만 효과가 경미하다. 또한 learning rate를 크게 설정하는 대규모 훈련 시 W가 너무 크게 비약해서 학습이 안되는 상황이 발생할 수도 있다. 실제로 ReLU를 사용한 Network에서 10~20% 정도가 dead ReLU에 빠진다고 한다. 문제이긴 하지만 그럼에도 불구하고, train은 잘 된다고 한다. Leaky ReLU $$f(x) = max(0.01x,x)$$
ReLU와 다르게 음수에도 음수 기울기를 주어 경사도를 죽일 수 있는 문제를 해결하고자 한다. ReLU와 같게 연산이 빠르다는 장점도 있다. PReLU $$f(x) = max(ax,x)$$
PReLU는 음수 영역에 backpropagation에서의 파라미터로 기울기 값으로 유연성을 더해주어 조금 더 융통성을 가해 주었고 ELU 다음의 ELU는 평균 출력이 더욱 0에 가까워지게 만든 함수이다.]]></description>
</item>
<item>
    <title>[CS231n] 05.Convolutional Neural Networks</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture5/</link>
    <pubDate>Fri, 12 Jul 2024 20:01:34 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture5/</guid>
    <description><![CDATA[개요 CS231n의 5강에 대한 내용을 정리 할 것이다. 저번 강에서는 W를 업데이트 하는 과정 Chain-Rule과 간단한 Neural Networks에 대해서 배웠는데 이번 강에서는 Convolutional Neural Networks에 대해서 배울 것이다. A bit of history 위 그림을 보면 Hubel &amp; Wiesel이 고양이 실험을 했는데 이때 시각 피질 안의 뉴런이 local receptive field를 가지게 된다는 것을 알게 되었다. 또한 이렇게 겹쳐지는 receptive field들이 전체 시야를 이루게 된다는 사실을 알게 되었다. 어떤 뉴런은 low level, 또 어떤 뉴런은 high level의 특징들을 포착하는 것의 조합으로 이루게 된다. 이러한 지식을 기반으로 image처리를 위해 98년에 Lenet, 2012년에 AlexNet이 등장하게 되었다. Convolutional Neural Networks 저번 시간에 node들이 linear하게 모두 연결되어 있는 층을 Fully Connected (FC)층이라고 한다. 또 이 FC층이 행렬의 내적 계산을 통한 아주 효율적인 계산을 할 수 있다고 배웠다. 따라서 위 그림을 보면 하나의 layer는 하나의 연산을 통해 계산을 하여 편의성을 더해주는 사실을 볼 수 있다. 하지만 이미지 처리를 할 땐 위의 FC층이 좋지 않다. 왜냐하면 이미지의 spatial(공간적)정보가 손실이 되기 때문이다. 이러한 문제점을 해결하기 위하여 Convolutional Neural Networks (CNN)이 등장하게 되었다. CNN에 대해 더 자세히 알아보면 위 그림과 같은 이미지가 있고 필터가 존재한다. 이 필터는 우리가 linear classification에서 배웠던 W의 역할을 한다. 이런 필터는 input image위를 아래와 같이 슬라이딩 하면서 요소별 곱을 하고 그것을 또 하나의 합으로 나타낸다.
위 그림에선 3 * 3의 크기의 filter가 5 * 5의 크기의 image 위를 슬라이딩 하고 있다. 이 filter에 중요한 사실이 있다. 바로 filter의 depth 크기는 input volume의 depth랑 항상 같다는 점이다. 왜냐하면 우리가 시각적으로 보기에 image위를 슬라이딩 하는 것 처럼 보이지만 실제로의 연산은 $w^Tx$에서 image에서 filter가 겹쳐지는 부분만큼 가져온 후 1차원으로 늘린 연산이다. 따라서 위 그림과 같이 5*5*3의 크기인 filter연산은 곧 75-1D의 내적 연산(1차원)을 수행하는 것으로 이해하면 될 것 같다. 이렇게 연산을 수행하기 때문에 input volume의 depth랑 filter depth랑 같아야 filter의 내적 연산을 수행할 수 있게 된다. 이렇게 filter가 슬라이딩을 하여 연산을 한 번 모두 하면 위 그림과 같이 하나의 activation map이 나온다. 만약 필터가 6개가 있다면 앞서 말한 내적 연산을 6번 수행하여 ouput의 depth가 6인 activation map이 나오게 된다. 각 필터는 input volume에서 특정 유형의 템플릿이나 개념을 찾는다. 6개의 각각 다른 가중치를 지닌 activation map으로 생각하면 된다. 이런 layer들을 연속적으로 쌓아나가면 그것이 convolution network가 된다. 이런 convolution network에서는 처음에 low-level(edge 등등)의 특징을 추출하고 점점 깊어지면 high-level의 복잡하고 추상적인 개념들이 나타나게 된다. 이 정보들을 FC층에 넣고 각 class 수의 확률 연산을 하게되면 분류가 이루어진다. 왜 FC층이 필요하며, 어떻게 분류가 이루어지는지 의문을 가졌다. high-level features는 넓은 reception field를 가지고 있다. (점점 깊어질 수록 이미지의 resolution이 줄어들기 때문) 이러한 복잡하고 풍부한 정보를 포함하고 있는 feature들을 linear하게 놓고 모든 정보를 연결(Fully connected)을 하여 각각의 class별 weight를 계산을 하게 되면 해당 이미지가 class별 확률(softmax)이 나오게 될 것이다. 따라서 CNN으로 feature를 추출하고 FC 층에서 분류하는 이유는 이러한 과정을 통해 각 클래스별 확률을 효과적으로 계산할 수 있기 때문입니다. 이러한 방법들은 앞서 history에서 말한 인간의 인식 능력에서의 시각피질과 유사한 특징을 보이는 것을 알 수 있다. 픽셀은 항상 일관된 순서를 가지며, 서로 인접한 픽셀끼리 영향을 준다. 만약 모든 근처의 픽셀이 빨간색이라면 해당 픽셀도 빨간색일 가능성이 높다. 이렇게 픽셀은 주변 픽셀 값과 비교하여 정보를 추측할 수 있습니다. 이런 특성을 locality라고 합니다. 따라서 위 그림과 같이 sub sampling과정을 통해 image의 resolution을 줄이고 local feature들에 대한 연산을 통해 global feature(high-level)로 나아가 weight 변수를 줄이고 변화에 무관한 invariance를 얻게 되는 것이다.]]></description>
</item>
<item>
    <title>[CS231n] 04.Introduction to Neural Networks</title>
    <link>https://goodyoung.github.io/posts/cs231n/lecture4/</link>
    <pubDate>Mon, 08 Jul 2024 21:02:03 &#43;0900</pubDate>
    <author>GoodYoung</author>
    <guid>https://goodyoung.github.io/posts/cs231n/lecture4/</guid>
    <description><![CDATA[개요 CS231n의 4강에 대한 내용을 정리 할 것이다. 저번 강에서는 Loss Function과 Optimization에 대해서 배웠는데 이번 강에서는 W를 업데이트 하는 과정인 Chain-Rule과 간단한 Neural Networks에 대해서 배울 것이다. Backpropagation 지난 과정에 gradient에는 두가지 종류가 있다고 배웠다. 그 중 빠르고 정확한 analytic gradient에 대해서 활용해볼 것이다. 각 과정의 연산 과정을 Computational graph을 활용하여 표현한다면 analytic gradient를 활용할 수 있게 된다.
이를 통해 함수는 BackPropagation이라는 기술을 사용하고, gradient를 얻기 위하여 Chain-rule를 활용한다.
BackPropagation의 과정은 다음과 같다. 각 입력이 local node로 들어오고 다음 노드로 직접 전달된다. local gradient는 이때의 입력된 노드의 출력의 gradient이다. 각각의 입력마다 그때의 local gradient를 구한다. 즉, z에 대한 x로의 미분, z에 대한 y로의 미분을 구한다. 이를 Forward Pass (Foward Propagation)이라고 한다. Forward Pass의 맨 마지막에는 loss function을 통한 loss가 나온다. Forward Pass가 모든 노드가 진행이 되었으면 Backward Pass (Back Propagation)이 진행된다. 이때 Back Propagation은 수많은 계산을 거쳐 나온 loss에 대한 z의 미분을 나타내고 이는 global gradient (위 그림에선 빨간색 글씨로 gradients라고 표기)라고 칭한다. 이때 그럼 loss에 대한 x, y의 미분값을 구할 수 있게 되는데 이때 활용되는 개념이 Chain-rule이다. Forward Pass로 구한 local gradient의 값과 그 노드의 global gradient를 곱하면 우리고 최종적으로 원하는 gradient가 나오게 된다. $gradient = local \ gradient * global \ gradient$ 이런 Computational graph에서 그룹화를 할 수 있다는 사실도 알 수 있다. 위 그림을 보면 sigmoid gate로 하나의 노드로 묶어서 계산 할 수도 있다. 따라서 얼마나 그룹화를 하여 노드를 표현할 것인지에 대한 고민이 필요할 수 있다. 또한 Back Propagation에는 3가지 패턴이 존재한다고 한다. add gate gradient 전달하는 역할 max gate 한 방향으로 gradient 모두 전달하는 역할 mul gate 서로 gradient 전환하는 역할 종합적으로 위에서 배운 Back Propagation을 아래와 같이 일반화된 식으로 표현한다. $$ \frac{\partial f}{\partial x} = \sum \frac{\partial f}{\partial q_i} \cdot \frac{\partial q_i}{\partial x} $$
이제 위에서 배운 Back Propagation에서 변수가 벡터라고 생각한다면 gradient는 Jacobian matrix로 표현할 수 있게 된다. Jacobian matrix: 각 요소의 미분을 포함하는 행렬 따라서 4096의 input이 들어온다면 이 Jacobian matrix의 크기는 $4096 * 4096$ 일 것이다. 이때 gradient의 각 요소는 함수의 최종 출력에 얼마나 영향을 미치는가를 정량화 한 값으로 표현이 되고 이는 결국 편미분한 값과 이어지게 된다. 따라서 입력의 어떤 차원이 출력의 어떤 차원에 영향을 주는지, 그래서 Jacobian matrix는 입력의 각 요소가 오직 출력의 해당 요소에만 영향을 주기 때문에 대각 행렬이 될 것이다. Neural Networks 위 그림 처럼 2계층 신경망을 얻기 위해 다른 것 위에 비선형 변환을 하면 된다. 이렇게 계속 층층 쌓아가면 Deep Neural Network의 형태가 된다. 위의 W1, W2는 각각 gradient로 학습 시키고, 그 gradient들은 Chain-rule으로 계산하여 구한다. 이런 비선형성의 특징을 표현하기 위해 activation function이라는 함수가 존재한다. 이는 강의 후반부에 더 자세히 다룬다고 한다. 아래는 이 Forward pass과정을 코드로 표현한 것이다. f = lambda x: 1.0/(1.0+ np.exp(-x)) # sigmoid (activation function) x = np.random.randn(3, 1) # random input vector h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer out = np.dot(W3,h2) + b3 # output neuron (1*1) Reference https://chasuyeon.tistory.com/entry/cs231n-4%EA%B0%95-%EC%A0%95%EB%A6%AC-Introduction-to-Neural-Networks ]]></description>
</item>
</channel>
</rss>
