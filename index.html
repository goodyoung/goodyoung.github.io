<!DOCTYPE html>
<html lang="en-us">
    <head>
	<meta name="generator" content="Hugo 0.121.2">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="Good Young" />
<meta property="og:description" content="This is my cool site" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://goodyoung.github.io/" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Good Young"/>
<meta name="twitter:description" content="This is my cool site"/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/" /><link rel="alternate" href="/index.xml" type="application/rss+xml" title="Good Young">
    <link rel="feed" href="/index.xml" type="application/rss+xml" title="Good Young"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "url": "https:\/\/goodyoung.github.io\/","inLanguage": "en-us","author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": "This is my cool site","name": "Good Young"
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="page home" data-home="posts"><div class="home-profile"><div class="home-avatar"><a href="/posts/" title="Posts"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/profile.jpeg"
        data-srcset="/images/profile.jpeg, /images/profile.jpeg 1.5x, /images/profile.jpeg 2x"
        data-sizes="auto"
        alt="/images/profile.jpeg"
        title="/images/profile.jpeg" /></a></div><div class="home-subtitle"><div id="id-1" class="typeit"></div></div><div class="links"><a href="https://github.com/goodyoung" title="GitHub" target="_blank" rel="noopener noreffer me"><i class="fab fa-github fa-fw" aria-hidden="true"></i></a><a href="mailto:goodyoung.goh@gmail.com" title="Email" rel="me"><i class="far fa-envelope fa-fw" aria-hidden="true"></i></a><a href="/index.xml" title="RSS" target="_blank" rel="noopener noreffer me"><i class="fas fa-rss fa-fw" aria-hidden="true"></i></a></div></div>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/cs236/lecture4/">[CS236] 4. Maximum Likelihood Learning</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2025-06-30">2025-06-30</time></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a>&nbsp;<a href="/categories/deep-generative-models/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Generative Models</a></span></div><div class="content">개요 이번 포스트에서는 CS236 강의의 4강 내용을 정리한다. 3강에선 데이터셋의 분포를 학습하여 Model Family를 파라미터화 하는 방법을 배웠다. 이번 4강에서는 데이터셋에 대하여 모델 파라미터 $\theta$ 를 찾는 방법을 배우게 된다. 3강에서 다뤘던 내용을 다시 한 번 살펴보자. 데이터가 실제 분포 $P_\text{data}$ 로부터 추출된 m개의 샘플 $\mathcal{D}$ 가 있다고 가정을 해보자. 그럼 생성 모델의 목표는 모델 $\mathcal{M}$ 에서 $P_\text{data}$ 와 가능한 가장 가까운 $P_\theta$ 를 학습하는 것이다. 이때의 모델 $\mathcal{M}$ 은 Bayes net이 될 수 있고, FVSBN이 될 수 있는 것 이다. 하지만 $P_\theta$ 가 완전히 실제 분포를 포착할 순 없다. 왜냐하면 제한된 data의 문제와 컴퓨팅 파워 문제가 있기 때문이다. 784개의 이진 픽셀로 이루어진 이미지를 생각해보면, 가능한 모든 이미지는 $\text{2}^\text{784} \approx \text{10}^\text{236}$ 가지이다. 대략 백만 개의 샘플로는 이 공간을 거의 다룰 수 없다. 그래서 우리는 $P_\text{data}$ 의 분포를 잘 근사하는 $P_\theta$ 를 선택해야한다. 그렇다면 어떤 것이 잘 근사하는 모델($P_\theta$)일까? 잘 근사하는 모델은 우리가 하려는 task에 따라 다르다. (Density Estimation에서는 전체 확률 분포를 잘 근사하는 것이 중요, Specific Prediction Task은 특정 예측을 만드는 분포(조건부 확률)가 중요, Structure or Knowledge Discovery 모델 그 자체의 구조가 중요) 생성의 관점에선 어떤 확률적 추론 쿼리에 답해야 하기 때문에 우린 전체 분포를 배워야한다. 따라서 학습을 Density Estimation 문제로 볼 수 있다. 여기서 말하는 확률적 추론 쿼리(any probabilistic inference query)란 확률 분포로부터 도출되는 질문들(조건부 확률, 마진, 샘플링 등)을 의미한다. 그래서 우리의 목표는 $P_\text{data}$ 에 가장 가까운 $P_\theta$ 를 만드는 것이 제일 중요하다. 그렇다면 가까운 정도를 어떻게 평가할까? 다음 챕터에서 설명 할 것이다. Kullback-Leibler divergence(KL-divergence) 어떤 두 분포간의 가까운 정도를 측정하기 위하여 Kullback-Leibler divergence(KL-divergence)지표를 사용하게 된다. 수식은 아래와 같다. $$D_{\text{KL}}(p || q) = \sum_x p(x) \log \frac{p(x)}{q(x)}$$
KL-Divergence는 몇가지 특징이 있다. $D_{\text{KL}}(p || q) \geq 0$ 이고, 같을 땐 $p = q$ 일 때 이다. $D_{\text{KL}}(p || q) \not = D_{\text{KL}}(q || p)$ 으로 비대칭적인 성질을 지닌다. 이 지표는 정보이론 관점에서 p와 q에 기반한 압축 방식이 얼마나 잘되는지 보여준다. (p가 진짜 분포이고 q가 학습한 분포) q의 분포로 p를 인코딩을 했을 때 생기는 비트 낭비량을 확인할 수 있다. 우리의 목표는 $P_\theta$ 가 $P_\text{data}$ 와 가깝도록 만드는 것이기 때문에 $D_{\text{KL}}(P_\text{data} || P_\theta)$ 로 표현할 수 있다. 따라서 KL-Divergence의 값을 확인하여 data를 잘 압축할 수 있는 모델을 선정해야한다. KL이 작을수록, 압축 손실도 작아짐 → 더 나은 모델 $$ D_\text{KL}(P_\text{data} || P_\theta) = \mathbb{E_{x \sim P_\text{data}}}[\log P_\text{data}(x)] - \mathbb{E_{x \sim P_\text{data}}}[\log P_\theta(x)] $$
위 KL을 다음과 같이 분해가 가능하고, 앞 항은 $P_\theta$ 에 영향받지 않는 상수이기 때문에 두번째 항에 집중을 할 것이다. 그렇게 되면 수식은 아래와 같아진다. $$ argmin_\theta \ D_(P_\text{data} || P_\theta) = argmin_\theta -\mathbb{E_{x \sim P_{\text{data}}}} [\log P_\theta(x)] = argmax_\theta \ \mathbb{E_{x \sim P_{\text{data}}}} [\log P_\theta(x)] $$
그럼 이제 KL을 최소화 하기 위하여 두번째 항을 최대화 하는것이 목표이다. 이 수식을 통하여 알 수 있는 KL의 특징이 있다. 바로 두 모델 간 KL을 비교했을 때 누가 가까운지는 알지만 얼마나 가까운지 (정확한 거리)는 모른다는 것이다. $D_(P_\text{data} || P_{\theta_{1}}) - D_(P_\text{data} || P_{\theta_{2}})$ 가 계산이 되면 상수(첫번째 항)가 사라져서 거리를 모르게 된다. 이제 이 수식을 풀려고 보니, 정리된 수식의 모든 $P_\text{data}$를 우리는 일반적으로 (expected log-likelihood) 구할 수 없다. 왜냐하면 현실에선 이 기대값을 계산할 수 없으므로 주어진 데이터 샘플의 평균으로 근사 해야한다. (empirical log-likelihood) 따라서 데이터 샘플의 평균으로 근사하면 아래와 같은 식이 나오고, 그것을 최대화 하는 $\theta$를 찾는 방향으로 학습을 진행하면 된다.</div><div class="post-footer">
        <a href="/posts/cs236/lecture4/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/kl-divergence/">KL-Divergence</a>,&nbsp;<a href="/tags/cs236/">CS236</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/cs236/lecture3/">[CS236] 3. Autoregressive Models</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2025-05-07">2025-05-07</time></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a>&nbsp;<a href="/categories/deep-generative-models/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Generative Models</a></span></div><div class="content">개요 CS236: Deep Generative Models (Stanford)는 스탠포드 대학교에서 진행하는 딥러닝 기반 생성 모델(Deep Generative Models) 에 대한 심화 강의이다. 이번 포스트에서는 CS236 강의의 3강 내용을 정리한다. 3강에서는 주어진 데이터셋 D의 분포를 학습하여, 이로부터 새로운 샘플을 생성하는 방법을 배운다. 그렇게 하기 위해서 2가지 절차가 진행되어야 한다: Model Family를 파라미터화 (3강에서 다룰 내용) 데이터셋 D에 대하여 모델 파라미터 $\theta$ 를 찾는 방법 (4강에서 다룰 내용) Autoregressive Models $$ \begin{aligned} p(x_1, x_2, \ldots, x_{784}) &amp;= p(x_1),p(x_2 \mid x_1),p(x_3 \mid x_1, x_2), \cdots, p(x_{784} \mid x_1, \ldots, x_{783}) \\ p(x_1, \cdots, x_{784}) &amp;= p_{\text{CPT}}(x_1; \alpha_1) \cdot p_{\text{logit}}(x_2 \mid x_1; \alpha_2) \cdot p_{\text{logit}}(x_3 \mid x_1, x_2; \alpha_3) \cdots p_{\text{logit}}(x_n \mid x_1, \cdots, x_{n-1}; \alpha_n) \\ \end{aligned} $$
Autoregressive Model은 각 픽셀을 이전 픽셀들에 대한 조건부 확률로 예측하는 모델이다. 이러한 조건부 확률을 계산을 하기 위해서 Chain rule factorization을 사용한다. 하지만 이때 모든 확률 조건들을 저장할 수 없다. 따라서 어떤 신경모델, 함수를 사용하여 조건문을 모델링 하려고 한다. 위의 두번째 수식 처럼 logistic regression 함수를 이용한 방법이 그 예시이다. $$ \begin{aligned} p_{\text{CPT}}(X_1 = 1; \alpha_1) = \alpha_1 &amp;, p(X_1 = 0) = 1 - \alpha_1, \\ p_{\text{logit}}(X_2 = 1 \mid x_1; \alpha_2) &amp;= \sigma(\alpha_2^0 + \alpha_2^1 x_1), \\ p_{\text{logit}}(X_3 = 1 \mid x_1, x_2; \alpha_3) &amp;= \sigma(\alpha_3^0 + \alpha_3^1 x_1 + \alpha_3^2 x_2) \end{aligned} $$
MNIST 데이터셋으로 Autoregressive Model을 구성하는 수식을 살펴보게 되면 위의 수식과 같다. 첫번째 픽셀이 black or white인지 조건부 확률 테이블(CPT)에서 값을 받아 배정된다.(보통 black) 그 후 순서에 따라 각 값을 예측하게 된다. Fully Visible Sigmoid Belief Network (FVSBN) 초창기 autoregressive model인 FVSBN은 chain rule을 이용해 확률분포를 나타낸 이후, 많은 컴퓨팅 파워를 요구하는 조건부 확률을 매개변수화한 이후 logistic regression 알고리즘을 적용해 학습하는 생성모델 알고리즘이다. 만약 pixel이 4개라고 가정을 하면, 이전 픽셀에 따라서 값이 다른 것을 확인할 수 있다. 이 모델의 경우 파라미터는 $1+2+3+\cdot\cdot\cdot+n \approx n^2/2$ 인 것도 확인할 수 있다. FVSBN에서 sampling하는 방법이다. 랜덤한 난수를 먼저 생성하고, 그 난수($\bar{x_1}$)로부터 $\bar{x_2}$ 가 나오게 된다. 하지만 FVSBN의 sampling 결과가 좋진 않다. Neural Autoregressive Density Estimation (NADE) NADE는 많은 컴퓨팅 파워를 요구하는 FVSBN의 한계를 보완하기 위해서 logistic regression 대신 Neural Network를 사용한 모델이다. $$ \begin{aligned} h_i &amp;= \sigma(W_{\cdot,&lt;i} x_{&lt;i} + c) \\ \hat{x_i} = p(x_i \mid x_1, \ldots, x_{i-1}) &amp;= \sigma(\alpha^\top h_i + b_i) \end{aligned} $$
위 수식의 $h_i$는 neural network를 의미한다. NADE는 Weight Sharing(Weight tying)기법을 활용하여 계산량을 대폭 줄였다. Weight Sharing은 학습 과정에서 은닉층($h_i$)에서 사용하는 가중치 $w_i$를 동일하게 유지하는 방법이다. 층이 깊어질수록 기존 가중치 벡터 (W)에 열벡터 하나만 추가하는 것으로 생각하면 된다. 이런 결과로 계산 복잡도가 O(n)으로 줄어든 것을 확인할 수 있다. 위 사진이 NADE의 생성 결과인데, 왼쪽이 sample이고 오른쪽이 생성 결과이다. 이미지 구조(분포)를 꽤나 잘 파악하고 있는 것으로 확인할 수 있다. 생성의 작동 방식은 왼쪽 sample에서 픽셀 값을 가져오고 NADE가 계산을 통하여 확률 값이 나오게 된다. 따라서 각 픽셀에 대한 확률 값이기 때문에 오른쪽 샘플의 결과는 0~1사이의 확률값이기때문에 이미지가 부드러워 보인다. 지금까지는 이진 데이터만 다루었다. 하지만 color 이미지와 같은 범주형 변수일 때는 어떻게 작동 할까? $x_i \in {0, \ldots, K}$ 인 다항 변수의 모델링이 필요할 것이다. 다항 변수의 모델링을 하기 위해서 각 $x_i$에 softmax를 취하여 범주형 확률 분포를 만들어주면 기존의 조건부 확률을 계산할 수 있게 된다.</div><div class="post-footer">
        <a href="/posts/cs236/lecture3/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/bayesian-network/">Bayesian network</a>,&nbsp;<a href="/tags/conditional-independence/">Conditional Independence</a>,&nbsp;<a href="/tags/cs236/">CS236</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/cs236/lecture2/">[CS236] 2. Background</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2025-04-20">2025-04-20</time></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a>&nbsp;<a href="/categories/deep-generative-models/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Generative Models</a></span></div><div class="content">개요 CS236: Deep Generative Models (Stanford)는 스탠포드 대학교에서 진행하는 딥러닝 기반 생성 모델(Deep Generative Models) 에 대한 심화 강의이다. 이번 포스트에서는 CS236 강의의 2강 내용을 정리한다. 2강에서는 말그대로 Generative 분야를 공부하기 전에 필요한 배경 지식을 설명하고 있다. 조건부 독립 (conditional independence) generative model vs discriminative model 생성 모델이란 주어진 데이터(x)의 확률 분포(p(x))를 학습하여 샘플링하여 유사한 데이터를 생성해내는 것이다. ($x ~ p(x)$) 이런 확률 분포 p(x)에 대하여 여러 활용을 할 수 있는데, 아래는 그 활용에 대해서 나타낸다. 밀도 추정: 데이터 유사도 기반 이상 탐지 (원하는 object엔 p(x)가 높다.) Unsupervised representation learning: 공통된 특징 추출 그럼 이제 이러한 데이터의 확률분포 p(x)를 어떻게 표현을 할 것이냐가 먼저이다. Representation p(x) 확률 분포의 표현은 여러가지이다. 베르누이 분포와 범주형(categorical)분포는 위 사진과 같이 표현할 수 있다. 만약 3개의 discrete random variable인 이미지 데이터라면, $p(R = r, G = g, B = b)$의 joint distribution을 구하기 위하여 $256 * 256 * 256 - 1$의 파라미터가 발생한다. 이때 말하는 파라미터란 무엇인가? 파라미터란 확률 분포를 정의하기 위해 필요한 개별 확률값을 의미한다. $X_1, X_2 \in {0, 1}$ 의 변수가 있다고 가정할 때 가능한 조합은 4가지 ((0,0), &hellip; (1,1))이다. 각 조합에 대해 확률 값을 지정해야 하지만, 전체 확률 합은 1이어야 하므로 4개 중 3개만 자유롭게 정하면 나머지 하나는 자동으로 정해진다. 따라서 이 경우 $4 - 1 = 3$ 개의 파라미터를 가진다. 만약 n개의 binary(Bernoulli) random variable이라면 가능한 image의 수는 $2 \times 2 \times &hellip; \times 2 = 2^n$ (n: pixel 수)일 것이다. 그럼 이때 이 분포에서 sampling을 한다면 특정 분포의 joint distribution $p(x_1,&hellip;,x_n)$은 $2^n - 1$의 파라미터가 필요하다. 따라서 random variable의 수에 따라 파라미터의 수가 기하급수적으로 증가한다는 사실을 알 수 있다. 이 모든 값들을 컴퓨터에 저장하기엔 무리가 있다. 그렇기 때문에 수학적 가정이 필요한 순간이다. 그래서 독립성 가정(Independence Assumption)을 하겠다. $X_1, &hellip; , X_n$ 베르누이 분포를 만족하는 확률 변수들이 있다고 가정했을 때 $p(x_1, &hellip; , x_n) = p(x_1)\dot\dot\dot p(x_n)$ 을 만족한다. 이 경우, 가능한 상태(이미지)는 동일하게 $2^n$이고 joint distribution의 파라미터는 $n$이다. Marginal distribution $p(x_1)$의 파라미터가 1이다. 따라서 $1 + 1 + &hellip; + 1$이기 때문에 파라미터는 n이 된다. 독립성 가정은 너무 strong해서 위의 그림 처럼 모든 값이 독립적으로 무작위 값을 선택하여 샘플링 결과가 안좋다. $$p(S_1 \cap S_2 \cap \cdots \cap S_n) = p(S_1) \cdot p(S_2 \mid S_1) \cdot \cdots \cdot p(S_n \mid S_1 \cap \cdots \cap S_{n-1})$$ $$p(S_1 \mid S_2) = \frac{p(S_1 \cap S_2)}{p(S_2)} = \frac{p(S_2 \mid S_1) \cdot p(S_1)}{p(S_2)} $$
따라서 다른 가정이 필요하여 두가지 중요한 rule(공식)이 있다. 위는 순서대로 Chain rule (probability)와 Bayes' rule의 식이다. $$p(x_1, x_2, \ldots, x_n) = p(x_1) \cdot p(x_2 \mid x_1) \cdot p(x_3 \mid x_1, x_2) \cdots p(x_n \mid x_1, \ldots, x_{n-1})$$
Chain Rule을 사용하면 위의 식이 된다. 이때 파라미터는 $1+2+&hellip;+2^\text{n-1} = 2^n - 1$이다. 여전히 exponential하다는 것을 알 수 있다. 이 chain rule을 사용한 식에 conditional independence를 가정을 한다. $$p(x_1, x_2, \ldots, x_n) = p(x_1) \cdot p(x_2 \mid x_1) \cdot p(x_3 \mid x_2) \cdots p(x_n \mid x_{n-1})$$
그렇게 되면 위의 식과 같아지고 파라미터는 $2n - 1$로 해결이 가능해졌다. ($X_{i+1} \perp {X_1, \ldots, X_{i-1}} \mid X_i$ 이렇게도 표기한다.) Bayes Network Chain rule을 통해 모든 확률변수의 joint distribution을 표현할 수 있지만, 이때 conditional independence를 활용하면 필요한 파라미터 수를 대폭 줄일 수 있다.</div><div class="post-footer">
        <a href="/posts/cs236/lecture2/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/bayesian-network/">Bayesian network</a>,&nbsp;<a href="/tags/conditional-independence/">Conditional Independence</a>,&nbsp;<a href="/tags/cs236/">CS236</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/dm-gans/">[Paper Review]Diffusion Models Beat GANs on Image Synthesis(ADM-G)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2025-02-28">2025-02-28</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/diffusion/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Diffusion</a></span></div><div class="content">개요 GAN 대비 Diffusion Model(DM)의 이미지 생성 성능을 비교하며, DM이 GAN을 능가하는 이유를 분석한 Diffusion Models Beat GANs 논문을 리뷰할 것이다. Introduction 요즘의 생성 모델은 많이 발전해오고 있다. 하지만 그 중에도 발전 가능성이 아직 많다. 생성 모델이 더욱 발전 한다면, 사용할 수 있는 분야가 셀 수 없이 많다. GAN은 여러 평가 지표 (FID, Inception Score, Precision, &hellip; 등)에 의하여 image generation에서 SOTA를 달성하고 있다. 하지만 이 평가 지표는 다양성을 완전히 포착하지 않고, likelihood-based model보다 더 다양성을 포착하지 않는다. 또한, GAN은 최적의 하이퍼 파라미터와 정규화를 하지 않으면 모델이 붕괴하기 때문에 훈련이 어렵다. 이러한 GAN의 단점 때문에 다른 domain에 적용하기에도 어렵고, 확장시키기도 어려워졌다. 그 결과 likelihood-based model이 GAN의 sampling image quality와 비슷하게 발전해왔다. likelihood-based model도 결국 단점이 있었는데, sampling 시 GAN보다 매우 느리고 sample quality 또한 기대에 미치진 못한다. Likelihood-based model의 한 종류인 Diffusion model이 등장하여 확장성도 있고, 높은 품질을 만들어내는 성능을 보였주었다. CIFAR-10에서 SOTA를 달성했지만 다른 어려운 dataset(LSUN, ImageNet)에 대해서는 GAN에 밀려있었다. 논문 저자들이 Improved Denoising Diffusion Probabilistic Models에서 diffusion model의 신뢰성을 증가시키는 연구를 했지만 FID가 GAN에 비하여 경쟁력 있지는 않았었다. 본 논문에선 GAN과 Diffusion model이 차이가 나는 두 가지 요인에서 비롯된다고 가정한다. 최근 GAN 연구에서 사용된 모델 아키텍처는 광범위하게 탐색되고 최적화되었다. GAN은 다양성(diversity)과 정확성(fidelity) 사이에서 trade-off의 균형을 조절한다. 따라서 GAN은 높은 품질의 샘플을 생성하는 대신 전체 데이터 분포를 완전히 포괄하지는 못한다. GAN은 Generator, Discriminator로 나뉘어져 있으므로 둘 간 균형을 조정한다. 본 논문에선 이 두 가지의 요인을 Diffusion model에도 적용하고자 한다. 먼저 모델의 아키텍쳐를 개선하고, 이후 다양성과 정확성 간의 trade-off의 균형을 조절 할 수 있는 기법을 개발한다. 이 결과 본 논문에서 제안한 diffusion model이 새로운 SOTA를 달성하며, GAN을 이기게 되었다. Background 이번 단락에서는 Diffusion model 중 DDPM에 대한 간단한 배경을 설명하고 있다. Diffusion model에 대한 자세한 설명은 해당 링크에서 보면 될 것 같다. DDPM에서의 목표는 조금 더 덜 노이즈가 포함된 $x_\text{t-1}$ 를 $x_t$에서 생성하는 과정을 학습하게 된다. DDPM에서 학습하기 위한 loss로 실제 변분 하한(Variational Lower Bound) $L_\text{vlb}$를 단순화한 $L_\text{simple}$이 성능이 좋음을 관찰하였다. 이런 훈련 절차와 샘플링 절차는 denoising score matching model과 동일하다고 한다. 다음으로, 조금 더 나은 diffusion model을 설명하게 되는데 기존 DDPM에서는 reverse process에서의 분산 $\Sigma_\theta(x_t, t)$ 을 고정된 값으로 설정하였는데, 이런 고정된 분산이 샘플링 단계 수가 적을 때 성능이 낮아질 수 있다. 따라서 $\Sigma_\theta(x_t, t)$ 를 파라미터화 하여 해결하려고 했고, 훈련 loss 또한 $L_\text{vlb}$ 과 $L_\text{simple}$ 를 함께 사용하는 hybrid objective로 해결한다. 본 논문에서도 해당 objecive와 parameterization을 사용한다. 또한 DDIM의 Non-Markovian 과정으로 인한 샘플링 스텝을 줄이는 방법 또한 본 논문에서 사용한다. 마지막으로 샘플 품질을 평가하는 metrics에 관한 설명으로 이어진다. Metrics 중 Inception Score(IS)는 ImageNet 클래스 분포를 얼마나 잘 학습했는지를 측정하는 메트릭이다. 개별 샘플이 특정 클래스의 예제를 얼마나 그럴듯하게 평가하면서도, 모델이 전체 dataset 클래스 분포를 잘 반영했는지 측정한다. 이런 IS도 한계점이 있는데, 아래는 IS의 한계점을 설명한 것이다. 모든 클래스에 대한 전체 분포를 얼마나 잘 커버하는지 평가하지 못한다. 데이터셋의 일부를 단순히 암기한 모델도 높은 IS 점수를 가질 수 있다. Fréchet Inception Distance (FID)는 IS보다 더 다양성을 잘 평가할 수 있는 방법이다. Inception-V3 모델의 latent space에서 두 이미지 분포 간 거리를 측정하여 두 이미지 분포 간의 symmetric measure of distance를 측정하게 된다. sFID라는 변형 버전은 기존 FID보다 공간적 특성을 고려하여 더 정교한 평가가 가능하다.</div><div class="post-footer">
        <a href="/posts/paper/dm-gans/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/diffusion/">Diffusion</a>,&nbsp;<a href="/tags/dm-gans/">DM-GANs</a>,&nbsp;<a href="/tags/classifier-guidance/">Classifier Guidance</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/ldm/">[Paper Review]High-Resolution Image Synthesis with Latent Diffusion Models(LDM)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2025-02-17">2025-02-17</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/diffusion/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Diffusion</a></span></div><div class="content">개요 DDPM 이후, 고해상도 이미지 생성을 위해 효율적인 latent space에서 확산 과정을 수행하는 LDM에 대해 논문 리뷰를 할 것이다. Introduction 이미지 합성(Image synthesis)은 최근 가장 빠르게 발전이 되어왔지만, 많은 컴퓨터 계산 비용이 크다. 특히 고해상도 복원 문제는 AR(Autoregressive) 기반 모델들이 자주 사용하지만 모델 수십억 개의 파라미터를 요구한다. GAN은 학습 방식의 한계 때문에 multi-modal 분포에서는 모델링 하는데 한계가 있다. 이러한 가운데, Diffusion model이 이미지 합성의 여러 분야에서 뛰어난 성과(SOTA)를 보여줬다. Diffusion 모델은 다른 모델들과 다르게 모델 붕괴(model collapse), 학습 불안정성, 많은 파라미터에서 강점을 지닌다. 이러한 Diffusion 모델도 문제점이 있는데, Diffusion 모델은 mode-covering 성질을 갖고 있다. 그래서 데이터의 모든 세부 패턴을 학습하려는 경향이 있어서 많은 연산 자원을 필요로 하게 된다. Reweighted Variational Objective 방법이 연산을 줄이려고 하지만 여전히 계산량이 많다. A100 기준으로 50,000개의 샘플을 생성하는데 5일이 걸린다. 이러한 문제는 두 가지 영향을 보여준다. 첫 번째로는 훈련 시 거대한 컴퓨팅 자원을 필요로 하므로 일반 연구자나 소규모 연구팀에게 접근성이 낮다. 두 번째로는 추론 시 높은 비용과 시간을 소모하여 학습 뿐 아니라 샘플링 시에도 매우 비효율적이다.' 따라서 이 두 가지의 문제를 해결하기 위한 것이 핵심이다. 본 논문은 pixel space에서 이미 학습된 diffusion 모델을 분석하는 것 부터 시작한다. 기존의 DM은 픽셀 단위에서 학습을 진행 하였다. 이미지 자체에 대해서 훈련하는 방식이였다. 위 그림과 같이 모든 likelihood-based 모델들의 학습 과정은 두 단계로 나뉠 수 있다. 첫 번째로 높은 주파수 영역의 세부사항을 제거하며 압축을 수행하는 지각적 압축 단계이고, 두 번째로는 실제 생성 모델이 데이터의 의미적이고 개념적인 구성을 학습하는 단계이다. 본 논문은 위 두 단계와 동일하지만, 계산적으로는 더욱 효율적인 공간을 사용하는 모델을 제안한다. 본 논문에서 모델은 두 단계를 제안한다. 첫 번째로 autoencoder를 학습하여 pixel space와 지각적으로 동일하지만, 더욱 효율적인 저차원의 잠재 공간을 만든다. 두 번째로는 추가적인 공간 압축에 의존할 필요 없이 잠재 공간에서 diffusion 모델을 학습시켜 공간적 차원성(spatial dimensionality)에 대해 더 나은 확장성을 갖고 있다. 이미지 자체에 노이즈를 추가하여 학습을 했던 방식과 달리, 잠재 공간에서 학습을 하자는 것이다. 이러한 복잡성 감소 때문에 단 한 번의 네트워크 실행으로도 효율적인 이미지 생성을 할 수 있고, 본 논문은 이 모델을 Latent Diffsion Models(LDMs) 라고 부른다. 이 방법의 장점은 방대한 autoencoder를 한 번만 학습을 하게 된다면, 이를 통해 나온 latent space를 여러 DM 모델의 훈련에 사용할 수 있게 된다. 이는 곧 여러 task에서도 재사용할 수 있게 된다는 것이다. 마지막으로 본 논문의 주요 contribution에 대해 정리한다. Transformer만으로 이루어진 접근법들과 달리 더 높은 차원의 데이터에도 효율적으로 적용할 수 있다.
여러 task(inpainting 등)에서 계산 비용을 크게 감소 시키면서 경쟁력 있는 성능을 달성하였다.
기존 연구는 재구성(reconstruction)과 생성(generative)능력 사이의 차이를 조절하는 것이 중요했지만, 본 논문의 모델은 그것이 필요 없다.
두 가지를 분리해서 해결했기 때문이다. Autoencoder는 오직 **재구성(압축과 복원)**만 담당한다. Diffusion 모델은 오직 **이미지 생성(새로운 이미지 합성)**만 담당한다. 초해상도와 같은 고밀도 작업에서도 적용이 가능하다.
cross attention을 기반으로 하는 매커니즘을 개발하여 multi-modal data에도 사용할 수 있다.
Method Introduction에서도 설명이 되어있듯이, 기존 DM은 pixel space에서 매우 비용이 큰 연산을 수행해야한다는 단점이 있다. 이를 해결하기 위하여 압축과 생성 단계를 분리 하였고, 압축 단계는 계산 비용이 작은 autoencoder를 사용한다. 이를 통해 계산 효율성이 증가하고, 반복적으로 사용할 수 있는 latent space를 제공하여 범용적인 압축 모델이 된다. 또한 본 논문은 UNet의 Inductive bias를 활용하여 공간적 구조를 잘 표현하기 때문에, 과도하게 압축하지 않고도 효과적으로 이미지를 잘 생성해낼 수 있게 된다.</div><div class="post-footer">
        <a href="/posts/paper/ldm/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/diffusion/">Diffusion</a>,&nbsp;<a href="/tags/ldm/">LDM</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/ddim/">[Paper Review]Denoising Diffusion Implicit Models(DDIM)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2025-02-13">2025-02-13</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/diffusion/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Diffusion</a></span></div><div class="content">개요 DDPM 이후 DDPM의 샘플링 속도를 개선하여 보다 빠른 생성이 가능한 DDIM에 대하여 논문 리뷰 할 것이다. Introduction Deep generative model은 여러 분야에서 높은 품질의 샘플을 생성할 능력을 보여준다. GAN은 다른 어떤 생성 모델보다 더 높은 품질의 샘플을 생성 할 수 있다. 그러나 GAN은 매우 특정한 최적화 기법과 네트워크 구조를 선택해야 돼서, data 분포가 여러 모드를 충분히 학습하지 못하는 현상이 발견된다. 따라서 최근 연구에선 DDPM 같은 생성 모델이 GAN과 유사한 성능을 생성할 수 있다. DDPM과 같은 모델들은 adversarial training을 하지 않고, gaussian noise가 다양한 정도로 추가된 샘플을 복원하도록 denoising autoencoder를 훈련하는 방식으로 동작한다. 샘플링 과정은 Markov chain 방식을 따르며, 처음에는 화이트 노이즈에서 시작하여 점점 denoising하여 이미지를 복원하는 방식으로 진행된다. 이는 Langevin dynamics와 forward diffusion과정을 역전시키는 두 가지 방식으로 구현이 된다. DDPM과 같은 모델들은 샘플을 생성하는데 많은 반복이 필요하다는 것이 문제점이다. 따라서 GAN에 비하여 속도가 매우 느리다. 또한 이미지의 크기가 커질수록 더욱 심각해진다. 따라서 본 논문은 DDPM과 GAN의 효율성 차이를 줄이기 위해 DDIMs을 제안한다. DDIM은 암시적 확률 모델(implicit probabilistic models)(Mohamed &amp; Lakshminarayanan, 2016)과 밀접한 관련이 있다. 암시적 확률 모델은 DDPM과 동일한 목적 함수로 훈련된다는 점에서 유사하다. 본 논문에서는 기존 DDPM의 Markov Chain 확산 과정을 Non-Markovian 확산 과정으로 일반화 한다. Non-Markov를 사용하면 짧은 마르코프 체인을 구성할 수 있게 된다. 이는, 샘플링 속도를 획기적으로 줄일 수 있게 된다. 따라서 동일한 신경망(목적 함수)를 사용하면서도, Markov가 아닌 다양한 확산 과정을 선택함으로써 더욱 넓은 범위의 생성 모델을 자유롭게 선택할 수 있다. 또한, DDIM은 DDPM과 비교하여 세 가지의 장점을 지닌다.
샘플링 속도를 더욱 가속화 해도 DDPM과 비교하여 더 뛰어난 샘플 품질을 제공한다. DDIM에 일관성 속성이 있기 때문에 초기 latent variable에서 출발하여 높은 수준의 특징을 공유하게 된다. 초기 latent variable을 조작하여 의미적으로 유의미한 이미지 보간을 수행할 수 있다. Background 해당 부분에서는 DDPM의 전반적인 내용에 대해서 설명을 한다. 이는 이전의 글에 더 자세히 나와 있다. DDPM의 샘플링 속도가 너무 느려 이를 해결하기 위해 DDIM이 등장하여 속도를 개선한다. Variational Inference For Non-Markovian Forward Processes 생성 모델은 inference process의 역(reverse)을 추정하기 때문에, 많은 반복을 줄이기 위해 inference process에 대한 새로운 접근이 필요하다. DDPM의 핵심적인 관찰은 DDPM의 목적함수가 오직 marginal probability distribution(주변 확률 분포) $q(x_t \mid x_0)$에만 의존하고 joint distribution(공동 확률 분포) $q(x_\text{1:T}\mid x_0)$엔 직접적으로 의존하지 않는다는 점이다. 즉, 동일한 주변 확률 분포를 가지면서 다양한 공동 확률 분포가 존재하기 때문에, Markovian 특성을 갖지 않는 대체적인 생성 과정을 설계할 수 있다. Non-Markovian 추론 과정을 사용하더라도 DDPM과 동일한 대리 목적 함수를 유지할 수 있다. Non-Markovian Forward Porcesses 따라서 주변 확률 분포인 $q_\sigma(x_T|x_0)$ 는 $q_\sigma(x_T|x_0) = \mathcal{N}(\sqrt{\alpha_T}x_0, (1 - \alpha_T)I)$ 는 DDPM과 DDIM 둘 다 만족한다. 하지만 공동 확률 분포인 $q_\sigma(x_\text{1:T}|x_0)$에 대해서는 DDPM과 DDIM이 아래와 같이 다르게 표현되고 있다. $$ \begin{aligned} DDPM &amp;: q(x_\text{1:T} | x_0) := \prod_\text{t=1}^T q(x_t | x_\text{t-1}) \\ DDIM &amp;: q(x_\text{1:T} | x_0) := q_\sigma(x_T|x_0) \prod_\text{t=2}^T q_\sigma(x_\text{t-1} | x_t,x_0) \end{aligned} $$
DDIM의 공동 확률 분포 증명 확인 각 시간 단계 $x_t$가 단순히 $x_\text{t-1}$에만 의존하는 것이 아니라, $x_0$에 대한 직접적인 정보도 포함하도록 식을 변형 하려고 한다. $$ \begin{aligned} q_{\sigma}(x_{1:T} | x_0) &amp;:= q(x_1 | x_0) q(x_2 | x_1, x_0) q(x_3 | x_2, x_0) \dots q(x_T | x_{T-1}, x_0) \\ &amp;:= \cancel{q(x_1 | x_0)} \frac{q(x_1 | x_2, x_0) \cancel{q(x_2 | x_0)}} {\cancel{q(x_1 | x_0)}} \frac{q(x_2 | x_3, x_0) \cancel{q(x_3 | x_0)}}{\cancel{q(x_2 | x_0)}} \dots \frac{q(x_{T-1} | x_T, x_0) q(x_T | x_0)}{\cancel{q(x_{T-1} | x_0)}} \\ &amp;(\therefore q(x_T | x_{T-1}, x_0) = \frac{q(x_{T-1} | x_T, x_0) q(x_T | x_0)}{q(x_{T-1} | x_0)} \text{From Bayes&rsquo; rule})\\ &amp;:= q_\sigma(x_T|x_0) \prod_\text{t=2}^T q_\sigma(x_\text{t-1} | x_t,x_0) \end{aligned} $$ 따라서 DDPM은 Markovian의 특성을 가지고 있지만, DDIM은 Non-Markovian의 특징을 가지고 있다는 것을 알 수 있다.</div><div class="post-footer">
        <a href="/posts/paper/ddim/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/diffusion/">Diffusion</a>,&nbsp;<a href="/tags/ddim/">DDIM</a>,&nbsp;<a href="/tags/implement/">Implement</a></div></div>
</article><ul class="pagination"><li class="page-item active">
                    <span class="page-link">
                        <a href="/">1</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/2/">2</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/3/">3</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/6/">6</a>
                    </span>
                </li></ul></div></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>

</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@8.6.0/dist/index.umd.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"data":{"id-1":"Happy Happy Coding"},"lightgallery":true,"typeit":{"cursorChar":"|","cursorSpeed":500,"data":{"id-1":["id-1"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
