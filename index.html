<!DOCTYPE html>
<html lang="en-us">
    <head>
	<meta name="generator" content="Hugo 0.121.2">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="Good Young" />
<meta property="og:description" content="This is my cool site" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://goodyoung.github.io/" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Good Young"/>
<meta name="twitter:description" content="This is my cool site"/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/" /><link rel="alternate" href="/index.xml" type="application/rss+xml" title="Good Young">
    <link rel="feed" href="/index.xml" type="application/rss+xml" title="Good Young"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "url": "https:\/\/goodyoung.github.io\/","inLanguage": "en-us","author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": "This is my cool site","name": "Good Young"
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="page home" data-home="posts"><div class="home-profile"><div class="home-avatar"><a href="/posts/" title="Posts"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/profile.jpeg"
        data-srcset="/images/profile.jpeg, /images/profile.jpeg 1.5x, /images/profile.jpeg 2x"
        data-sizes="auto"
        alt="/images/profile.jpeg"
        title="/images/profile.jpeg" /></a></div><div class="home-subtitle"><div id="id-1" class="typeit"></div></div><div class="links"><a href="https://github.com/goodyoung" title="GitHub" target="_blank" rel="noopener noreffer me"><i class="fab fa-github fa-fw" aria-hidden="true"></i></a><a href="mailto:goodyoung.goh@gmail.com" title="Email" rel="me"><i class="far fa-envelope fa-fw" aria-hidden="true"></i></a><a href="/index.xml" title="RSS" target="_blank" rel="noopener noreffer me"><i class="fas fa-rss fa-fw" aria-hidden="true"></i></a></div></div>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/deeplab-v1/">[Paper Review]Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs(DeepLab V1)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-05-30">2024-05-30</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div><div class="content">개요 Image Segmentation 문제를 해결하기 위해 제안된 대표적인 딥러닝 모델인 DeepLab V1에 대한 논문 리뷰를 해보려고 한다. Abstract 기존 Deep Convolutional Neural Networks이하(DCNN)의 마지막 층에서 충분한 localized가 이루어지지 않았기 때문에 고수준 작업(이미지 분류)에서 좋은 성능을 발휘한다. 그러나 이런 불변성 특성 때문에 마지막 층의 응답은 특정 객체의 위치를 세밀하게 나타내지 못하여, 각 픽셀의 정확한 분류가 필요한 객체 분할(segementation)에는 단점이 될 수 있다. 불변성이란 특정 객체의 위치나 회전 여부에 상관없이 객체의 가장 두드러지는 특징을 추출할 수 있다는 것이다.
따라서 이러한 단점을 극복하기 위해 CRF랑 DCNN 마지막 층과 결합 하고, GPU에서 초당 8 프레임의 계산이 가능하게 한 hole 알고리즘을 앞으로 설명할 것 이다. Introduction 지난 2년 동안 DCNN은 이미지 분류, 객체 탐지 등 고수준 문제에서의 컴퓨터 비전 시스템의 성능을 향상시켰다. End-to-end 방식으로 학습된 DCNN이 SIFT, HOG과 같은 의존 시스템 보다 현저히 더 나은 결과를 제공한다. 이러한 이유는 불변성 덕분이었지만 지역적 정확도를 원하는 pose estimation, semantic segmentation등의 작업(low-level task)에서는 방해가 된다. signal down-sampling과 spatial insensitivity(invariance)이라는 두가지의 어려움이 있었다. signal down-sampling은 반복되는 downsampling 작업과 max-pool작업 때문에 낮아지는 해상도가 문제였다. 본 논문에서는 해당 문제를 atrous (with holes)알고리즘을 통해 해결을 하려고 한다. invariance은 DCNN의 특징 때문에 공간적 정확성을 본질적으로 제한시키는 문제가 있다. 본 논문에서는 해당 문제를 fully-connected Conditional Random Field(CRF)로 해걸하려고 한다. CRC은 Krahenbühl &amp; Koltun이 제안한 fully connected pairwise를 사용한다. 이는 부스팅 기반의 픽셀 분류기의 성능을 크게 향상시킨다. 아래는 DeepLab system의 세가지 큰 장점들 이다. speed atrous algorithm accuracy PASCAL semantic segmentation challenge에서 SOTA달성 simplicity DCNN &amp; CRFs으로 구성되어 있다. Related Work Segmentation에 대한 여러 work들이 있었지만 DeepLab은 FCN과 비슷하게 픽셀 표현에 직접 작동한다. FCN, 다른 모델들과 DeepLab의 차이점은 CRF와 DCNN-based unary term의 결합이라고 볼 수 있다. 본 논문 이후 더 향상된 방법을 사용한 여러 DeepLab의 버전(V2,V3)들이 나와 있다. CONVOLUTIONAL NEURAL NETWORKS FOR DENSE IMAGE LABELING [그림 1] DeepLabv1의 DCNN 구조 VGG16으로 모델을 구성하였으며 더 효율적인 특징 추출기를 사용하였고 그것을 기반으로 finetuned와 re-purposed를 하였다. EFFICIENT DENSE SLIDING WINDOW FEATURE EXTRACTION WITH THE HOLE ALGORITHM 본 논문에선 효과적인 CNN 특징 추출을 위한 방법에 대해서 설명을 한다. 먼저 그것을 구현하기 앞서 VGG16을 FCN과 같이 FC층을 Fully Convolutional층으로 변환을 하고 최종 이미지가 원본의 해상도를 얻게 변환을 한다. 이때 32 pixel 간격으로 매우 드물게 계산이 되기 때문에 이를 8 pixel로 변경이 필요하였다. 따라서 마지막 2개의 pooling층을 스킵하였다. 32 pixel로 한다면 localization이 떨어지기 때문에 8 pixel로 변경이 필요했다. 또한 VGG16의 네트워크는 5개의 pooling층이 존재하는 네트워크이다. 따라서 마지막 2개를 스킵한다면 원본 해상도의 1/8만큼의 해상도가 나오게 되는 것이다. 원본 이미지의 해상도를 복구 하기 위하여 즉, upsample을 하기 위하여 filter에 zero를 추가하여 sparse한 filter를 얻는 방식의 hole algorithm을 사용하였다. 따라서 마지막 3개의 convolutional은 2배가 되었고 first fully connected layer는 4배가 되었다. 이때 필터를 그대로 유지하면서 각각 입력 스트라이드 2 또는 4 픽셀을 사용하여 적용되는 특징 맵을 드문드문 샘플링함으로써 이 방법을 더 효율적으로 구현할 수 있게 된다. [그림 2] Atrous Convoltion 이때 나타나는 hole algorithm (atrous convolution)이란 필터 중간 중간에 0을 채워 넣어서 학습해야할 파라미터 수는 유지하면서 보다 넓은 영역(Receptive Field)을 참조하게끔 하는 방법이다. Receptive Field(수용 영역): 특징 추출에 사용되는 한 개의 뉴런이 받아들일 수 있는 입력 데이터의 영역
따라서 atrous convolution을 사용하면 Receptive Field가 넓어지며 localization이 완화되는 효과가 있다. 따라서 이런 atrous convolution을 사용하게 된다면 두가지의 이점이 발생한다.</div><div class="post-footer">
        <a href="/posts/paper/deeplab-v1/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/unet/">UNet</a>,&nbsp;<a href="/tags/implement/">Implement</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/segmentation/">segmentation</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/cs231n/lecture3/">[CS231n] 03.Loss Functions and Optimization</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-05-24">2024-05-24</time></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a></span></div><div class="content">개요 CS231n의 3강에 대한 내용을 정리 할 것이다.
저번 강에서는 선형 분류기에 대해서 공부를 했는데 이때 어떻게 이미지의 가중치인 W의 값들을 업데이트 하는 방법에 대해서 집중적으로 다룰 것이다.
continue</div><div class="post-footer">
        <a href="/posts/cs231n/lecture3/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/loss-function/">Loss Function</a>,&nbsp;<a href="/tags/optimization/">Optimization</a>,&nbsp;<a href="/tags/cs231n/">CS231n</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/unet/">[Paper Review]Convolutional Networks for Biomedical Image Segmentation(UNet)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-05-17">2024-05-17</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div><div class="content">개요 Image Segmentation 문제를 해결하기 위해 제안된 대표적인 딥러닝 모델인 UNet에 대한 논문 리뷰를 해보려고 한다. Abstract 딥러닝 네트워크에선 성공적인 훈련을 위해선 수많은 annotated training samples들이 필요한데 이것들을 효율적으로 사용할 수 있는 강력한 data augmentation 방법에 대해서 소개한다. 그 다음으론 Unet의 구조에 대해서 간략한 소개가 나온다. Unet의 구조는 context들을 포착할 수 있는 contracting path와 localization을 정확하게 해주는 expanding path가 서로 대칭적으로 이루어져 있다는 것을 알려준다. 그리고 또한 Unet은 적은 양의 image로 학습을 할 수 있고 ISBI challenge에서 그 전 모델보다 더 좋은 성능을 보여주었다. 이런 Unet에 대해서 좀 더 자세히 Introduction의 파트에서 설명하게 될 것이다. Introduction 본 논문에서는 지난 2년 간 deep convolutional net이 성공을 이룬 이유는 Krizhevsky의 연구(Alex Net) 때문이라고 설명했다. 보통 convolutional network의 사용은 classification task(하나의 이미지를 single class label) 였지만 biomedical image task는 localization(class label이 픽셀 단위로 분류) 이 필요했다. 또한 biomedical tasks에서는 수많은 양의 dataset들을 구하기 어려운 한계도 있었다. [그림 1] sliding-window 따라서 위 그림 처럼 이전에 sliding-window 방식으로 픽셀의 주변 patch가 한 단위가 되어 pixel predict를 하여 localization을 수행하는 방법이 있었지만 두가지의 단점이 있었다. 모든 patch 별로 연산을 하므로 시간이 오래 걸린다. 수많은 중복된 패치들이 많다. patch 크기가 크면 max-pooling layer가 많아져서 localization accuracy가 떨어지고 반대로 patch 크기가 작아지면 littel context를 하여서 localization accuracy와 use of context 간의 trade off가 있다. 따라서 이러한 단점들을 해결하기 위해 Fully convolutional network의 구조를 변형하여 더욱 elegant한 구조로 모델을 만들었다. 적은 데이터셋으로도 work가 가능하고 더욱 더 정밀한 segmentation이 가능해짐 [그림 2] unet main idea Unet의 main idea는 contracting layer(downsampling) 에서 pooling operator가 upsampling operator로 교체되는 연속적인 layer들을 보완하는 것이다. contracting path로 부터 high resolution feature들이 upsampled output과 합쳐진다. 그런 다음 연속적인 합성곱 층들이 더 precise한 output을 조합한다. output의 해상도를 증가시킨다. Upsampling part에서 더 많은 feature channel들을 가지고 있게 된다. 이는 context 정보가 high resolution으로 잘 전달이 되게 한 것이다. 따라서 contracting path와 expansive path가 U-shaped architecture가 된다. 또한 Unet은 fully connected layer를 사용하지 않고 오직 각각의 convolution의 필요한 부분만 사용을 하였다. 이것은 overlap-tile strategy으로 임의의 큰 이미지의 seamless segmentation을 가능하게 한다. 다른 방식은 GPU memory의 한계 때문에 거대한 이미지를 처리할 수 없었다. Sliding Window 방식과 유사하지만 1개의 픽셀만 classification을 하는 것이 아닌 패치 영역을 classification을 한다. 또한 전체 영역을 다 예측하는 것이 아닌 572의 입력을 받아 388 크기의 아웃풋을 생성한다. fully connected layer를 사용하지 않고 오직 convolution만 사용하여 임의의 이미지를 입력받아도 문제가 없다. 큰 이미지를 한정된 GPU에서 학습하게 된다. Border region을 예측하기 위해서 missing context는 input image를 mirroring 하여 extrapolated(나타나게 만들 수 없는 부분을 예측)로 예측이 된다. 위의 오른쪽 그림을 보면 노란색 박스 영역을 예측 하려면 파란색 박스 영역의 이미지(노란 부분보다 더 큰 크기)가 필요한데 이렇게 누락되는 부분은 mirroring의 전략을 가져간다. 실제로 오른쪽 그림의 왼쪽을 자세히 보면 테두리의 이미지가 원본 이미지랑 대칭임을 볼 수 있게 된다. 또한 그 다음 그림은 단계로 넘어가는 과정인데 이전에 예측에 사용했던 patch와 겹치기 때문에 overlap-tile이라고 불린다. 또 만약 572의 입력이 들어올 때 388로 예측을 하기 때문에 외각 부분은 예측할 수 없어서 이 때문에 mirroring 전략을 취하는 것이다. [그림 3] elastic deformation Unet은 little training data에서도 가능했다. 모델 학습 시 data augmentation을 image의 invariance(이미지 불변성)를 학습하는 elastic deformation방법을 적용하였다. deformation: 변형
image를 다양하게 변형하여 augmentation을 하는 방법이다.</div><div class="post-footer">
        <a href="/posts/paper/unet/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/unet/">UNet</a>,&nbsp;<a href="/tags/implement/">Implement</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/segmentation/">segmentation</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/cs231n/lecture2/">[CS231n] 02.Image Classification</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-05-14">2024-05-14</time></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a></span></div><div class="content">개요 CS231n을 공부하면서 정리를 위해 글을 작성해보려고 한다. 1강의 내용은 컴퓨터 비전에 대해 전반적인 역사, 이 course를 통해 얻게 될 내용들에 대해서 소개를 해주었다. 따라서 따로 정리할 것은 없어서 정리하지는 않았다. The Problem 컴퓨터는 사람과 달리 이미지를 숫자로 인식을 한다. 따라서 사람이 의미하는 것과 컴퓨터가 이해하는 것에는 차이(Semantic Gap)가 있기 마련이다. 또한 컴퓨터가 image를 인식할 때 여러 문제(problem)들이 발생하게 된다. Viewpoint variation (카메라의 움직임) Illumination (색상 차이) Deformation (다양한 모습) Occlusion (부분적으로 물체 보이는 현상) Background Clutter (배경 혼란 현상) Intraclass variation (다양한 종류) 따라서 우리는 이런 problem들을 극복할 수 있는 robust하고 확장성이 뛰어난 모델을 만들어야 한다. 이런 모델들을 만들기 위해서 Data-Driven Approach(하나의 접근 방식)를 제안한다. Data-Driven Approach에 대해서 아래에 설명을 작성했다.
image와 label들의 dataset을 모은다. 간단한 분류기를 machine learning에 의해 훈련킨다. (함수에서 train) 새로운 이미지를 가지고 분류기를 평가한다. (함수에서 predict) Nearest Neighbor 이 방법은 학습 데이터와 새로운 이미지들을 비교해서 학습 데이터 중에서 가장 유사한 이미지로 레이블을 예측하는 방법이다. Nearest Neighbor에서 이때 유사한 이미지로 판단하기 위해서 L1(manhattan) distance가 있다. L1 distance는 새로운 이미지의 pixel에 학습 데이터를 뺀 절대값들의 합을 구하여 그 합들이 가장 최소로 나온 이미지를 유사하다고 판단하는 방법이다. [그림 1] Nearest Neighbor 위 그림과 함께 Nearest Neighbor의 과정을 보자면 train 과정에서 모든 훈련 데이터를 저장한다. 그리고 predict 과정에서 모든 훈련 데이터를 비교하여 가장 유사한 훈련 데이터 image를 찾게 된다. 이 과정에서 train: O(1), predict: O(N)(# of iunput)의 시간 복잡도를 가지게 되는데 이것은 잘못되었다고 표현을 한다. 왜냐하면 우리는 보통 분류기가 빠르게 예측하기(predict)를 원하지만 Nearest Neighbor에서는 그것이 반대가 되어있는 것을 알 수 있게 된다. 또한 Nearest Neighbor알고리즘은 가장 가까운 이웃만을 보기 때문에 위와 같은 초록색이 대부분인 영역에서 노란색을 예측하는 문제가 발생 할 수 있다. 따라서 K-Nearest Neighbor알고리즘이 나오게 된다. K-Nearest Neighbor [그림 2] K-Nearest Neighbor 그 다음으로 가까운 Neighbor을 K개의 만큼 찾고, 그것끼리 다수결의 결정으로 예측을 하는 K-Nearest Neighbor방법이 있다. 또 이때 유사도를 결정할 때 L1 distance가 아닌 L2(Euclidean) distance의 계산식이 있다. 이것은 L1과 달리 좌표계가 무엇이든 상관이 없지만 L1의 경우에는 좌표계에 따라 계산값이 변할 수 있다. 각각 어떤것이 좋은지는 데이터의 성격에 따라 다르다 input 데이터가 각각의 항목이 중요한 성격(salary, 성별, 연봉 등)을 가진다면 L1이 적당할 수 있지만 어떤 역할인지 상관이 없으면 L2가 더 좋을 수 있다. 위 그림 2를 보게 되면 K가 커질 수록 좀 더 경계들이 부드러워지는 모습을 볼 수 있게 된다. 그렇다면 K가 무조건 커지면 좋은 것이냐? 그건 또 아니다. 이때 중요한 것이 적절한 K를 결정하는 것이 중요하다. 이런 K같은 요소들을 Hyperparameter이라고 부르고 다음으로 이런 적절한 Hyperparameter을 선택하는 방법에 대해서 설명하겠다. Hyperparameter 이런 Hyperparameter는 직접 시도해보고 가장 좋은 값을 찾는 것이 정답이다. [그림 3] Hyperparameter Setting idea #1: 학습데이터만을 학습하여 하이퍼 파라미터를 선택
idea #2: train한 모델을 바탕으로 Test set에서만 잘 동작하는 하이퍼 파라미터를 선택
idea #3: train set으로 모델을 학습, validation set으로 가장 좋았던 하이퍼 파라미터 선택, validation set에서 고른 하이퍼 파라미터를 바탕으로 test set을 test시작
따라서 이런 적절한 Hyperparameter가 무엇인지 결정할 때 위 3개의 방법 중 idea #3방법이 제일 좋다. 왜냐하면 idea1,2는 unseen data에 대한 정확도가 떨어진다. 결국 idea #2도 새로운 data에 약한 특성을 보인다. 예를 들어, idea #2일 때 K값을 바꿔가면서 이 중 가장 높은 성능을 지닌 K를 정한다고 했을 때 이는 test데이터에 좋은 성능을 보이는 K값인 것이다.</div><div class="post-footer">
        <a href="/posts/cs231n/lecture2/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/image-classification/">Image classification</a>,&nbsp;<a href="/tags/cs231n/">CS231n</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/fcn/">[Paper Review]Fully Convolutional Networks for Semantic Segmentation(FCN) & Implement</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-04-14">2024-04-14</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/implement/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Implement</a></span></div><div class="content">개요 Semantic Segmentation 문제를 해결하기 위해 제안된 딥러닝 모델인 Fully Convolutional Networks(이하 FCN) 논문 리뷰를 해보려고 한다. Abstract FCN은 convolutional network 자체로 end-to-end 학습, pixels-to-pixels을 훈련을 하고 semantic segmentation에서 state-of-the-art(이하 SOTA)를 달성했다.
이때 나오는 end-to-end 학습이란 독립적인 것이 아니라 하나의 모델에서 작업이 다 끝나고, 모델의 모든 filter들이 학습이 가능한 학습을 하는 학습 방법이라고 생각하면 된다. [그림 1] end-to-end 학습 end-to-end 학습이 아닌 모델인 pattern recognition모델은 사람들이 직접 filter를 설계하여 classifier만 학습을 하는 방법이다. 본 논문의 핵심 아이디어는 임의의 크기인 input과 input과 같은 크기의 output을 생성하는 fully convolutional network를 구축하는 것이다. 또한 기존의 classification networks(AlexNet, VGG net, GoogLeNet)을 통해 transfer learning을 통한 fine-tuning을 진행하였다. 마지막으로 모델의 deep layer에서 얻은 의미(Semantic) 정보와 shallow layer에서 얻은 외관(Appearance)정보를 섞는 skip architecture를 정의하였다. [그림 2] CNN layers Semantic information deep, coarse(굵다) layer
CNN에서 제일 deep한 위치에 있는 layer들이 객체의 외관은 파악하기 힘든 반면에 feature들이 활성화가 된 부분을 보면 의미가 있는 정보를 나타낸다. Appearance information shallow, fine layer
CNN의 첫 번째 layer에 있는 filter들은 보통 edge feature들을 추출하기 때문에 윤곽과 관련된 feature들을 추출한다. Introduction Convolutional networks의 등장으로 인해 classification, local task(object detection, key-point등)와 같은 분야에서 엄청난 발전을 이룰 수 있었다. 이러한 Convolutional networks의 다음 단계는 segmentation을 위한 모든 pixel에 대한 예측이다. 또한 기존의 방법들 (patchwise training, pre-post processing)등의 수고로움을 해결할 수 있다고 나와있다. 이때 설명하는 patchwise training이란 FCN이 나오기 전 segmentation 학습 방법론 이다. [그림 3] Patchwise training Patchwise learning 방식 특정 크기의 patch를 설정 및 CNN input 이때의 input -&gt; CNN에 의해 classification이 된다. 이때 특정 class로 분류가 되었다면, 해당 patch 중앙에 위치한 pixel을 분류된 class로 분류한다. 이 과정을 슬라이딩 윈도우 방식으로 모든 픽셀을 반복한다. 따라서 Patchwise learning의 방법에는 여러 단점이 생기게 된다. 많은 계산량 patch끼리 겹칠 때의 중복 계산의 우려, patch의 크기를 크게할 때 분류가 애매한 상황 patch 크기를 줄여주면 low resolution(해상도가 낮음)의 문제 마지막으로 FCN이 등장하기 전 semantic한 정보와 location한 정보를 어떻게 잘 조합할 지 몰랐다. 이를 해결하기 위해서 skip architecture가 나오게 된다. Fully convolutional networks 이제까진 기존 CNN에 대한 설명과 FCN이 나오기전의 모델들의 단점들을 살펴보며 FCN 모델의 우수성을 추상적으로 설명했다. 이젠 FCN의 알고리즘을 구체적으로 설명 해보려고 한다. Adapting classifiers for dense prediction [그림 4] FC -&gt; Convolutional layers 전형적인 분류 net(LeNet, AlexNet &hellip;)은 FC층 때문에 고정적인 input size와 non-spatial output을 가진다. 입력 이미지의 크기에 비례하여 FC층에 들어가기 위해 flatten되는 neuron의 수가 비례하기 때문에 input size가 고정적이다. 이때 flatten 되기 때문에 이미지의 공간적 정보의 손실이 있게 된다. 따라서 이러한 linear한 FC층을 [그림 1]처럼 Fully Convolutional한 구조로 변경을 하였다. 이로써 모든 layer에 Conv를 적용하여 ground truth를 각 layer의 출력으로부터 얻을 수 있어 forward와 backward가 계산 효율성에서 장점을 얻는다. 또한 단점이었던 공간 정보의 손실이 없어지는 것도 해결하였다. convolutionalization된 층의 resulting map에 해당되는 특정 위치가 특정 patch 상의 CNN 결과랑 같게 된다. 지금까진 FCN구조의 encoder 부분을 설명한 것과 같다. 다음으론 output map(coarse output)으로 부터 dense prediction을 하는 방법을 알아 볼 것 이다. Shift-and-stitch is filter rarefaction dense prediction을 하기 위해 output map을 upscaling을 해야 한다. upscaling을 하기 위한 방법으로 본문에선 shift-and-stitch방법을 고려했다고 나타난다. [그림 5] shift and stitch 그림을 토대로 max pooling을 하고 위치 정보를 저장하여 원래의 이미지 크기로 upscaling이 가능하다. 하지만 계산 비용이 큰 단점이 있게 된다.</div><div class="post-footer">
        <a href="/posts/paper/fcn/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/fcn/">FCN</a>,&nbsp;<a href="/tags/implement/">Implement</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/segmentation/">segmentation</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EA%B5%AC%ED%98%84/">논문 구현</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/segmentation/segmentation-3/">[DL]Image Segmentation 3</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-04-04">2024-04-04</time></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a></span></div><div class="content">개요 이전 글에 이어서 Image segmentation의 접근 방법 (Edge 기반 방법, 영역 기반 방법)에 대해서 알아보고자 한다. 먼저 Edge 기반 방법의 Edge Detection에 대해 알아보겠다. Edge Detection Edge Detection은 영상(Image)의 Edge(경계선, 윤곽선)을 검출하는 것이다. 이때 Edge는 깊이, 색깔, 조명이 달라서등 다양한 원인에 의해서 생길 수 있다. Edge는 이러한 원인들에 의해서 밝기가 급격하게 변한다라고 말할 수 있다. 따라서 영상상의 Edge의 밝기를 분석했을 때 Edge에 해당하는 부분은 픽셀의 밝기가 급격하게 변한다라는 특징을 가지고 있다. [그림 1] intensity function 위 [그림 1]을 보면 중간 그림 처럼 아래로 떨어지는 부분이나, 위로 오르는 부분이 Edge이다. 이 부분을 미분을 해보면 우측 그림 처럼 극값이 Edge라고 할 수 있다. 따라서 Edge 검출 방법은 미분을 이용하여 근처 픽셀과의 변화율을 찾아 검출하는 것이다. 변화율을 찾을 때 1차 미분, 2차 미분을 이용하는 방법이 있다. Edge Detection에선 1차 미분의 중앙 차분 방법을 사용하는데 이 방법을 영상 속 픽셀에 적용시키기 위하여 미분 마스크를 사용하게 된다. 미분 마스크는 가로(x), 세로(y) 방향이 있고 x,y 방향 둘 다 계산(1차 미분)을 하여 두 개의 결과를 합쳐서 일정 threshold를 기준으로 Edge를 검출 할 수 있게 된다. [그림 2] Edge Detection flow [그림 3] Mask Filter 미분 마스크의 여러 형태(마스크의 값)이 있는데 이 중 일반적으로 간단한 sobel을 많이 사용한다. 라이브러리 cv2의 Sobel 메서드를 사용하면 미분 마스크를 사용해볼 수 있다. import cv2 img = cv2.imread(&#34;some_img.png&#34;, cv2.IMREAD_GRAYSCALE) dx = cv2.Sobel(img,-1,1,0, delta = 128) # delta는 시각화를 위하여 dy = cv2.Sobel(img,-1,0,1, delta = 128) # delta는 시각화를 위하여 3*3 필터로 각각 x방향, y방향 편미분을 하는 방식 Region Growing 영역 기반 방식에서 가장 많이 사용되는 방식이 region-growing 알고리즘이다. 이 방식은 **기준 픽셀(SEED)**을 정하고 기준 픽셀과 비슷한 속성을 갖는 픽셀로 영역을 확장하여 더 이상 속성을 갖는 것들이 없으면 확장을 마치는 방식이다. **기준 픽셀(SEED)**를 정하는 방식은 Thresholding &amp; Morphological등의 방법이 사용될 수 있다. Thresholding을 사용하여 그림을 추출 한 후 구체적인 포인트를 뽑기 위해 Morphological Erosion을 하는 방식이 있다. 시드포인트를 정한 후 확장하는 방법도 다양하다. 원래의 시드 픽셀과 비교 기존 시드 픽셀과 비교하여 일정 범위 이내가 되면 영역을 확장하는 방법. (잡음에 민감, seed에 민감) 확장된 위치의 픽셀과 비교 원래 시드 픽셀의 위치가 아니라 영역이 커지는 만큼 시드 픽셀도 방향에 맞게 같이 커지는 방식. [그림 4] Region Growing Reference https://faceyourfear.tistory.com/78 https://gaussian37.github.io/vision-concept-edge_detection/ https://jstar0525.tistory.com/53#0.-edge https://blog.naver.com/laonple/220875555860 https://blog.naver.com/laonple/220890216653 https://medium.com/dawn-cau/region-based-segmentation-c1b2e06a3e2f </div><div class="post-footer">
        <a href="/posts/segmentation/segmentation-3/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/basic/">basic</a>,&nbsp;<a href="/tags/image-preprocessing/">image preprocessing</a>,&nbsp;<a href="/tags/dl/">DL</a></div></div>
</article><ul class="pagination"><li class="page-item active">
                    <span class="page-link">
                        <a href="/">1</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/2/">2</a>
                    </span>
                </li></ul></div></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.121.2">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2022 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://goodyoung.github.io" target="_blank">GoodYoung</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@8.6.0/dist/index.umd.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"data":{"id-1":"Happy Happy Coding"},"lightgallery":true,"typeit":{"cursorChar":"|","cursorSpeed":500,"data":{"id-1":["id-1"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
