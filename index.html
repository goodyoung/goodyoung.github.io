<!DOCTYPE html>
<html lang="en-us">
    <head>
	<meta name="generator" content="Hugo 0.121.2">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Good Young</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="Good Young" />
<meta property="og:description" content="This is my cool site" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://goodyoung.github.io/" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Good Young"/>
<meta name="twitter:description" content="This is my cool site"/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://goodyoung.github.io/" /><link rel="alternate" href="/index.xml" type="application/rss+xml" title="Good Young">
    <link rel="feed" href="/index.xml" type="application/rss+xml" title="Good Young"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "url": "https:\/\/goodyoung.github.io\/","inLanguage": "en-us","author": {
                "@type": "Person",
                "name": "GoodYoung"
            },"description": "This is my cool site","name": "Good Young"
    }
    </script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Good Young">GoodYoung Dev Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Good Young">GoodYoung Dev Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="page home" data-home="posts"><div class="home-profile"><div class="home-avatar"><a href="/posts/" title="Posts"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/profile.jpeg"
        data-srcset="/images/profile.jpeg, /images/profile.jpeg 1.5x, /images/profile.jpeg 2x"
        data-sizes="auto"
        alt="/images/profile.jpeg"
        title="/images/profile.jpeg" /></a></div><div class="home-subtitle"><div id="id-1" class="typeit"></div></div><div class="links"><a href="https://github.com/goodyoung" title="GitHub" target="_blank" rel="noopener noreffer me"><i class="fab fa-github fa-fw" aria-hidden="true"></i></a><a href="mailto:goodyoung.goh@gmail.com" title="Email" rel="me"><i class="far fa-envelope fa-fw" aria-hidden="true"></i></a><a href="/index.xml" title="RSS" target="_blank" rel="noopener noreffer me"><i class="fas fa-rss fa-fw" aria-hidden="true"></i></a></div></div>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/ddpm/">[Paper Review]Denoising Diffusion Probabilistic Models(DDPM)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2025-01-10">2025-01-10</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div><div class="content">개요 Diffusion의 기초인 DDPM에 대한 논문을 리뷰할 것이다. Introduction 최근 몇 년간 다양한 생성 모델(deep generative models) 이 높은 품질의 샘플을 생성하는 성과를 보였다. 에너지 기반 모델(Energy-Based Models) 및 Score Matching 기법도 GAN과 비슷한 수준의 이미지 품질을 생성하는 연구가 등장했다. 이 논문은 Diffusion Probabilistic Models의 발전이다. Diffusion Probabilistic Models (DPMs, 2015): Markov chain을 기반으로 데이터를 점진적으로 샘플링하는 모델 Markov Chain이란? - 어떤 시간에 특정 state에 도달하든, 그 이전에 어떤 state를 거쳐왔든 다음 state로 갈 확률은 항상 같다는 성질이다. DPMs는 변분 추론을 활용해 샘플을 생성하기 위해 **확산 과정(diffusion process)을 반전(reverse)**시키는 방법을 학습한다. 변분 추론(variational inference) : 확률 모델에서 복잡한 후행 확률 분포(posterior distribution)를 근사하는 방법 중 하나이다. 정확한 posterior를 계산하는 것은 intractable하여 $p(z|x)$를 계산하는 것 대신 $q_\theta(z|x)$로 근사하는 방법이다. 즉, 변분 추론을 통해 모델의 역방향 과정의 확률 분포( $p(x_\text{t-1} | x_t)$ )를 근사하여, $p_\theta(x_\text{t-1} | x_t)$ 를 통하여 정방향 과정에서 추가된 노이즈를 제거하는 방법을 학습 DPMs은 정의하기 쉽고 학습이 효율적이지만, 지금까지 고품질 샘플을 생성할 수 있다는 실증적인 연구가 부족했다. 하지만 이 논문(DDPM)에서 DPMs이 고품질 샘플을 생성하고, 때때로 다른 모델보다 더 좋은 성능을 보인다. DDPM을 특정 방식으로 파라미터화 하면 다중 노이즈 수준에서 Denoising Score Matching과 동일하며, Annealed Langevin Dynamics와도 유사한 샘플링 방식을 갖는다는 점을 발견 한다. 아직 이해를 못한 부분이다. DDPM도 샘플의 품질은 뛰어나지만, log-likelihood 기반 모델들과 비교했을 때 경쟁력 있는 성능이 존재하진 않는다. Loss 중 많은 부분이 눈에 보이지 않는 미세한 이미지 세부 정보를 설명하는데 사용이 된다. log-likelihood model : VAE, Normalizing Flows, Autoregressive Models log 가능도를 최대화 하는 모델들 Background 4번에서 DPMs는 정방향 과정에서 추가된 노이즈(destroy)를 제거하는 방법을 학습하는 과정을 수행한다고 설명하였다. 물리학적으로 미세한 세계에서는 destroy된 상태를 backward로 복원시키는 것이 가능하다. DDPM에서는 노이즈를 점차 추가하여 destroy하는 정방향 과정을 diffusion process이라고 한다. 또한, 노이즈를 제거(denoising)하여 다시 복원하는 확률 분포(노이즈)를 학습하는 과정을 reverse process라고 한다. Diffusion Process Diffusion Process에 대해서 먼저 간단한 설명을 해보려고 한다. $$ q(x_\text{1:T} | x_0) := \prod_\text{t=1}^T q(x_t | x_\text{t-1}), \quad q(x_t | x_\text{t-1}) := \mathcal{N}(x_t ; \sqrt{1 - \beta_t} x_\text{t-1}, \beta_t I) $$
Diffusion Process의 근사 후행 확률은 $q(x_\text{1:T}|x_0)$ 으로 나타낼 수 있으며, 이는 마르코프 연쇄(Markov Chain)로 정의되어 이 과정에서 step마다 가우시안 노이즈를 추가하는 방식으로 작동한다. 이때 노이즈는 $\beta_1, &hellip; ,\beta_T$ 의 형태인 variance schedule에 따라 조절된다. $\beta_t$ 는 reparameterization trick에 의해서 학습이 될 수도 있고, 하이퍼파라미터로 고정될 수 있다. 다음은 $\beta_t$ 에 대하여 reparameterization trick을 적용한 것을 알아본 것이다.
$\beta_t$에 대하여 reparameterization trick을 적용 한 경우 $\beta_t$ 를 gaussian 분포에서 샘플링하는 확률적 변수로 설정한다면:
$\beta_t = \sigma(\tilde{\beta_t}),\qquad \tilde{\beta_t} \sim \mathcal{N}(\mu_\beta, \sigma_\beta^2) $
$\tilde{\beta_t}$ : 가우시안 분포에서 샘플링 된 값 $\sigma(\cdot)$ : activation function을 활용하여 $\beta_t$가 양수임을 보장 $\mu_\beta, \sigma_\beta $ : 학습 가능한 파라미터 위 식의 $\tilde{\beta_t}$ 을 gaussian 분포에서 직접 샘플링 하기 때문에 이는 확률적 연산이 포함되므로 미분이 불가능해진다.
미분이 불가능해지면 손실 함수의 그라디언트를 계산할 수 없게 된다. (학습 불가능) 그렇기 때문에 미분 가능해지기 위하여 $\beta_t$ 에 gaussian 분포에서 직접 샘플링하는 대신, 표준 정규 분포에서 샘플링한 후, 변형하는 방법인 reparametrization trick의 방법을 사용한다.
해당 방법을 사용하면 다음과 같이 표현 될 수 있다: $\tilde{\beta_t} = \mu_\beta + \sigma_\beta \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)$
이제 확률적 연산을 결정적 연산으로 변경하여 $\tilde{\beta_t}$ 를 샘플링하는 과정이 미분 가능해졌다.</div><div class="post-footer">
        <a href="/posts/paper/ddpm/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/diffusion/">Diffusion</a>,&nbsp;<a href="/tags/ddpm/">DDPM</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/dcgan/">[Paper Review]Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks(DCGAN)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-09-13">2024-09-13</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div><div class="content">개요 GAN논문 다음으로 Generative분야에서 기초가 되는 논문인 DCGAN에 관한 리뷰를 할 것이다. Introduction Unlabeled data를 통한 재사용 가능한 feature representation을 학습하는 것은 활발한 연구이다. Computer vision에서는 무제한의 양의 image와 video를 활용하여 좋은 intermediate representation을 배울 수 있게 된다. 본 논문은 좋은 image representation을 얻기 위한 방법으로 GAN을 훈련시키고 지도학습에 generator와 discriminator를 다시 사용하는 방법을 제안을 한다. 이런 GAN은 likelihood를 최대화 하는데 매력적인 대안을 제공한다. 그들의 학습 과정에서 heuristic한 cost function이 없어지는 장점을 얻게 된다. GAN은 unstable한 훈련 방법으로 알려져 있는데 종종 output으로 무의미한 것 또한 나오게 된다. GAN에서 Discriminative와 Generator에서 각각 objective function를 통하여 gradient를 업데이트 시켜주는 방향이 상이하기 때문에 간혹 오류가 난다. GAN이 무엇을 학습하는지, 그리고 multi-layer GAN의 중간 표현을 이해하고 시각화하려는 연구는 매우 제한적이다. 따라서 본 논문은 위의 몇가지 제약사항을 해결하는 몇가지 contribution을 제안한다. 제약을 두어 GAN을 훈련 시 더욱 안정적이게 하였다. Discriminative를 image classification task로 훈련시켰는데, 이는 unsupervised algorithm보다 더 좋은 성능을 보여준다. GAN이 학습한 필터를 시각화 하고, 특정 필터가 특정 객체를 그리는 방법을 학습했다는 것을 실험적으로 보여준다. Generator들이 생성된 sample의 의미론적을 쉽게 조작할 수 있는 흥미로운 vector 속성을 가지고 있음을 보여준다. Approach and Model Architecture 이미지를 생성하기 위해 CNN을 사용한 GAN을 확장하는 것은 성공하지 못하였다. 이전에도 GAN에 CNN을 적용하려는 시도는 LAPGAN이 있었다. LAPGAN의 저자들이 저해상도의 생성 image를 반복적으로 upscale하는 것을 통하여 신뢰할 수 있는 방법을 고안하였다. 본 논문에서 또한 CNN을 사용한 GAN을 확장하는 것에 어려움을 느꼈지만 계속된 탐색 끝에 GAN을 확장하는데 적합한 구조를 찾았다. 우리의 주요 방법은 CNN구조를 적용하고 수정하는 세가지 주된 변화가 있다. 첫 번째 방법으로 spatial pooling(such as maxpooling)을 all convolutional net으로 변경하는 것이다. 이를 통하여 Discriminative의 spatial downsampling을 배우게 한다. 이 방법은 generator 또한 적용을 하여, 그것의 고유 spatial upsampling을 배우게 한다. 두 번째 방법으로는 fully connected층을 삭제하는 것이다. 삭제하고 Global average pooling을 사용한다. Global average pooling을 사용하는 것은 모델의 안정성을 높이지만 수렴 속도가 느리다. 기존 GAN은 모든 층이 fully connected layer로 구성되어 있다. Generator과 Discriminator의 입력과 출력에 각각 highest convolutional feature가 직접 연결을 하는 것이 잘 작동을 했다. 위 그림과 같이 GAN의 첫 번째 layer는 uniform noise 분포 $z$를 받게 되는데 이는 4차원 텐서로 변환되고 convolutional의 시작으로 사용된다. Discriminative의 마지막 convolutional layer은 flatten된 후, single sigmoid 출력으로 전달된다. 세 번째 방법으로는 Batch normalization으로 input으로 들어오는 값을 평균 0, 분산 1로 정규화를 하여 learning을 안정하게 하는 것이다. 이것은 poor initialization때문에 훈련 문제를 해결해주고 깊은 모델에서 gradient flow를 잘 해결할 수 있게 도와준다. 이는 generator가 GAN에서 흔하게 나올 수 있는 문제인 모든 샘플을 한 지점으로 붕괴시키는 것을 방지하게 된다. 그러나 모든 layer에 batch normalization을 적용시키는 것은 모델의 불안정성을 보여주었다. 따라서 generator의 output과 discriminator의 input에 적용하지 않는 방법을 선택하여 해당 문제를 해결하였다. 마지막으로 ReLU함수는 generator에서 사용이 되지만 output layer에서는 Tanh가 사용이 된다. 본 논문에서는 유한한 범위를 갖는 활성화 함수를 사용하면 빨리 학습이되고, 훈련 분포의 color 공간을 cover할 수 있음을 관찰하였다고 한다. 그래서 본 논문은 discriminative에서 Leaky ReLU를 사용했다. 이는 고해상도 작업에 더욱 수월하다고 알려져있다. GAN 논문에서는 Maxout을 사용하였다. 해당 챕터의 마지막에서 DCGAN에서 사용된 구조를 정리를 해주고 있다. Discriminator는 pooling layer를 strided convolutional로 교체를 하고 generator는 pooling layer를 fractional-strided convolutional로 교체를 한다. 용어는 어렵지만 쉽게 생각하여 down sampling과 upsampling을 하는 것이다. Generator와 Discriminator 둘 다 batchnorm을 사용한다.</div><div class="post-footer">
        <a href="/posts/paper/dcgan/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/dcgan/">DCGAN</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/generative/">Generative</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/gan/">[Paper Review]Generative Adversarial Nets(GAN)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-09-02">2024-09-02</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div><div class="content">개요 VAE논문 다음으로 Generative분야에서 기초가 되는 논문인 GAN에 관한 리뷰를 할 것이다. Introduction 다양한 data환경에서 확률 분포를 잘 표현할 수 있는 풍부하고 계층적인 모델을 발견하는 것이 딥러닝의 잠재력이다. 딥러닝의 두드러진 성공 사례는 풍부한 감각 input을 class label로 매핑하는 판별 모델이다. 이런 성공은 backpropagation과 dropout의 algorithm, gradient를 가진 조각별 선형적 유닛들(ReLU)에 기반을 두고 있다. 그동안 Deep generative model은 큰 영향을 끼치진 못했다. Likelihood에 대한 추정과 관련된 전략에서 발생하는 계산을 근사화하는 것이 어렵고, 조각별 선형적 유닛들을 활용하여 generative에서 활용하는 것도 어렵기 때문이다. 따라서 본 논문은 이런 어려움들을 피할 수 있는 추정 절차를 제안한다. 제안된 adversarial nets은 generative model은 discriminative model과 맞서게 된다. Discriminative model은 model 분포에서 나온 것인지 data 분포에서 나온 것인지 구분하는 방법을 학습한다. Generative model은 위조자와 비슷하게 생각할 수 있다. 이는 fake 지폐를 만들어 경찰의 탐지 없이 사용하려고 한다. 반면 discriminative model은 경찰에 비유할 수 있다. 이런 fake 지폐를 탐지하려고 한다. 이런 상황 속에서 두개의 model이 자신들의 방법을 개선하려고 한다. 결국 위조지폐가 진짜 지폐와 구별되지 않게 되게 된다. 이 framework는 다양한 모델과 최적화 방법에 대해서 특정한 training algorithm을 도출할 수 있다. Generative 모델이 MLP을 통해 random noise를 통과시켜 sample을 생성하는 특수한 경우를 탐구합니다. 이런 특수한 경우를 적대적 신경망(Adversarial Nets)이라고 부른다. Generative, discriminative model 둘 다 backpropagation을 통하여 업데이트를 하며 generative model을 통하여 sampling을 할 땐 forward만 수행한다. Adversarial nets 이젠 Adversarial nets에 대한 설명을 할 것 이다. Adversarial modeling framework는 both multilayer perceptron 일 때 곧바로 적용할 수 있다. Data x에 $p_g$ 분포를 배우기 위하여 사전에 noise 변수 $p_z(z)$를 선언한다. 그 후 $G(z;\theta_g)$의 data space에 매핑을 하게 된다. 이 때 $G$는 파라미터 $\theta_g$를 지닌 미분 가능한 함수이다. 또한 두번째 multilayer perceptron이고 output이 하나의 scalar가 나오는 $D(x;\theta_d)$가 있게 된다. 이때 $D(x)$는 $p_g$분포와는 다른 $x$로부터 나왔을 확률을 representation한다. 이때 나온 output(single scalar)이 1: real ~ 0: fake일 확률을 나타낸다. 본 논문에서 나오는 훈련은 $D$는 훈련 샘플과 $G$에서 나온 샘플 (fake image) 모두에 대해 올바른 label(real or fake)을 할당하는 확률을 최대화하도록 훈련이 된다. 또 동시에 $G$를 $log(1-D(G(z))$ 를 사용하여 최소화 하는 방식으로 훈련을 하게 된다. 위의 내용을 총 정리한 $V(G,D)$ 의 목적 함수(Object Function)를 나타내면 아래의 식이다. $$\min_G \max_D V(D, G) = E_{x \sim p_{\text{data}}(x)} [\log D(x)] + E_{z \sim p_z(z)} [\log(1 - D(G(z)))]$$
위의 Adversarial net을 나타내는 목적 함수를 좀 더 살펴 보도록 할 것 이다. $\min_G \max_D V(D, G)$ 의 부분을 먼저 살펴보자면 $V(D, G)$ 식에서 $G$ 는 낮추고자 하고 $D$ 는 높이고자 하는 것을 알 수 있다. $D$의 목표는 진짜 데이터에 대해 높은 확률을 부여하고 가짜 데이터에 대해 낮은 값을 부여하도록 하는 역할이기 때문에 최대화를 목표로 한다. $G$의 목표는 생성된 데이터가 $D$에게 진짜 데이터 처럼 보이게 하는 역할이기 때문에 최소화를 목표로 한다. $E_{x \sim p_{\text{data}}(x)} [\log D(x)]$ 은 원본 데이터 $p_\text{data}(x)$에서 한개의 데이터인 $x$를 sampling을 하여 그 $x$를 $D$에 넣은 값에 $log$를 취한 값의 기대값을 나타낸다. 따라서 앞서 13번을 보면 사전에 noise 변수인 $p_z(z)$ 를 선언한다고 나와 있다. 따라서 $E_{z \sim p_z(z)} [\log(1 - D(G(z)))]$ 의 식은 하나의 noise 분포 $p_z(z)$ 에서 한 값을 sampling하여 그 $x$ 를 생성자 $G$ 에 넣고 가짜 이미지를 만든 다음에 $D$에 넣고 $1-D$의 형태로 만든 값에 $log$를 취한 값의 기대값을 나타낸다. 그래서 이런 두 항을 $D$의 관점에서 봤을 때 maximize하기 때문에 원본 데이터(x)에 대해서는 real(1)을 찾을 수 있도록 하고 반면에 가짜 이미지(z)가 들어왔을 때는 그 이미지가 fake(0)인지 분류할 수 있게 가능하게 한다.</div><div class="post-footer">
        <a href="/posts/paper/gan/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/gan/">GAN</a>,&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/generative/">Generative</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/multimodal-survey/">[Paper Review]Multimodal Machine Learning: A Survey and Taxonomy</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-08-29">2024-08-29</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/survey/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Survey</a></span></div><div class="content">개요 Multi-Modal Learning에 관련한 서베이 논문을 리뷰해보려고 한다. Introduction 세상을 둘러싼 환경은 여러 modality를 포함하고 있다. 사람들은 이러한 modality를 sensory modality(vision or touch)와 연결을 짓는다.
본 논문은 natural language, visual, vocal signal에 관해서 중점을 두어 설명을 한다.
Multi modal을 AI에 사용하기 위해서는 multimodal message에 대해 여러 정보(multiple modalities)를 연결시킬줄 알아야 한다.
또한 여러 데이터 간 데이터의 이질성으로 인하여 Multimodal machine learning에서는 여러 해결해야할 문제가 있는데 본 논문에서는 5개를 제시한다.
첫 번째는 Representation이다. 이는 multimodal data를 어떻게 잘 요약하고 표현을 할 지에 대한 문제이다.
두 번째는 Translation이다. 이는 하나의 modality에서 다른 하나의 modality로 어떻게 mapping(translate)을 할 지에 대한 문제이다.
이미지에 대한 올바른 방식의 해석이 있어도 단 하나의 완벽한 해석은 존재하지 않는다. 세 번째는 Alignment이다. 이는 여러개의 modality로부터 요소 사이의 관계들을 정렬하여 식별하는 것이다. 서로 다른 modality 간의 유사성을 측정하고, 가능한 장거리 의존성 및 모호성을 처리해야 한다. 네 번째는 Fusion이다. 여러 modality의 추론 결과를 합치는 것이다. 다른 modality로부터 오는 정보들은 다양한 예측 결과를 가져올 수 있다. 다섯 번째는 Co-learning이다. Modality간에 knowledge를 전달하는 것이다. 이것은 한 modality의 data가 부족할 때 유용하다. 위 표는 multi-modal을 적용하는 application에서 위에 설명한 5가지의 challenge의 포함 여부를 나타낸 것이다.
위 표를 확인하며 multi-modal로 활용할 수 있는 task가 무엇인지도 함께 확인하면 될 것 같다.
Representation 이젠 5가지의 challenge에 대해서 설명할 것이다. 먼저 첫 번째로 설명하는 Multimodal Representation에 대한 설명이다.
여러 modality를 representing하는 것은 다양한 어려움이 존재한다.
이질성인 데이터에서 어떻게 섞을 것인지 다른 종류의 noise를 어떻게 처리할 것인지 missing data를 어떻게 처리할 것인지 좋은 representation하는 방법은 model의 성능을 중요하다. (최근 speech recognition, visual object detection 등의 성능 향상 사례가 있다.)
또한 좋은 representation을 위한 몇 가지 속성으로 부드러움(smoothness), 시간적 및 공간적 일관성(temporal and spatial coherence), 희소성(sparsity), 자연스러운 클러스터링(natural clustering) 등이 있다.
Multi modal representation을 위한 여러 속성들이 있다. 그것은 표현 공간에서의 유사성이다. 이는 해당 개념들의 유사성을 반영해야 하며, 일부 modality가 없어도 쉽게 표현을 얻을 수 있어야 하고, 관찰된 modality를 바탕으로 누락된 모달리티를 채울 수 있어야 합니다.
이전까지 단일 modality에 대한 연구는 광범위 하게 연구되어 왔다. 이미지에 관련한 data는 SIFT기법 에서 CNN기법으로 연구되어 왔고 audio domain은 음향적 특징들이 deep neural network에서 rnn으로 연구되어 왔다.
이런 와중 multi-modal에선 단일 modality에 대한 연구들을 단순히 concat하는 방법만 사용하고 있다. 이런 방법론들이 변화되고 있다.
따라서, 본 논문에선 joint와 coordinated라는 두 가지의 representation 방법을 소개한다.
joint는 각각의 modality를 같은 representation space에 결합하는 방식이다. 이는 아래와 같은 수식으로 표현할 수 있다. ($x_1, x_n$등은 각각 modality이다.)
$$ x_m = f(x_1,&hellip;,x_n)$$
coordinated는 각각의 modality를 각각 분리해서 처리하지만 similarity 규정을 사용해 coordinated space에 가져온다. 이는 아래와 같은 수식으로 표현할 수 있다. $$f(x_1) ~ g(x_2)$$
Joint Representation Joint Representation은 독립적인 modality 특징들을 concatenation을 한다고 생각하면 된다.
앞으로는 data의 representation 방법 중 가장 유명한 방법인 Neural network에서 Joint Representation을 하는 방법에 대해서 설명할 것이다.
Neuraul network을 사용해 multi modal representation을 구축하기 위해 각 modality는 여러 개의 개별 신경 계층으로 시작하고, 이후 이 modality들을 joint space에 투영하는 hidden layer가 따른다.
이렇게 joint된 representation 들은 hidden layer를 거치거나 예측에 직접적으로 사용을 한다.
이런 neural network에서 훈련을 할 때, 많은 label data가 필요하게 된다. 따라서 unsupervised data에서 autoencoder를 사용해 이러한 표현을 pre-training하는 것이 일반적이다.
하나의 예시로 denoising하는 여러개 autoencoder를 stack한 후 다른 autoencoder layer를 사용하여 fuse하게 된다.</div><div class="post-footer">
        <a href="/posts/paper/multimodal-survey/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/computer-vision/">computer vision</a>,&nbsp;<a href="/tags/multi-modal-learning/">multi-modal learning</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/paper/vae/">[Paper Review]Auto-Encoding Variational Bayes(VAE)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-08-26">2024-08-26</time></span>&nbsp;<span class="post-category">included in <a href="/categories/paper-review/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Paper Review</a>&nbsp;<a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a></span></div><div class="content">개요 Generative분야에서 기초가 되는 논문인 VAE에 관한 리뷰를 할 것이다. Introduction 연속적인 latent variable(잠재변수)나 파라미터가 계산이 힘든 후방 확률 분포를 가지는 확률 모델을 어떻게 효과적으로 추론하고 훈련을 시키는 방법에 대해 설명이 나온다. 디코더만으로 학습 및 훈련을 진행할 수 없어서 인코더를 가져온 것이다. 해당 내용은 Method 부분에서 자세하게 다룰 것이다. 기존의 방법인 Variational Bayesian(VB) 방법은 계산하기 힘든 사후 확률을 적절하게 최적화 한다. 하지만 이 방법은 후방 확률에 대한 기대값의 분석적 해결책을 요구하며, 일반적인 경우에도 계산이 어렵다. 그래서 본 논문에서 기존 변동의 lower bound의 reparameterization가 어떻게 간단하고 다른 비편향적인 estimator의 lower bound를 만드는지 알려준다. Stochastic Gradient Variational Bayes(SGVB) estimator가 posterior inference(후방 추론)을 잘 하도록 한다. 이는 확률적 경사 하강법을 사용하여 곧바로 최적화도 된다. 독립적이고 동일한 분포를 띄는 이 latent variable에서 본 논문은 Auto-Encoding VB 알고리즘을 제안한다. SGVB estimator를 사용하여 추론과 learning을 neural network인 recognition model에 효율적으로 적용한다. 비싼 추론 없이 모델 param을 효과적으로 배우는 방법을 사용한다. 해당 estimator를 사용하면 효과적인 후방 추론을 수행하게 하여 최적화가 된다. 위처럼 neural network에서 recognition으로 사용하게 되면 variational auto-encoder(VAE)가 된다. Method 어떠한 dataset이 있을 때 실제 파라미터 $\theta$와 latent variable $z^\text{(i)}$ 는 우리가 알 수 없다. 따라서 본 논문에선 Intractability하고 large dataset에도 잘 최적화 할 수 있는 알고리즘을 개발을 한다. 그렇게 하기 위하여 세가지 문제를 설명한다. 파라미터 $\theta$ 에 대한 효율적인 근사를 하는 문제 관찰된 값 $x$와 $\theta$를 기반으로 latent variable $z$를 효율적인 사후 추론 하는 문제 $x$의 효율적인 근사 추론하는 문제 이미지 노이즈 제거(denoising), 이미지 복원, 초해상도 등 가능 이런 문제를 해결하기 위하여 recognition model ($q_{\phi}(z \mid x)$)을 도입하였다. 이는 $p_{\theta}(x \mid z)$를 가장 근사화 하는 네트워크 이다. 이 네트워크의 목표는 decoder에서 training data(x)의 likelihood를 최대화 하고 싶은 것이다. 따라서 해당 목표를 수학 식으로 표현하자면 아래와 같다. 이는 x가 나올 수 있는 확률 ($p_{\theta}(x)$)이 가장 커지는 distribution을 찾는 것으로 생각하면 된다. $$p_{\theta}(x) = \int p_{\theta}(z) , p_{\theta}(x \mid z) \ dz $$
이때 위 식은 아래의 식에서 나왔다. $$\frac{P(x, z)}{P_{\theta}(z)} = p(x \mid z)$$
따라서 식을 정리하면 아래와 같다. $$p_{\theta}(x) = \int P(x, z) \ dz $$
위 식을 해석하면 x와 z가 동시에 일어날 확률을 모든 z에 대해서 적분하면 그것이 x의 확률이 되는 것이다. 하지만 문제는 모든 z에 대해서 적분을 하기가 intractability하는 것이다. 이 문제를 해결하기 위하여 확률적 인코더인 $q_{\phi}(z \mid x)$ (encoder)가 나오게 되었다. Evidence LowerBOund (ELBO) 다음은 data likelihood를 어떻게 최적화 하는지에 대해서 설명을 할 것이다. 먼저 우리의 목적인 $p_{\theta}(x^i)$의 식을 풀어보자면 아래와 같다. $$\log p_{\theta}(x^{(i)}) = E_{z \sim q_{\phi}(z \mid x^{(i)})} \left[ \log p_{\theta}(x^{(i)}) \right]$$
적분에서 기대값으로 변경하기 위하여 log를 씌우고 Decoder에서 $z$가 $q_{\phi}(z \mid x^{(i)})$ (encoder)의 분포를 따를 때를 의미한다. 위 식을 조금 더 분해하면 아래와 같다. 베이즈 정리를 사용하여 식을 변형하고 상수를 곱하였다. $$= E_{z} \left[ \log \frac{p_{\theta}(x^{(i)} \mid z) p_{\theta}(z)}{p_{\theta}(z \mid x^{(i)})} \right] (\therefore p(z \mid x^{(i)}) = \frac{p(x^{(i)} \mid z) p(z)}{p(x^{(i)})})$$ $$= E_{z} \left[ \log \frac{p_{\theta}(x^{(i)} \mid z) p_{\theta}(z)}{p_{\theta}(z \mid x^{(i)})} \frac{q_{\phi}(z \mid x^{(i)})}{q_{\phi}(z \mid x^{(i)})} \right] $$
위 식에서 log 변환을 하여 아래와 같은 식을 만들게 된다. $$ = E_{z} \left[ \log p_{\theta}(x^{(i)} \mid z) \right] - E_{z} \left[ \log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z)} \right] + E_{z} \left[ \log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z \mid x^{(i)})} \right] $$
다음의 형태를 KL-divergence의 형태로 변경을 하게 되면 최종적인 아래의 식이 나오게 된다.</div><div class="post-footer">
        <a href="/posts/paper/vae/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a>,&nbsp;<a href="/tags/transformer/">Transformer</a>,&nbsp;<a href="/tags/generative/">Generative</a>,&nbsp;<a href="/tags/vae/">VAE</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/posts/cs231n/lecture10/">[CS231n] 10. Recurrent Neural Networks</a>
    </h1><div class="post-meta"><span class="post-author"><a href="https://goodyoung.github.io" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>GoodYoung</a></span>&nbsp;<span class="post-publish">published on <time datetime="2024-08-21">2024-08-21</time></span>&nbsp;<span class="post-category">included in <a href="/categories/dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DL</a>&nbsp;<a href="/categories/theory/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Theory</a>&nbsp;<a href="/categories/lecture/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Lecture</a></span></div><div class="content">개요 CS231n의 10강에 대한 내용을 정리 할 것이다. Recurrent Neural Networks 이번 강의에서는 Recurrent Neural Networks에 대해서 설명을 한다. 지금까지의 networks는 one-to-one의 구조를 띈 모델이지만 위 그림과 같이 여러 구조를 띌 수 있게 된다. 따라서 각 구조 별로 다양한 task로 활용할 수 있다. one-to-many image captioning many-to-one sentiment classification many-to-many machine translation videio classification on frame level $$h_t = f_W(h_\text{t-1},x_t)$$
다음은 RNN의 기본 수식이다. 이전 상태($h_\text{t-1}$, hidden state)와 입력 값($x_t$)을 계산하여 new state로 나오게 된다. 더 자세한 수식은 위 그림과 같다. 위의 수식을 기반으로 RNN의 Computational Graph로 표현을 하게 된다면 위 그림과 같다. 보면 각 스텝에서 다 같은 W를 사용하는 것을 확인할 수 있게 된다. 위 그림은 RNN에서 Many-to-Many일 때 Loss 계산 까지 표현되는 그림인데 각각의 time-step마다 local gradient를 계산하게 되고 그것이 합쳐진 L까지 가서 upstream gradient로 내려오게 된다. 다음은 Many-to-One일 때 인데 최종 output인 y가 summarize context를 모든 time-step에 따른 하나의 결과를 담고 있다. 다음은 One-to-Many인데 이것은 fix size 형태로 입력이 되고 variable size인 output을 출력하는 형태이다. 이러한 Many-to-One구조와 One-to-Many를 섞은 구조인 Seq2Seq구조도 있다. Encoder에서 다양한 크기의 input이 들어오고 그것의 전체를 요약하는 $h_t$가 있게 된다. Encoder에서 나온 vector를 Decoder로 넘겨 다양하게 출력을 할 수 있게 한다. 이때 입력, 출력의 크기를 조절하기 위하여 입력 토큰, 출력 토큰을 지정하여 모델이 처음과 끝을 알 수 있게 한다. 이 구조는 번역을 하는 task에서 사용할 수 있게 된다. (영어 -&gt; franch) 이제는 하나의 문자를 입력 받고 다음으로 올 문자를 예측하는 모델을 설명할 것이다. 문자를 모델에 입력하기 위하여 하나의 벡터로 넣기 위하여 embedding 작업을 거치는데 embedding 방법 중 one-hot-encoding의 방법을 사용하였다. 그 후 그것에 가중치 행렬($W_\text{xh}$)을 곱하여 hidden layer로 들어가고 또 그게 다시 output layer로 출력이 된다. 여기서 hello라는 결과를 보여야 하기 때문에 맨 처음에 e라는 출력값을 가진다. 하지만 이 경우 잘못 예측을 하여 o로 예측을 한 경우이다. 다음으로 e가 input으로 들어가고 새로운 hidden state를 만든다. 이러한 과정을 계속 거치면 이 모델은 이전의 문장들의 문맥을 참고하여 다음 문자가 무엇일지 예측을 할 수 있게 되는 것이다. 그렇다면 이 모델의 Test time에서는 어떻게 작동을 하고 있는지 보여준다. 이 모델을 잘 활용을 하기 위해서는 sampling이라는 샘플링 기법을 활용하는 것이다. 위 그림은 모델이 output layer를 거쳐 softmax를 취하여 확률 분포를 사용하여 sampling을 한 과정을 나타낸다. 해당 경우에 e는 뽑힐 확률이 굉장히 작았음에도 불구하고 운이 좋게 e가 샘플링 되었다. 또한 이 샘플링으로 나온 e를 다음 input으로 넣어주고 반복 과정을 거쳐 test를 하게 된다. 가장 높은 스코어를 선택하지 않고 샘플링을 하는 이유는 모델에서 다양성을 얻을 수 있게되기 때문이다. 항상 h를 첫 input으로 놓는다고 할 때, 확률분포로 샘플링을 하게 된다면 그럴듯한 다양한 문장들을 출력할 수 있어 이것이 출력의 다양성으로 이어진다. 다음은 RNN에서 역전파를 사용했을 때를 나타낸다. 각 스텝마다의 loss를 계산해서 최종적인 output을 나타내게 되는데 시퀀스가 너무 길게 되면 문제가 생길수도 있다고 한다. 길면 메모리 사용량도 많고 학습이 너무 느릴 것이다. data point의 모든 요소에 대한 기울기를 계산하는 것은 엄청난 비용을 초래한다. 따라서 이러한 문제를 해결하기 위하여 나온 방법은 truncated backpropagation이다. 입력이 너무 길다고 하더라도 train할 때 한 스텝을 일정 단위로 자르고 단계를 진행하기 때문에 위의 문제를 해결할 수 있다. 이전에 계산한 hidden state는 계속 유지하고 반복해야 한다. backpropagation은 현재 배치만큼만 진행해야 한다. Image Captioning 다음은 Image captioning에 관한 설명이다. Image captioning을 하기 위해선 CNN에 Image를 넣고 CNN의 result vector를 다시 RNN에 넣어서 caption에 사용 될 문자를 하나씩 생성해가는 방식이다.</div><div class="post-footer">
        <a href="/posts/cs231n/lecture10/">Read More</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/lstm/">LSTM</a>,&nbsp;<a href="/tags/rnn/">RNN</a>,&nbsp;<a href="/tags/neural-networks/">Neural Networks</a>,&nbsp;<a href="/tags/cs231n/">CS231n</a></div></div>
</article><ul class="pagination"><li class="page-item active">
                    <span class="page-link">
                        <a href="/">1</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/2/">2</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/3/">3</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/5/">5</a>
                    </span>
                </li></ul></div></div>
            </main><footer class="footer">
  
</footer>
</div>
</body>

</html>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@8.6.0/dist/index.umd.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":500},"data":{"id-1":"Happy Happy Coding"},"lightgallery":true,"typeit":{"cursorChar":"|","cursorSpeed":500,"data":{"id-1":["id-1"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-86432198-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-86432198-1" async></script></body>
</html>
